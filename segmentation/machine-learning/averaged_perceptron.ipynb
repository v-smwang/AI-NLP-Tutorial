{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于平均感知器的分词器\n",
    "\n",
    "## 原理\n",
    "\n",
    "&emsp;&emsp;我们知道对于一句话进行分词，我们可以通过给每个字进行标注状态（B：分词词首；M：分词词中；E：分词词尾；S：单个词分词）来分词，然后去计算出每个字到底是什么状态，从而完成分词。\n",
    "\n",
    "&emsp;&emsp;那使用感知器，无非也就是用感知器来预测出每个字的4种状态的概率分别是多少，然后利用维特比算法找出整个句子最大概率的状态路径，从而完成分词。和hmm分词差不多，选取的是hmm里的特征，每个特征进行权重训练，而不像hmm直接计算得来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class AveragedPerceptron(object):\n",
    "\n",
    "    '''An averaged perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            weights = self.weights[feat]\n",
    "            for label, weight in weights.items():\n",
    "                scores[label] += value * weight\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda label: (scores[label], label))\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "def train(nr_iter, examples):\n",
    "    '''Return an averaged perceptron model trained on ``examples`` for\n",
    "    ``nr_iter`` iterations.\n",
    "    '''\n",
    "    model = AveragedPerceptron()\n",
    "    for i in range(nr_iter):\n",
    "        random.shuffle(examples)\n",
    "        for features, class_ in examples:\n",
    "            scores = model.predict(features)\n",
    "            guess, score = max(scores.items(), key=lambda i: i[1])\n",
    "            if guess != class_:\n",
    "                model.update(class_, guess, features)\n",
    "    model.average_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "1、对每个句子中的每个字，取7种特征【unigrams+bigrams】\n",
    "\n",
    "> $C_{i-1}$ #前一个词 \\\n",
    "> $C_i$ #当前词 \\\n",
    "> $C_{i+1}$ #后一个词 \\\n",
    "> $C_{i-2}/C_{i-1}$ #前两个词与前一个词 \\\n",
    "> $C_{i-1}/C_i$ #前一个词与当前词 \\\n",
    "> $C_i/C_{i+1}$  #当前词与后一个词 \\\n",
    "> $C_{i+1}/C_{i+2}$  #后一个词与后两个词\n",
    "\n",
    "注意：还需要添加第8个特征即前一个字的类标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集处理完成！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_feature = {'BL=B':0, 'BL=M':1, 'BL=E':2, 'BL=S':3, 'BL=_BL_':4}  # 特征取值集合\n",
    "sentence_feature_dict = {}  # 记录每个句子中每个字的特征，key是句子，value是特征list\n",
    "sentence_tag_dict = {}  # 每个字的类标，，key是句子，value是类标list\n",
    "all_sentences = []  # 所有句子\n",
    "START_CHAR = '\\1'\n",
    "END_CHAR = '\\2'\n",
    "\n",
    "count = 0\n",
    "\n",
    "def handle_feature(feature, char_feature, all_feature):\n",
    "    feature_id = all_feature[feature] if feature in all_feature else len(all_feature)\n",
    "    all_feature[feature] = feature_id\n",
    "    char_feature.append(feature_id)\n",
    "\n",
    "# weights = np.ones(())\n",
    "data_f = open('./data/RenMinData.txt', 'r', encoding='utf-8')\n",
    "for line in data_f.readlines():\n",
    "    count += 1\n",
    "    line = line.strip()\n",
    "    # 打标签\n",
    "    words = line.split(' ')\n",
    "    all_sentences.append(''.join(words))\n",
    "    tag_list = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "        else:\n",
    "            tag_list.append(all_feature['BL=B'])\n",
    "            for w in word[1:len(word)-1]: # 中间字\n",
    "                tag_list.append(all_feature['BL=M'])\n",
    "            tag_list.append(all_feature['BL=E'])\n",
    "    sentence_tag_dict[''.join(words)] = tag_list\n",
    "    \n",
    "    # 获取特征    \n",
    "    sentence = line.replace(' ','')\n",
    "    sentence_feature = []\n",
    "    for i, char in enumerate(sentence):\n",
    "        char_feature = []\n",
    "        # 前2\n",
    "        pre2 = sentence[i-2] if i>=2 else START_CHAR\n",
    "        # 前1\n",
    "        pre1 = sentence[i-1] if i>=1 else START_CHAR\n",
    "        # 当前\n",
    "        cur = char\n",
    "        # 后1\n",
    "        next1 = sentence[i+1] if i < len(sentence)-1 else END_CHAR\n",
    "        # 后2\n",
    "        next2 = sentence[i+2] if i < len(sentence)-2 else END_CHAR\n",
    "        \n",
    "        # unigrams\n",
    "        one = pre1+'1'\n",
    "        handle_feature(one, char_feature, all_feature)\n",
    "        \n",
    "        two = cur+'2'\n",
    "        handle_feature(two, char_feature, all_feature)\n",
    "        \n",
    "        three = next1+'3'\n",
    "        handle_feature(three, char_feature, all_feature)\n",
    "        \n",
    "        # bigrams\n",
    "        four = pre2+'/'+pre1+'4'\n",
    "        handle_feature(four, char_feature, all_feature)\n",
    "        \n",
    "        five = pre1+'/'+cur+'5'\n",
    "        handle_feature(five, char_feature, all_feature)\n",
    "        \n",
    "        six = cur+'/'+next1+'6'\n",
    "        handle_feature(six, char_feature, all_feature)\n",
    "        \n",
    "        seven = next1+'/'+next2+'7'\n",
    "        handle_feature(seven, char_feature, all_feature)\n",
    "        \n",
    "        sentence_feature.append(char_feature)\n",
    "        #print('char_feature',char_feature)\n",
    "    sentence_feature_dict[''.join(words)] = sentence_feature     \n",
    "print('数据集处理完成！')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共句子数： 297959\n",
      "238367\n",
      "i-- 0\n",
      "i-- 10000\n",
      "i-- 20000\n",
      "i-- 30000\n",
      "i-- 40000\n",
      "i-- 50000\n",
      "i-- 60000\n",
      "i-- 70000\n",
      "i-- 80000\n",
      "i-- 90000\n",
      "i-- 100000\n",
      "i-- 110000\n",
      "i-- 120000\n",
      "i-- 130000\n",
      "i-- 140000\n",
      "i-- 150000\n",
      "i-- 160000\n",
      "i-- 170000\n",
      "i-- 180000\n",
      "i-- 190000\n",
      "i-- 200000\n",
      "i-- 210000\n",
      "i-- 220000\n",
      "i-- 230000\n",
      "训练集准确率： 0.9675375423865655\n",
      "验证集准确率： 0.959462607158562\n",
      "i-- 0\n",
      "i-- 10000\n",
      "i-- 20000\n",
      "i-- 30000\n",
      "i-- 40000\n",
      "i-- 50000\n",
      "i-- 60000\n",
      "i-- 70000\n",
      "i-- 80000\n",
      "i-- 90000\n",
      "i-- 100000\n",
      "i-- 110000\n",
      "i-- 120000\n",
      "i-- 130000\n",
      "i-- 140000\n",
      "i-- 150000\n",
      "i-- 160000\n",
      "i-- 170000\n",
      "i-- 180000\n",
      "i-- 190000\n",
      "i-- 200000\n",
      "i-- 210000\n",
      "i-- 220000\n",
      "i-- 230000\n",
      "训练集准确率： 0.9746846766258163\n",
      "验证集准确率： 0.9648003988105345\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "tag_list = [0,1,2,3]  # 标签id\n",
    "hidden_node_num = len(tag_list)  # 隐状态个数（标签个数）\n",
    "# 参数\n",
    "global W\n",
    "# W = np.random.randn(hidden_node_num, len(all_feature))\n",
    "W = np.zeros((hidden_node_num, len(all_feature)))\n",
    "             \n",
    "def viterbi_decode(sentence_feature, W):\n",
    "    path_prob = []  # 路径和该路径的概率\n",
    "    pre_X = []\n",
    "    best_path = []  # 最好的路径，也就是最好的tag\n",
    "    for i, char_feature in enumerate(sentence_feature):\n",
    "        \n",
    "        if i == 0:\n",
    "            char_feature.append(4)  # 前一个字的标签\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率【因为是第一个字所以只用到发射概率和初始概率】\n",
    "            # 第一个tag是当前字的标签，第二个tag是上一个字的标签\n",
    "            path_prob.append({tag:(tag, prob) for tag, prob in enumerate(Z)})\n",
    "        else:\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率\n",
    "            res = {}\n",
    "            for j, prob in enumerate(Z):\n",
    "                # 加上上一个字的标签分别为0，1，2，3时的概率，然后求最大【发射概率、转移概率、初始概率】\n",
    "                max_prob, pre_tag =  max([(prob + W[j][pre_tag] + value[1], pre_tag) for pre_tag, value in path_prob[i-1].items()])\n",
    "                res[j] = (pre_tag, max_prob)\n",
    "            path_prob.append(res)\n",
    "    #print('path_prob:',path_prob)\n",
    "    # 确定最后一个字的tag，然后回溯，确定路径（确定每个字的tag）\n",
    "    last_tag = -1\n",
    "    max_prob = -100000\n",
    "    for tag in tag_list:\n",
    "        prob = path_prob[-1][tag][1]\n",
    "        if max_prob < prob:\n",
    "            last_tag = tag\n",
    "    #last_tag = sorted(, key=lambda x: x[1][1], reverse = True)[0][0]\n",
    "    #print('last_tag:', last_tag)\n",
    "    # last_tag = np.argmax([pp[1][1] for pp in path_prob[-1]])\n",
    "    best_path.append(last_tag)\n",
    "    for pp in reversed(path_prob):\n",
    "        best_path.append(pp[last_tag][0])\n",
    "        last_tag = pp[last_tag][0]\n",
    "    best_path.pop()\n",
    "    best_path.reverse()\n",
    "    #print('best_path:', best_path)\n",
    "    return best_path\n",
    "                \n",
    "def precision_and_recall(predict_sentence_tags):\n",
    "    correct_count = 0\n",
    "    for sentence, predict_sentence_tag in predict_sentence_tags:\n",
    "        if predict_sentence_tag == sentence_tag_dict[sentence]:\n",
    "            correct_count += 1\n",
    "    return correct_count / len(predict_sentence_tags)\n",
    "    \n",
    "    \n",
    "    \n",
    "def precision_and_recall2(predict_sentence_tags):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for sentence, predict_sentence_tag in predict_sentence_tags:\n",
    "        for i, tag in enumerate(predict_sentence_tag):\n",
    "            total_count +=1\n",
    "            if tag == sentence_tag_dict[sentence][i]:\n",
    "                correct_count += 1\n",
    "    return correct_count / total_count\n",
    "    \n",
    "\n",
    "# 训练\n",
    "def train(all_sentences, maxIteration, ratio = 0.7):\n",
    "    global W\n",
    "    end = int(np.floor(ratio*len(all_sentences)))\n",
    "    print(end)\n",
    "    x_train = all_sentences[:end]\n",
    "    x_val = all_sentences[end:]\n",
    "    for iter in range(maxIteration):\n",
    "        random.shuffle(x_train)  # 打乱\n",
    "        start = time.time()\n",
    "        for i, sentence in enumerate(x_train):\n",
    "            sentence_feature = sentence_feature_dict[sentence]\n",
    "#             print('i--', i)\n",
    "            if i % 10000 == 0:\n",
    "#                 print('10000次耗时：', time.time()-start)\n",
    "                start = time.time()\n",
    "                print('i--', i)\n",
    "#             start = time.time()\n",
    "            predict_tag = viterbi_decode(sentence_feature, W)\n",
    "#             print('viterbi_decode耗时：', time.time()-start)\n",
    "            actual_tag = sentence_tag_dict[sentence]\n",
    "#             print('predict_tag:', predict_tag)\n",
    "#             print('actual_tag:', actual_tag)\n",
    "            if predict_tag == actual_tag:\n",
    "                #print('true')\n",
    "                continue\n",
    "            else:\n",
    "                # 更新权重：每个特征上的实际标签权重增加(+1)，其他标签权重减少(-1)\n",
    "#                 start = time.time()\n",
    "                for j, char_feature in enumerate(sentence_feature):\n",
    "#                     print('predict_tag:', predict_tag)\n",
    "#                     print('actual_tag:', actual_tag)\n",
    "                    if predict_tag[j] == actual_tag[j]:\n",
    "                        continue\n",
    "#                     print('char_feature:',char_feature)\n",
    "                    for tag in tag_list:\n",
    "                        if tag == actual_tag[j]:\n",
    "                            #print('W[tag][char_feature] == ', W[tag][char_feature])\n",
    "                            W[tag][char_feature] += 1\n",
    "                            if j == 0:  # 第一个字\n",
    "                                W[tag][4] += 1\n",
    "                            else:\n",
    "                                W[tag][actual_tag[j-1]] += 1\n",
    "                            #print('W[tag][char_feature] ++1 == ', W[tag][char_feature])\n",
    "                        elif tag == predict_tag[j]:\n",
    "                            W[tag][char_feature] -= 1\n",
    "                            if j == 0:  # 第一个字\n",
    "                                W[tag][4] -= 1\n",
    "                            else:\n",
    "                                W[tag][predict_tag[j-1]] -= 1\n",
    "#                 print('W耗时：', time.time()-start)\n",
    "\n",
    "        # 本轮次训练集和验证集上的结果\n",
    "        train_predict_sentence_tags = [(sentence, viterbi_decode(sentence_feature_dict[sentence], W)) for sentence in x_train[:10000]]\n",
    "        print('训练集准确率：', precision_and_recall2(train_predict_sentence_tags))\n",
    "        val_predict_sentence_tags = [(sentence, viterbi_decode(sentence_feature_dict[sentence], W)) for sentence in x_val]\n",
    "        print('验证集准确率：', precision_and_recall2(val_predict_sentence_tags))\n",
    "\n",
    "print('总共句子数：', len(all_sentences))\n",
    "train(all_sentences, 2, ratio = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "&emsp;&emsp;对于没有出现在训练集里的特征如何处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此外还要定期到全国各省去访问。\n",
      "此外还要定期到全国各省去访问。\n",
      "[1, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 0, 2, 3]\n",
      "[0, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# 获取句子特征\n",
    "def get_sentence_features(sentence):\n",
    "    print(sentence)\n",
    "    sentence_features = []\n",
    "    for i, char in enumerate(sentence):\n",
    "        char_feature = []\n",
    "        # 前2\n",
    "        pre2 = sentence[i-2] if i>=2 else START_CHAR\n",
    "        # 前1\n",
    "        pre1 = sentence[i-1] if i>=1 else START_CHAR\n",
    "        # 当前\n",
    "        cur = char\n",
    "        # 后1\n",
    "        next1 = sentence[i+1] if i < len(sentence)-1 else END_CHAR\n",
    "        # 后2\n",
    "        next2 = sentence[i+2] if i < len(sentence)-2 else END_CHAR\n",
    "\n",
    "        # unigrams\n",
    "        one = pre1+'1'\n",
    "        if one in all_feature:\n",
    "            char_feature.append(all_feature[one])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% one)\n",
    "\n",
    "        two = cur+'2'\n",
    "        if two in all_feature:\n",
    "            char_feature.append(all_feature[two])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% two)\n",
    "\n",
    "        three = next1+'3'\n",
    "        if three in all_feature:\n",
    "            char_feature.append(all_feature[three])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% three)\n",
    "\n",
    "        # bigrams\n",
    "        four = pre2+'/'+pre1+'4'\n",
    "        if four in all_feature:\n",
    "            char_feature.append(all_feature[four])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% four)\n",
    "\n",
    "        five = pre1+'/'+cur+'5'\n",
    "        if five in all_feature:\n",
    "            char_feature.append(all_feature[five])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% five)\n",
    "\n",
    "        six = cur+'/'+next1+'6'\n",
    "        if six in all_feature:\n",
    "            char_feature.append(all_feature[six])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% six)\n",
    "\n",
    "        seven = next1+'/'+next2+'7'\n",
    "        if seven in all_feature:\n",
    "            char_feature.append(all_feature[seven])\n",
    "        else:\n",
    "            print('%s--特征没有在模型的特征中，权重设为0'% seven)\n",
    "        sentence_features.append(char_feature)\n",
    "    return sentence_features\n",
    "\n",
    "# 获取句子分词结果\n",
    "def get_sentence_seg_result(sentence, predict_tag):\n",
    "    seg_list = []\n",
    "    word = ''\n",
    "    for i, tag in enumerate(predict_tag):\n",
    "        if tag == 'B':\n",
    "            seg_list.append(word)\n",
    "            word = ''\n",
    "        \n",
    "# 预测\n",
    "sentence = '此外 还要 定期 到 全国 各省 去 访问 。  '.replace(' ','')\n",
    "sentence_feature = get_sentence_features(sentence)\n",
    "predict_tag = viterbi_decode(sentence_feature, W)\n",
    "print(sentence)\n",
    "print(predict_tag)\n",
    "print(sentence_tag_dict[sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练速度提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
