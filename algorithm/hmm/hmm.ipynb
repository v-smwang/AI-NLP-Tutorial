{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、马尔可夫性质\n",
    "\n",
    "&emsp;&emsp;马尔科夫性质——当前的状态只和上一时刻有关，在上一时刻之前的任何状态都和我无关。我们称其符合马尔可夫性质。\n",
    "\n",
    "&emsp;&emsp;具有马尔科夫性质的状态满足下面公式：\n",
    "\n",
    "&emsp;&emsp;$P(S_{t+1}|S_1,S_2,...,S_t) = P(S_{t+1}|S_t)$\n",
    "\n",
    "&emsp;&emsp;根据公式也就是说给定当前状态$S_t$，将来的状态与t时刻之前的状态已经没有关系。\n",
    "\n",
    "# 二、马尔可夫链\n",
    "\n",
    "&emsp;&emsp;马尔可夫链是指具有马尔可夫性质的随机过程。在过程中，在给定当前信息的情况下，过去的信息状态对于预测将来状态是无关的。——可以称之为马尔科夫无后效性。\n",
    "\n",
    "<img src=\"./imgs/markov_chain.png\" alt=\"马尔科夫链\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、隐马尔可夫模型\n",
    "\n",
    "&emsp;&emsp;隐马尔可夫模型(Hidden Markov Model, HMM)是一种统计模型，在语音识别、行为识别、NLP、故障诊断等领域具有高效的性能。\n",
    "\n",
    "&emsp;&emsp;HMM是关于时序的概率模型，描述一个含有未知参数的马尔可夫链所生成的不可观测的状态随机序列，再由各个状态生成观测随机序列的过程。\n",
    "\n",
    "&emsp;&emsp;HMM是一个双重随机过程---具有一定状态的隐马尔可夫链和随机的观测序列。\n",
    "\n",
    "&emsp;&emsp;HMM随机生成的状态随机序列被称为状态序列；每个状态生成一个观测，由此产生的观测随机序列，被称为观测序列。\n",
    "\n",
    "&emsp;&emsp;<font color=\"red\">举个例子来理解下hmm，我们使用输入法打字，敲出的每个字符就是观测序列，而实际我们想写的话就是隐藏序列。那不同公司的输入法都是在努力提高猜测咱们要写的话的能力。</font>\n",
    "\n",
    "## 1、定义\n",
    "\n",
    "<img src=\"./imgs/hmm.PNG\" alt=\"隐马尔科夫模型\" width=\"800\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：\n",
    "\n",
    "<span style=\"text-align:center;display:block;\">$Q={q_1,q_2,...,q_N},V={v_1,v_2,...v_M}$</span>\n",
    "\n",
    "&emsp;&emsp;其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。\n",
    "\n",
    "&emsp;&emsp;对于一个长度为T的序列，I对应的状态序列, O是对应的观察序列，即：\n",
    "\n",
    "<span style=\"text-align:center;display:block;\">$I={i_1,i_2,...,i_T},O={o_1,o_2,...o_T}$</span>\n",
    "\n",
    "&emsp;&emsp;其中，任意一个隐藏状态$i_t∈Q$,任意一个观察状态$o_t∈V$\n",
    "\n",
    "&emsp;&emsp;HMM模型做了两个很重要的假设如下：\n",
    "\n",
    "1. 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，这个我们在MCMC(二)马尔科夫链中有详细讲述。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻t的隐藏状态是$i_t=q_i$,在时刻t+1的隐藏状态是$i_{t+1}=q_j$, 则从时刻t到时刻t+1的HMM状态转移概率$a_{ij}$可以表示为：\n",
    "\n",
    "<span style=\"text-align:center;display:block;\">$a_{ij}=P(i_{t+1}=q_j|i_t=q_i)$</span>\n",
    "\n",
    "&emsp;&emsp;这样$a_{ij}$可以组成马尔科夫链的状态转移矩阵A:\n",
    "     \n",
    "<span style=\"text-align:center;display:block;\">$A=[a_{ij}]_{N×N}$</span>\n",
    "\n",
    "2. 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t的隐藏状态是$i_t=q_j$, 而对应的观察状态为$o_t=v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$,满足：\n",
    "    \n",
    "<span style=\"text-align:center;display:block;\">$b_j(k)=P(o_t=v_k|i_t=q_j)$</span>\n",
    "\n",
    "&emsp;&emsp;这样$b_j(k)$可以组成观测状态生成的概率矩阵B:\n",
    "    \n",
    "<span style=\"text-align:center;display:block;\">$B=[b_j(k)]_{N×M}$</span>\n",
    "\n",
    "&emsp;&emsp;除此之外，我们需要一组在时刻t=1的隐藏状态概率分布Π:\n",
    "    \n",
    "<span style=\"text-align:center;display:block;\">$Π=[π(i)]_N$</span>&emsp;&emsp;其中$π(i)=P(i_1=q_i)$\n",
    "\n",
    "&emsp;&emsp;一个HMM模型，可以由隐藏状态初始概率分布Π, 状态转移概率矩阵A和观测状态概率矩阵B决定。Π,A决定状态序列，B决定观测序列。因此，HMM模型可以由一个三元组λ表示如下：\n",
    "    \n",
    "<span style=\"text-align:center;display:block;\">$λ=(A,B,Π)$</span>\n",
    "\n",
    "## 2、hmm能解决的问题\n",
    "\n",
    "1. 评估观察序列概率。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，计算在模型$λ$下观测序列O出现的概率$P(O|λ)$。这个问题的求解需要用到前向后向算法。\n",
    "\n",
    "2. 模型参数学习问题。即给定观测序列$O={o_1,o_2,...o_T}$，估计模型$λ=(A,B,Π)$的参数，使该模型下观测序列的条件概率$P(O|λ)$最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。\n",
    "\n",
    "3. 预测问题，也称为解码问题。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。【预测出的每一个隐状态都有N个值，每个值对应一个概率值；例如句子分词，那么就需要利用维特比算法找到整个句子的最大隐状态概率对应的隐状态路径。】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、评估观察序列概率\n",
    "\n",
    "## 1、求观测序列的概率\n",
    "\n",
    "这个问题是这样的。我们已知HMM模型的参数𝜆=(𝐴,𝐵,Π)。其中𝐴是隐藏状态转移概率的矩阵，𝐵是观测状态生成概率的矩阵， Π是隐藏状态的初始概率分布。同时我们也已经得到了观测序列$O =\\{o_1,o_2,...o_T\\}$,现在我们要求观测序列𝑂在模型𝜆下出现的条件概率𝑃(𝑂|𝜆)。\n",
    "    \n",
    "乍一看，这个问题很简单。因为我们知道所有的隐藏状态之间的转移概率和所有从隐藏状态到观测状态生成概率，那么我们是可以暴力求解的。\n",
    "\n",
    "我们可以列举出所有可能出现的长度为𝑇的隐藏序列$I = \\{i_1,i_2,...,i_T\\}$,分布求出这些隐藏序列与观测序列$O =\\{o_1,o_2,...o_T\\}$的联合概率分布𝑃(𝑂,𝐼|𝜆)，这样我们就可以很容易的求出边缘分布𝑃(𝑂|𝜆)了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体暴力求解的方法是这样的：首先，任意一个隐藏序列$I = \\{i_1,i_2,...,i_T\\}$出现的概率是：\n",
    "\n",
    "$$P(I|\\lambda) = \\pi_{i_1} a_{i_{1}i_{2}} a_{i_{2}i_{3}}... a_{i_{T-1}\\;\\;i_T}$$\n",
    "\n",
    "对于固定的状态序列$I = \\{i_1,i_2,...,i_T\\}$，我们要求的观察序列$O =\\{o_1,o_2,...o_T\\}$出现的概率是：\n",
    "\n",
    "$$P(O|I, \\lambda) = b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$$\n",
    "\n",
    "则𝑂和𝐼联合出现的概率是：\n",
    "\n",
    "$$P(O,I|\\lambda) = P(I|\\lambda)P(O|I, \\lambda) = \\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}\\;\\;i_T}b_{i_T}(o_T)$$\n",
    "\n",
    "然后求边缘概率分布，即可得到观测序列𝑂在模型𝜆下出现的条件概率𝑃(𝑂|𝜆)，但是我们需要计算所有的T个隐状态、N种隐状态的自由组合，即$T^N$种组合的概率然后累加：\n",
    "\n",
    ">边缘分布（Marginal Distribution）指在概率论和统计学的多维随机变量中，只包含其中部分变量的概率分布。\\\n",
    ">\\\n",
    ">**定义** \\\n",
    ">假设有一个和两个变量相关的概率分布：\\\n",
    ">$P(x,y)$\\\n",
    ">关于其中一个特定变量的边缘分布则为给定其他变量的条件概率分布：\\\n",
    ">$P(x)=\\sum_yP(x,y)=\\sum_yP(x|y)P(y)$ \\\n",
    ">解释：让y取所有的可能值，然后求出只有x变量的概率；举个例子：x为肤色（黑、白、黄）y为性别（男、女）那么$P(x)=P(x|男)+P(x|女)$ \\\n",
    ">参阅Wikipedia举例，下图中，X和Y遵从绿圈内所示的二元正态分布，红线和蓝线分别表示Y变量和X变量的边缘分布 \\\n",
    "><img src=\"imgs/marginal_distributions.png\" width=\"300\"/>\n",
    ">在这个边缘分布中，我们得到只关于一个变量的概率分布，而不再考虑另一变量的影响，实际上进行了降维操作。在实际应用中，例如人工神经网络的神经元互相关联，在计算它们各自的参数的时候，就会使用边缘分布计算得到某一特定神经元（变量）的值。\n",
    "\n",
    "$$P(O|\\lambda) = \\sum\\limits_{I}P(O,I|\\lambda)  = \\sum\\limits_{i_1,i_2,...i_T}\\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}\\;\\;i_T}b_{i_T}(o_T)$$\n",
    "\n",
    "虽然上述方法有效，但是如果我们的隐藏状态数𝑁非常多的那就麻烦了，此时我们预测状态有$N^T$种组合，算法的时间复杂度是$𝑂(𝑇𝑁^𝑇)$阶的。因此对于一些隐藏状态数极少的模型，我们可以用暴力求解法来得到观测序列出现的概率，但是如果隐藏状态多，则上述算法太耗时，我们需要寻找其他简洁的算法。\n",
    "\n",
    "前向后向算法就是来帮助我们在较低的时间复杂度情况下求解这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、前向算法求HMM观测序列的概率\n",
    "\n",
    "前向后向算法是前向算法和后向算法的统称，这两个算法都可以用来求HMM观测序列的概率。我们先来看看前向算法是如何求解这个问题的。\n",
    "\n",
    "前向算法本质上属于动态规划的算法，也就是我们要通过找到局部状态递推的公式，这样一步步的从子问题的最优解拓展到整个问题的最优解。\n",
    "\n",
    "在前向算法中，通过定义“前向概率”来定义动态规划的这个局部状态。什么是前向概率呢, 其实定义很简单：<font color=\"red\">定义时刻𝑡时隐藏状态为$q_i$, 观测状态的序列为$𝑜_1,𝑜_2,...𝑜_𝑡$的概率为前向概率。记为：</font>\n",
    "\n",
    "$$\\alpha_t(i) = P(o_1,o_2,...o_t, i_t =q_i | \\lambda)\\ \\ \\ \\ 注意：字母是alpha不是a$$\n",
    "\n",
    "既然是动态规划，我们就要递推了，现在我们假设我们已经找到了在时刻𝑡时各个隐藏状态的前向概率，现在我们需要递推出时刻𝑡+1时各个隐藏状态的前向概率。\n",
    "\n",
    "<img src=\"./imgs/forward_algorithm.png\" alt=\"前向算法\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，我们可以基于时刻𝑡时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即$\\alpha_t(j)a_{ji}$就是在时刻𝑡观测到$o_1,o_2,...o_t$，并且时刻𝑡隐藏状态$𝑞_𝑗$, 时刻𝑡+1隐藏状态$𝑞_i$的概率。表示为：\n",
    "\n",
    "$$P(o_1,o_2,...o_t,i_t=q_j,i_{t+1}=q_i|\\lambda)=\\alpha_t(j)a_{ji}$$\n",
    "\n",
    "如果将想上面所有的线对应的概率求和，即$\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}$就是在时刻𝑡观测到$o_1,o_2,...o_t$，并且时刻𝑡+1隐藏状态$𝑞_i$的概率。表示为：\n",
    "\n",
    "$$P(o_1,o_2,...o_t,i_{t+1}=q_i|\\lambda)=\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}$$\n",
    "\n",
    "继续一步，由于观测状态$O_{𝑡+1}$只依赖于𝑡+1时刻隐藏状态$𝑞_𝑖$, 这样$[\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}]b_i(o_{t+1})$就是在在时刻𝑡+1观测到$o_1,o_2,...o_t，o_{t+1}$，并且时刻𝑡+1隐藏状态𝑞𝑖的概率。表示为：\n",
    "\n",
    "$$P(o_1,o_2,...o_t,o_{t+1},i_{t+1}=q_i|\\lambda)=[\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}]b_i(o_{t+1})$$\n",
    "\n",
    "而这个概率，恰恰就是时刻𝑡+1对应的隐藏状态𝑖的前向概率，这样我们得到了前向概率的递推关系式如下：\n",
    "\n",
    "$$\\alpha_{t+1}(i) = \\Big[\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}\\Big]b_i(o_{t+1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的动态规划从时刻1开始，到时刻𝑇结束，由于$\\alpha_T(i)$表示在时刻𝑇观测序列为$o_1,o_2,...o_T$，并且时刻𝑇隐藏状态$𝑞_𝑖$的概率，我们只要将所有隐藏状态对应的概率相加，即$\\sum\\limits_{i=1}^N\\alpha_T(i)$就得到了在时刻𝑇观测序列为$o_1,o_2,...o_T$的概率。\n",
    "\n",
    "下面总结下前向算法。\n",
    "\n",
    "输入：HMM模型𝜆=(𝐴,𝐵,Π)，观测序列𝑂=($o_1,o_2,...o_T$)\n",
    "\n",
    "输出：观测序列概率𝑃(𝑂|𝜆)\n",
    "\n",
    "1) 计算时刻1的各个隐藏状态前向概率：\n",
    "\n",
    "$$\\alpha_1(i) = \\pi_ib_i(o_1),\\; i=1,2,...N$$\n",
    "\n",
    "2) 递推时刻2,3,...𝑇时刻的前向概率（递推关系从时刻1往时刻T推，由t时刻的前向概率可以求出t+1时刻的前向概率）：\n",
    "\n",
    "$$\\alpha_{t+1}(i) = \\Big[\\sum\\limits_{j=1}^N\\alpha_t(j)a_{ji}\\Big]b_i(o_{t+1}),\\; i=1,2,...N$$\n",
    "\n",
    "3) 计算最终结果：\n",
    "\n",
    "$$P(O|\\lambda) = \\sum\\limits_{i=1}^N\\alpha_T(i)$$\n",
    "\n",
    "从递推公式可以看出，我们的算法时间复杂度是$𝑂(𝑇𝑁^2)$，比暴力解法的时间复杂度$𝑂(𝑇𝑁^𝑇)$少了几个数量级。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、HMM前向算法求解实例\n",
    "\n",
    "\n",
    "我们的观察集合是:\n",
    "\n",
    "$$𝑉={红，白}，𝑀=2$$\n",
    "\n",
    "我们的状态集合是：\n",
    "\n",
    "$$𝑄=\\{盒子1，盒子2，盒子3\\}，𝑁=3$$\n",
    "\n",
    "而观察序列和状态序列的长度为3.\n",
    "\n",
    "初始状态分布为：\n",
    "\n",
    "$$\\Pi = (0.2,0.4,0.4)^T$$\n",
    "\n",
    "状态转移概率分布矩阵为：\n",
    "\n",
    "$$A = \\left( \\begin{array} {ccc} 0.5 & 0.2 & 0.3 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0.2 & 0.3 &0.5 \\end{array} \\right)$$\n",
    "\n",
    "观测状态概率矩阵为：\n",
    "\n",
    "$$B = \\left( \\begin{array} {ccc} 0.5 & 0.5 \\\\ 0.4 & 0.6 \\\\ 0.7 & 0.3 \\end{array} \\right)$$\n",
    "\n",
    "球的颜色的观测序列:\n",
    "\n",
    "$$O=\\{红，白，红\\}$$\n",
    "\n",
    "首先计算时刻1三个状态的前向概率：\n",
    "\n",
    "时刻1是红色球，隐藏状态是盒子1的概率为：\n",
    "\n",
    "$$\\alpha_1(1) = \\pi_1b_1(o_1) = 0.2 \\times 0.5 = 0.1$$\n",
    "\n",
    "隐藏状态是盒子2的概率为：\n",
    "\n",
    "$$\\alpha_1(2) = \\pi_2b_2(o_1) = 0.4 \\times 0.4 = 0.16$$\n",
    "\n",
    "隐藏状态是盒子3的概率为：\n",
    "\n",
    "$$\\alpha_1(3) = \\pi_3b_3(o_1) = 0.4 \\times 0.7 = 0.28$$\n",
    "\n",
    "现在我们可以开始递推了，首先递推时刻2三个状态的前向概率：\n",
    "时刻2是白色球，隐藏状态是盒子1的概率为：\n",
    "\n",
    "$$\\alpha_2(1) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_1(i)a_{i1}\\Big]b_1(o_2) = [0.1*0.5+0.16*0.3+0.28*0.2 ] \\times 0.5 = 0.077$$\n",
    "\n",
    "隐藏状态是盒子2的概率为：\n",
    "\n",
    "$$\\alpha_2(2) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_1(i)a_{i2}\\Big]b_2(o_2) = [0.1*0.2+0.16*0.5+0.28*0.3 ] \\times 0.6 = 0.1104$$\n",
    "\n",
    "隐藏状态是盒子3的概率为：\n",
    "\n",
    "$$\\alpha_2(3) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_1(i)a_{i3}\\Big]b_3(o_2) = [0.1*0.3+0.16*0.2+0.28*0.5 ] \\times 0.3 = 0.0606$$\n",
    "\n",
    "继续递推，现在我们递推时刻3三个状态的前向概率：\n",
    "时刻3是红色球，隐藏状态是盒子1的概率为：\n",
    "\n",
    "$$\\alpha_3(1) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_2(i)a_{i1}\\Big]b_1(o_3) = [0.077*0.5+0.1104*0.3+0.0606*0.2 ] \\times 0.5 = 0.04187$$\n",
    "\n",
    "隐藏状态是盒子2的概率为：\n",
    "\n",
    "$$\\alpha_3(2) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_2(i)a_{i2}\\Big]b_2(o_3) = [0.077*0.2+0.1104*0.5+0.0606*0.3 ] \\times 0.4 = 0.03551$$\n",
    "\n",
    "隐藏状态是盒子3的概率为：\n",
    "\n",
    "$$\\alpha_3(3) =  \\Big[\\sum\\limits_{i=1}^3\\alpha_2(i)a_{i3}\\Big]b_3(o_3) = [0.077*0.3+0.1104*0.2+0.0606*0.5 ] \\times 0.7 = 0.05284$$\n",
    "\n",
    "最终我们求出观测序列:$𝑂=\\{红，白，红\\}$的概率为：\n",
    "\n",
    "$$P(O|\\lambda) = \\sum\\limits_{i=1}^3\\alpha_3(i) = 0.13022$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、后向算法求HMM观测序列的概率\n",
    "\n",
    "后向算法和前向算法非常类似，都是用的动态规划，唯一的区别是选择的局部状态不同，后向算法用的是“后向概率”，那么后向概率是如何定义的呢？\n",
    "\n",
    "<font color=\"red\">定义时刻𝑡时隐藏状态为$𝑞_𝑖$, 从时刻𝑡+1到最后时刻𝑇的观测状态的序列为$𝑜_{𝑡+1},𝑜_{𝑡+2},...𝑜_𝑇$的概率为后向概率。记为：</font>\n",
    "\n",
    "$$\\beta_t(i) = P(o_{t+1},o_{t+2},...o_T| i_t =q_i , \\lambda)$$\n",
    "\n",
    "<img src=\"./imgs/backward_algorithm.png\" width=\"300\" alt=\"后向算法\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后向概率的动态规划递推公式和前向概率是相反的。现在我们假设我们已经找到了在时刻𝑡+1时各个隐藏状态的后向概率$𝛽_{𝑡+1}(𝑗)$，表示为：\n",
    "\n",
    "$$\\beta_{t+1}(j) = P(o_{t+2},o_{t+3}...o_T| i_{t+1} =q_j , \\lambda)$$\n",
    "\n",
    "现在我们需要递推出时刻𝑡时各个隐藏状态的后向概率。如上图，我们可以计算出观测状态的序列为$o_{𝑡+2},o_{𝑡+3},...o_𝑇$， 𝑡时隐藏状态为$𝑞_𝑖$, 时刻𝑡+1隐藏状态为$𝑞_𝑗$的概率为$𝑎_{𝑖𝑗}𝛽_{𝑡+1}(𝑗)$, 表示为：\n",
    "\n",
    "$$P(o_{𝑡+2},o_{𝑡+3},...o_𝑇|i_t=q_i,i_{t+1}=q_j,\\lambda)=𝑎_{𝑖𝑗}𝛽_{𝑡+1}(𝑗)$$\n",
    "\n",
    "接着可以得到观测状态的序列为$o_{𝑡+1},o_{𝑡+2},...o_𝑇$， 𝑡时隐藏状态为$𝑞_𝑖$, 时刻𝑡+1隐藏状态为$𝑞_𝑗$的概率为$a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$, 表示为：\n",
    "\n",
    "$$P(o_{𝑡+1},o_{𝑡+2},...o_𝑇|i_t=q_i,i_{t+1}=q_j,\\lambda)=a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$\n",
    "\n",
    "则把上面所有线对应的概率加起来，我们可以得到观测状态的序列为$o_{𝑡+1},o_{𝑡+2},...o_𝑇$， 𝑡时隐藏状态为$𝑞_𝑖$的概率为$\\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$，这个概率即为时刻𝑡的后向概率。表示为：\n",
    "\n",
    "$$P(o_{𝑡+1},o_{𝑡+2},...o_𝑇|i_t=q_i,\\lambda)=\\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们得到了后向概率的递推关系式如下：\n",
    "$$\\beta_{t}(i) = \\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$\n",
    "现在我们总结下后向算法的流程,注意下和前向算法的相同点和不同点：\n",
    "\n",
    "输入：HMM模型𝜆=(𝐴,𝐵,Π)，观测序列$O=(o_1,o_2,...o_T)$\n",
    "\n",
    "输出：观测序列概率𝑃(𝑂|𝜆)\n",
    "\n",
    "1) 初始化时刻𝑇的各个隐藏状态后向概率：\n",
    "\n",
    "$$\\beta_T(i) = 1,\\; i=1,2,...N$$\n",
    "\n",
    "2) 递推时刻𝑇−1,𝑇−2,...1时刻的后向概率（递推关系从T时刻往T-1,T-2...1时刻推，利用t+1时刻的后向概率可以求t时刻的后向概率）：\n",
    "\n",
    "$$\\beta_{t}(i) = \\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),\\; i=1,2,...N$$\n",
    "\n",
    "3) 计算最终结果：\n",
    "\n",
    "$$P(O|\\lambda) = \\sum\\limits_{i=1}^N\\pi_ib_i(o_1)\\beta_1(i)$$\n",
    "\n",
    "此时我们的算法时间复杂度仍然是$𝑂(𝑇𝑁^2)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、HMM常用概率的计算\n",
    "\n",
    "利用前向概率和后向概率，我们可以计算出HMM中单个状态和两个状态的概率公式。\n",
    "\n",
    "1.给定模型𝜆和观测序列𝑂,在时刻𝑡处于状态$𝑞_𝑖$的概率记为:\n",
    "\n",
    "$$\\gamma_t(i) = P(i_t = q_i | O,\\lambda) = \\frac{P(i_t = q_i ,O|\\lambda)}{P(O|\\lambda)}$$\n",
    "\n",
    "那么$P(i_t = q_i ,O|\\lambda)$怎么求呢？\n",
    "\n",
    "我们先分析下这个概率求的是啥，\n",
    "\n",
    "它的意思是:在给定HMM的参数λ，观测序列是$O(O_1,O_2,...O_T)$,在t时刻的状态为$q_i$的概率。\n",
    "\n",
    "换句话理解就是:从1到t时刻，t时刻的状态为$q_i$,输出是$O(O_1,O_2,...O_T)$的概率P1；然后t时刻之后到T，在t时刻状态为$q_i$的基础上，输出序列是$o_{t+1},o_{t+2}..o_T$的概率P2，P1和P2相乘即为所求。\n",
    "\n",
    "碰巧P1符合前向概率定义，P2符合后向概率定义，所以 $P1=\\alpha_t(i) \\ \\ \\  P2=\\beta_t(i)$，则:\n",
    "\n",
    "$$P(i_t = q_i ,O|\\lambda) = P1*P2 = \\alpha_t(i)\\beta_t(i)$$\n",
    "\n",
    "那么：\n",
    "\n",
    "$$P(O|\\lambda) = \\sum_j^N \\alpha_t(j)\\beta_t(j) \\ \\ \\ 注意：为啥P(O|\\lambda)用这个表示，因为这样方便计算$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\gamma_t(i) = \\frac{ \\alpha_t(i)\\beta_t(i)}{\\sum\\limits_{j=1}^N \\alpha_t(j)\\beta_t(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.给定模型𝜆和观测序列𝑂,在时刻𝑡处于状态$𝑞_𝑖$，且时刻𝑡+1处于状态$𝑞_𝑗$的概率记为:\n",
    "\n",
    "$$\\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\\lambda) = \\frac{ P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)}{P(O|\\lambda)}$$\n",
    "\n",
    "那么$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)$怎么求呢？\n",
    "\n",
    "我们先分析下这个概率求的是啥，\n",
    "\n",
    "它的意思是：在给定HMM的参数下，观测序列是O，t时刻的状态是$q_i$，t+1时刻的状态是$q_j$的概率。\n",
    "\n",
    "这个概率=$P(O_1,O_2,...O_t,i_t=q_i|λ)*P(q_i转移到t+1时刻的q_j)*P(o_{t+2},o_{t+3}..o_T|i_{t+1}=q_j, λ)*P(t+1时刻输出是O_{t+1})$\n",
    "\n",
    "逐个解释下：\n",
    "\n",
    ">$P(O_1,O_2,...O_t,i_t=q_i|λ)$:这不就是t时刻的前向概率$\\alpha_t(i)$么\\\n",
    ">$P(q_i转移到t+1时刻的q_j)$:这不就是转移概率$a_{ij}$么\\\n",
    ">$P(o_{t+2},o_{t+3}..o_T|i_{t+1}=q_j, λ)$:这不就是t+1时刻的后向概率$\\beta_{t+1}(i)$么\\\n",
    ">(注意:t+1时刻的后续输出序列是$o_{t+2},o_{t+3}..o_T$,即下标是从t+2开始，不是t+1开始，请仔细理解后向概率的定义)。\\\n",
    ">前面少了$O_{t+1}$这个时间点的观测序列，因此要补上$P(t+1时刻输出是O_{t+1})$,而这个不就是$b_{j}(O_{t+1})$么\n",
    "\n",
    "所以：\n",
    "\n",
    "$$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda) = \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以：\n",
    "\n",
    "$$P(O|\\lambda)=\\sum\\limits_{r=1}^N\\sum\\limits_{s=1}^N\\alpha_t(r)a_{rs}b_s(o_{t+1})\\beta_{t+1}(s)$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\xi_t(i,j) = \\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum\\limits_{r=1}^N\\sum\\limits_{s=1}^N\\alpha_t(r)a_{rs}b_s(o_{t+1})\\beta_{t+1}(s)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.将$𝛾_𝑡(𝑖)$和$𝜉_𝑡(𝑖,𝑗)$在各个时刻𝑡求和，可以得到：\n",
    "\n",
    "在观测序列𝑂下状态𝑖出现的期望值$\\sum_{𝑡=1}^𝑇𝛾_𝑡(𝑖)$\n",
    "\n",
    "在观测序列𝑂下由状态𝑖转移的期望值$\\sum_{𝑡=1}^{𝑇-1}𝛾_𝑡(𝑖)$\n",
    "\n",
    "在观测序列𝑂下由状态𝑖转移到状态𝑗的期望值$\\sum_{𝑡=1}^{𝑇-1}𝜉_𝑡(𝑖,𝑗)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、模型参数学习问题\n",
    "\n",
    "## 1、HMM模型参数求解概述\n",
    "\n",
    "HMM模型参数求解根据已知的条件可以分为两种情况。\n",
    "\n",
    "第一种情况较为简单，就是我们已知𝐷个长度为𝑇的观测序列和对应的隐藏状态序列，即${(𝑂_1,𝐼_1),(𝑂_2,𝐼_2),...(𝑂_𝐷,𝐼_𝐷)}$是已知的，此时我们可以很容易的用最大似然来求解模型参数。\n",
    "\n",
    "假设样本从隐藏状态$𝑞_𝑖$转移到$𝑞_𝑗$的频率计数是$𝐴_{𝑖𝑗}$,那么状态转移矩阵求得为：\n",
    "\n",
    "$$A = \\Big[a_{ij}\\Big], \\;其中a_{ij} = \\frac{A_{ij}}{\\sum\\limits_{s=1}^{N}A_{is}}$$\n",
    "\n",
    "假设样本隐藏状态为$𝑞_𝑗$且观测状态为$𝑣_𝑘$的频率计数是$𝐵_{𝑗𝑘}$,那么观测状态概率矩阵为：\n",
    "\n",
    "$$B= \\Big[b_{j}(k)\\Big], \\;其中b_{j}(k) = \\frac{B_{jk}}{\\sum\\limits_{s=1}^{M}B_{js}}$$\n",
    "\n",
    "假设所有样本中初始隐藏状态为$𝑞_𝑖$的频率计数为$𝐶(𝑖)$,那么初始概率分布为：\n",
    "\n",
    "$$\\Pi = \\pi(i) = \\frac{C(i)}{\\sum\\limits_{s=1}^{N}C(s)}$$\n",
    "\n",
    "可见第一种情况下求解模型还是很简单的。但是在很多时候，我们无法得到HMM样本观察序列对应的隐藏序列，只有𝐷个长度为𝑇的观测序列，即${(𝑂_1),(𝑂_2),...(𝑂_𝐷)}$是已知的，此时我们能不能求出合适的HMM模型参数呢？这就是我们的第二种情况，也是我们本文要讨论的重点。它的解法最常用的是鲍姆-韦尔奇算法，其实就是基于EM算法的求解，只不过鲍姆-韦尔奇算法出现的时代，EM算法还没有被抽象出来，所以我们本文还是说鲍姆-韦尔奇算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、鲍姆-韦尔奇（Baum-Welch）算法原理\n",
    "\n",
    "　鲍姆-韦尔奇算法原理既然使用的就是EM算法【（Expectation-Maximum）算法也称期望最大化算法】的原理，那么我们需要在E步求出联合分布𝑃(𝑂,𝐼|𝜆)基于条件概率$P(I|O,\\overline{\\lambda})$的期望，其中$\\overline{\\lambda}$为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数𝜆。接着不停的进行EM迭代，直到模型参数的值收敛为止。\n",
    "\n",
    "首先来看看E步，当前模型参数为$\\overline{\\lambda}$, 联合分布𝑃(𝑂,𝐼|𝜆)基于条件概率$P(I|O,\\overline{\\lambda})$的期望表达式为：\n",
    "\n",
    "$$L(\\lambda, \\overline{\\lambda}) = \\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "\n",
    "在M步，我们极大化上式，然后得到更新后的模型参数如下：　\n",
    "\n",
    "$$\\overline{\\lambda} = arg\\;\\max_{\\lambda}\\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "\n",
    "通过不断的E步和M步的迭代，直到$\\overline{\\lambda}$收敛。下面我们来看看鲍姆-韦尔奇算法的推导过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、鲍姆-韦尔奇算法的推导\n",
    "\n",
    "我们的训练数据为$\\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\\}$，其中任意一个观测序列$O_d = \\{o_1^{(d)}, o_2^{(d)}, ... o_T^{(d)}\\}$,其对应的未知的隐藏状态序列表示为：$I_d = \\{i_1^{(d)}, i_2^{(d)}, ... i_T^{(d)}\\}$\n",
    "\n",
    "首先看鲍姆-韦尔奇算法的E步，我们需要先计算联合分布$𝑃(𝑂,𝐼|\\lambda)$的表达式如下：\n",
    "\n",
    "$$P(O,I|\\lambda) = \\prod_{d=1}^D \\pi_{i_1^{(d)}}\\ b_{i_1^{(d)}}\\ (o_1^{(d)})\\ a_{i_1^{(d)}\\ i_2^{(d)}}\\ b_{i_2^{(d)}}\\ (o_2^{(d)}) ... a_{i_{T-1}^{(d)} \\ \\ i_T^{(d)}}\\ b_{i_T^{(d)}}\\ \\ (o_T^{(d)})$$\n",
    "\n",
    "我们的E步得到的期望表达式为：\n",
    "\n",
    "$$L(\\lambda, \\overline{\\lambda}) = \\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "\n",
    "在M步我们要极大化上式。由于$P(I|O,\\overline{\\lambda}) = \\frac{P(I,O|\\overline{\\lambda})}{P(O|\\overline{\\lambda})}$,而$𝑃(𝑂|\\overline{\\lambda})$是常数，因此我们要极大化的式子等价于：\n",
    "\n",
    "$$\\overline{\\lambda} = arg\\;\\max_{\\lambda}\\sum\\limits_{I}P(O,I|\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "\n",
    "我们将上面$𝑃(𝑂,𝐼|𝜆)$的表达式带入我们的极大化式子，得到的表达式如下：\n",
    "\n",
    "$$\\overline{\\lambda} = arg\\;\\max_{\\lambda}\\sum\\limits_{d=1}^D\\sum\\limits_{I}P(O,I|\\overline{\\lambda})(log\\pi_{i_1} + \\sum\\limits_{t=1}^{T-1}log\\;a_{i_t,i_{t+1}} +  \\sum\\limits_{t=1}^Tlog b_{i_t}(o_t))$$\n",
    "\n",
    "我们的隐藏模型参数$𝜆=(𝐴,𝐵,Π)$,因此下面我们只需要对上式分别对$𝐴,𝐵,Π$求导即可得到我们更新的模型参数$\\overline{\\lambda}$\n",
    "\n",
    "首先我们看看对模型参数Π的求导。由于Π只在上式中括号里的第一部分出现，因此我们对于Π的极大化式子为：\n",
    "\n",
    "$$\\overline{\\pi_i} = arg\\;\\max_{\\pi_{i_1}} \\sum\\limits_{d=1}^D\\sum\\limits_{I}P(O,I|\\overline{\\lambda})log\\pi_{i_1} = arg\\;\\max_{\\pi_{i}} \\sum\\limits_{d=1}^D\\sum\\limits_{i=1}^NP(O,i_1^{(d)} =i|\\overline{\\lambda})log\\pi_{i}$$\n",
    "\n",
    "由于$𝜋_𝑖$还满足$\\sum\\limits_{i=1}^N\\pi_i =1$，因此根据拉格朗日子乘法，我们得到𝜋𝑖要极大化的拉格朗日函数为：\n",
    "\n",
    "$$arg\\;\\max_{\\pi_{i}}\\sum\\limits_{d=1}^D\\sum\\limits_{i=1}^NP(O,i_1^{(d)} =i|\\overline{\\lambda})log\\pi_{i} + \\gamma(\\sum\\limits_{i=1}^N\\pi_i -1)$$\n",
    "\n",
    "其中，𝛾为拉格朗日系数。上式对$𝜋_𝑖$求偏导数并令结果为0， 我们得到：\n",
    "\n",
    "$$\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda}) + \\gamma\\pi_i = 0$$\n",
    "\n",
    "令𝑖分别等于从1到𝑁，从上式可以得到𝑁个式子，对这𝑁个式子求和可得：\n",
    "\n",
    "$$\\sum\\limits_{d=1}^DP(O|\\overline{\\lambda}) + \\gamma = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上两式消去𝛾,得到$𝜋_𝑖$的表达式为：\n",
    "\n",
    "$$\\pi_i =\\frac{\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda})}{\\sum\\limits_{d=1}^DP(O|\\overline{\\lambda})} = \\frac{\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda})}{DP(O|\\overline{\\lambda})} = \\frac{\\sum\\limits_{d=1}^DP(i_1^{(d)} =i|O, \\overline{\\lambda})}{D} =  \\frac{\\sum\\limits_{d=1}^DP(i_1^{(d)} =i|O^{(d)}, \\overline{\\lambda})}{D}$$\n",
    "\n",
    "利用前向概率的定义可得：\n",
    "\n",
    "$$P(i_1^{(d)} =i|O^{(d)}, \\overline{\\lambda}) = \\gamma_1^{(d)}(i)$$\n",
    "\n",
    "因此最终我们在M步$𝜋_𝑖$的迭代公式为：\n",
    "\n",
    "$$\\pi_i =  \\frac{\\sum\\limits_{d=1}^D\\gamma_1^{(d)}(i)}{D}$$\n",
    "\n",
    "现在我们来看看𝐴的迭代公式求法。方法和Π的类似。由于𝐴只在最大化函数式中括号里的第二部分出现，而这部分式子可以整理为：\n",
    "\n",
    "$$\\sum\\limits_{d=1}^D\\sum\\limits_{I}\\sum\\limits_{t=1}^{T-1}P(O,I|\\overline{\\lambda})log\\;a_{i_t,i_{t+1}} = \\sum\\limits_{d=1}^D\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N\\sum\\limits_{t=1}^{T-1}P(O,i_t^{(d)} = i, i_{t+1}^{(d)} = j|\\overline{\\lambda})log\\;a_{ij}$$\n",
    "\n",
    "由于$𝑎_{𝑖𝑗}$还满足$\\sum\\limits_{j=1}^Na_{ij} =1$。和求解𝜋𝑖类似，我们可以用拉格朗日子乘法并对$𝑎_{𝑖𝑗}$求导，并令结果为0，可以得到$𝑎_{𝑖𝑗}$的迭代表达式为：\n",
    "\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i, i_{t+1}^{(d)} = j|\\overline{\\lambda})}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i|\\overline{\\lambda})}$$\n",
    "\n",
    "利用前向概率的定义和$𝜉𝑡(𝑖,𝑗)$的定义可得到在M步$𝑎_{𝑖𝑗}$的迭代公式为：\n",
    "\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\xi_t^{(d)}(i,j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\gamma_t^{(d)}(i)}$$\n",
    "\n",
    "现在我们来看看𝐵的迭代公式求法。方法和Π的类似。由于𝐵只在最大化函数式中括号里的第三部分出现，而这部分式子可以整理为：\n",
    "\n",
    "$$\\sum\\limits_{d=1}^D\\sum\\limits_{I}\\sum\\limits_{t=1}^{T}P(O,I|\\overline{\\lambda})log\\;b_{i_t}(o_t) = \\sum\\limits_{d=1}^D\\sum\\limits_{j=1}^N\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})log\\;b_{j}(o_t)$$\n",
    "\n",
    "由于$𝑏_𝑗(𝑜_𝑡)$还满足$\\sum_{𝑘=1}^𝑀𝑏_𝑗(𝑜_𝑡=𝑣_𝑘)=1$。和求解$𝜋_𝑖$类似，我们可以用拉格朗日子乘法并对$𝑏_𝑗(𝑘)$求导，并令结果为0，得到$𝑏_𝑗(𝑘)$的迭代表达式为：\n",
    "\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})I(o_t^{(d)}=v_k)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})}$$\n",
    "\n",
    "其中$𝐼(𝑜_t^{(𝑑)}=𝑣_𝑘)$当且仅当$𝑜_t^{(𝑑)}=𝑣_𝑘$时为1，否则为0. 利用前向概率的定义可得$𝑏_𝑗(𝑜_𝑡)$的最终表达式为：\n",
    "\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1, o_t^{(d)}=v_k}^{T}\\gamma_t^{(d)}(j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}\\gamma_t^{(d)}(j)}$$\n",
    "\n",
    "有了$𝜋_𝑖,𝑎_{𝑖𝑗},𝑏_𝑗(𝑘)$的迭代公式，我们就可以迭代求解HMM模型参数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、鲍姆-韦尔奇算法流程总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概括总结下鲍姆-韦尔奇算法的流程。\n",
    "\n",
    "**输入：** 𝐷个观测序列样本${(𝑂_1),(𝑂_2),...(𝑂_𝐷)}$\n",
    "\n",
    "**输出：** HMM模型参数\n",
    "\n",
    "1)随机初始化所有的$𝜋_𝑖,𝑎_{𝑖𝑗},𝑏_𝑗(𝑘)$\n",
    "\n",
    "2)对于每个样本𝑑=1,2,...𝐷，用前向后向算法计算$\\gamma_t^{(d)}(i)，\\xi_t^{(d)}(i,j), t =1,2...T$\n",
    "\n",
    "3)更新模型参数：\n",
    "\n",
    "$$\\pi_i =  \\frac{\\sum\\limits_{d=1}^D\\gamma_1^{(d)}(i)}{D}$$\n",
    "\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\xi_t^{(d)}(i,j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\gamma_t^{(d)}(i)}$$\n",
    "\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1, o_t^{(d)}=v_k}^{T}\\gamma_t^{(d)}(j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}\\gamma_t^{(d)}(j)}$$\n",
    "\n",
    "4)如果$𝜋_𝑖,𝑎_{𝑖𝑗,}𝑏_𝑗(𝑘)$的值已经收敛，则算法结束，否则回到第2）步继续迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、预测问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、HMM最可能隐藏状态序列求解概述\n",
    "\n",
    "在HMM模型的解码问题中，给定模型$𝜆=(𝐴,𝐵,Π)$和观测序列$𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$，求给定观测序列O条件下，最可能出现的对应的状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$,<font color=\"red\">即$𝑃(𝐼^∗|𝑂)$要最大化</font>。\n",
    " \n",
    "一个可能的近似解法是求出观测序列𝑂在每个时刻𝑡最可能的隐藏状态$𝑖_𝑡^*$然后得到一个近似的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$。要这样近似求解不难，利用前向后向算法评估观察序列概率的定义：在给定模型𝜆和观测序列𝑂时，在时刻𝑡处于状态$𝑞_𝑖$的概率是$𝛾_𝑡(𝑖)$，这个概率可以通过HMM的前向算法与后向算法计算。这样我们有：\n",
    "\n",
    "$$i_t^* = arg \\max_{1 \\leq i \\leq N}[\\gamma_t(i)], \\; t =1,2,...T$$\n",
    "\n",
    "近似算法很简单，但是却不能保证预测的状态序列是整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况【也就是这样求解没有考虑到转移概率】。\n",
    "\n",
    "而维特比算法可以将HMM的状态序列作为一个整体来考虑，避免近似算法的问题【考虑转移概率】，下面我们来看看维特比算法进行HMM解码的方法。\n",
    "\n",
    "## 2、维特比算法概述\n",
    "\n",
    "维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。\n",
    "\n",
    "既然是动态规划算法，那么就需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。\n",
    "\n",
    "第一个局部状态是在时刻𝑡隐藏状态为𝑖所有可能的状态转移路径$i_1,i_2,...i_t$中的概率最大值。记为$𝛿_𝑡(𝑖)$:\n",
    "\n",
    "$$\\delta_t(i) = \\max_{i_1,i_2,...i_{t-1}}\\;P(i_t=i, i_1,i_2,...i_{t-1},o_t,o_{t-1},...o_1|\\lambda),\\; i =1,2,...N$$\n",
    "\n",
    "由$𝛿_𝑡(𝑖)$的定义可以得到𝛿的递推表达式：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\delta_{t+1}(i) & =  \\max_{i_1,i_2,...i_{t}}\\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\\lambda) \\tag{1}\\\\ \n",
    "& = \\max_{1 \\leq j \\leq N}\\;[\\delta_t(j)a_{ji}]b_i(o_{t+1}) \\tag{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个局部状态由第一个局部状态递推得到。我们定义在时刻𝑡隐藏状态为𝑖的所有单个状态转移路径$(i_1,i_2,...,i_{t-1},i)$中概率最大的转移路径中第𝑡−1个节点的隐藏状态为$Ψ_𝑡(𝑖)$,其递推表达式可以表示为：\n",
    "\n",
    "$$\\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}]$$\n",
    "\n",
    "有了这两个局部状态，我们就可以从时刻0一直递推到时刻𝑇，然后利用Ψ𝑡(𝑖)记录的前一个最可能的状态节点回溯，直到找到最优的隐藏状态序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、维特比算法流程总结\n",
    "\n",
    "总结下维特比算法的流程：\n",
    "\n",
    "**输入：** HMM模型𝜆=(𝐴,𝐵,Π)，观测序列$O=(o_1,o_2,...o_T)$\n",
    "\n",
    "**输出：** 最有可能的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$\n",
    "\n",
    "1）初始化局部状态【时刻1的各个隐藏状态前向概率和上一个时刻的所有最可能的隐状态】：\n",
    "\n",
    "$$\\delta_1(i) = \\pi_ib_i(o_1),\\;i=1,2...N$$\n",
    "\n",
    "$$\\Psi_1(i)=0,\\;i=1,2...N$$\n",
    "\n",
    "2)进行动态规划递推时刻𝑡=2,3,...𝑇时刻的局部状态：\n",
    "\n",
    "$$\\delta_{t}(i) = \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}]b_i(0_{t}),\\;i=1,2...N$$\n",
    "\n",
    "$$\\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}],\\;i=1,2...N$$\n",
    "\n",
    "3)计算时刻𝑇最大的$𝛿_𝑇(𝑖)$,即为最可能隐藏状态序列出现的概率。计算时刻𝑇最大的$Ψ_𝑡(𝑖)$,即为时刻𝑇最可能的隐藏状态。\n",
    "\n",
    "$$P* = \\max_{1 \\leq j \\leq N}\\delta_{T}(i)$$\n",
    "\n",
    "$$i_T^* = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{T}(i)]$$\n",
    "\n",
    "4)利用局部状态Ψ(𝑖)开始回溯。对于𝑡=𝑇−1,𝑇−2,...,1：\n",
    "\n",
    "$$i_t^* = \\Psi_{t+1}(i_{t+1}^*)$$\n",
    "\n",
    "最终得到最有可能的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、HMM维特比算法求解实例\n",
    "\n",
    "我们的观察集合是:\n",
    "\n",
    "$$𝑉={红，白}，𝑀=2$$\n",
    "\n",
    "我们的状态集合是：\n",
    "\n",
    "$$𝑄=\\{盒子1，盒子2，盒子3\\}，𝑁=3$$\n",
    "\n",
    "而观察序列和状态序列的长度为3.\n",
    "\n",
    "初始状态分布为：\n",
    "\n",
    "$$\\Pi = (0.2,0.4,0.4)^T$$\n",
    "\n",
    "状态转移概率分布矩阵为：\n",
    "\n",
    "$$A = \\left( \\begin{array} {ccc} 0.5 & 0.2 & 0.3 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0.2 & 0.3 &0.5 \\end{array} \\right)$$\n",
    "\n",
    "观测状态概率矩阵为：\n",
    "\n",
    "$$B = \\left( \\begin{array} {ccc} 0.5 & 0.5 \\\\ 0.4 & 0.6 \\\\ 0.7 & 0.3 \\end{array} \\right)$$\n",
    "\n",
    "球的颜色的观测序列:\n",
    "\n",
    "$$O=\\{红，白，红\\}$$\n",
    "\n",
    "首先计算时刻1三个状态的前向概率：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "首先需要得到三个隐藏状态在时刻1时对应的各自两个局部状态，此时观测状态为1：\n",
    "\n",
    "$$𝛿_1(1)=𝜋_1𝑏_1(𝑜_1)=0.2×0.5=0.1$$\n",
    "$$𝛿_1(2)=𝜋_2𝑏_2(𝑜_1)=0.4×0.4=0.16$$\n",
    "$$𝛿_1(3)=𝜋_3𝑏_3(𝑜_1)=0.4×0.7=0.28$$\n",
    "$$Ψ_1(1)=Ψ_1(2)=Ψ_1(3)=0$$\n",
    "\n",
    "现在开始递推三个隐藏状态在时刻2时对应的各自两个局部状态，此时观测状态为2：\n",
    "\n",
    "$$\\delta_2(1) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j1}]b_1(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.5, 0.16 \\times 0.3, 0.28\\times 0.2] \\times 0.5 = 0.028$$\n",
    "\n",
    "$$\\Psi_2(1)=3$$\n",
    "\n",
    "$$\\delta_2(2) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j2}]b_2(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.2, 0.16 \\times 0.5, 0.28\\times 0.3] \\times 0.6 = 0.0504$$\n",
    "\n",
    "$$\\Psi_2(2)=2$$\n",
    "\n",
    "$$\\delta_2(3) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j3}]b_3(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.3, 0.16 \\times 0.2, 0.28\\times 0.5] \\times 0.3 = 0.042$$\n",
    "\n",
    "$$\\Psi_2(3)=3$$\n",
    "\n",
    "继续递推三个隐藏状态在时刻3时对应的各自两个局部状态，此时观测状态为1：\n",
    "\n",
    "$$\\delta_3(1) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j1}]b_1(o_3) = \\max_{1\\leq j \\leq 3}[0.028 \\times 0.5, 0.0504 \\times 0.3, 0.042\\times 0.2] \\times 0.5 = 0.00756$$\n",
    "\n",
    "$$\\Psi_3(1)=2$$\n",
    "\n",
    "$$\\delta_3(2) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j2}]b_2(o_3) = \\max_{1\\leq j \\leq 3}[0.028  \\times 0.2, 0.0504\\times 0.5, 0.042\\times 0.3] \\times 0.4 = 0.01008$$\n",
    "\n",
    "$$\\Psi_3(2)=2$$\n",
    "\n",
    "$$\\delta_3(3) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j3}]b_3(o_3) = \\max_{1\\leq j \\leq 3}[0.028  \\times 0.3, 0.0504 \\times 0.2, 0.042\\times 0.5] \\times 0.7 = 0.0147$$\n",
    "\n",
    "$$\\Psi_3(3)=3$$\n",
    "\n",
    "此时已经到最后的时刻，我们开始准备回溯。此时最大概率为$𝛿_3(3)$,从而得到$𝑖_3^*=3$\n",
    "\n",
    "由于$Ψ_3(3)=3$,所以$𝑖_2^*=3$, 而又由于$Ψ_2(3)=3$,所以$𝑖_1^*=3$。从而得到最终的最可能的隐藏状态序列为：$(3,3,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、HMM模型维特比算法总结\n",
    "\n",
    "维特比算法也是寻找序列最短路径的一个通用方法，和dijkstra算法有些类似，但是dijkstra算法并没有使用动态规划，而是贪心算法。同时维特比算法仅仅局限于求序列最短路径，而dijkstra算法是通用的求最短路径的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
