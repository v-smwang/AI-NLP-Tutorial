{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zfoox.blog.csdn.net/article/details/103483863\n",
    "\n",
    "https://zfoox.blog.csdn.net/article/details/103544683\n",
    "\n",
    "https://zfoox.blog.csdn.net/article/details/103540312\n",
    "\n",
    "https://www.zhihu.com/question/40797593/answer/275171156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、什么是EM算法\n",
    "EM（Expectation-Maximum）算法也称期望最大化算法，曾入选“数据挖掘十大算法”中，可见EM算法在机器学习、数据挖掘中的影响力。EM算法是最常见的隐变量估计方法，在机器学习中有极为广泛的用途，例如常被用来学习高斯混合模型（Gaussian mixture model，简称GMM）的参数；隐式马尔科夫算法（HMM）、LDA主题模型的变分推断等等。\n",
    "\n",
    "# 二、EM算法原理\n",
    "\n",
    "EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation-Maximization Algorithm）。EM算法受到缺失思想影响，最初是为了解决数据缺失情况下的参数估计问题，其算法基础和收敛有效性等问题在Dempster、Laird和Rubin三人于1977年所做的文章《Maximum likelihood from incomplete data via the EM algorithm》中给出了详细的阐述。其基本思想是：首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。\n",
    "\n",
    "# 三、EM算法公式推导\n",
    "\n",
    "对于n个样本观察数据$x=\\left(x_{1}, x_{2}, \\ldots x_{n}\\right)$，找出样本的模型参数θ, 极大化模型分布的对数似然函数如下：\n",
    "$$\\hat{\\theta}=\\operatorname{argmax} \\sum_{i=1}^{n} \\log p\\left(x_{i} ; \\theta\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **极大似然估计** \\\n",
    "> 假设有一枚不均匀的硬币，抛10次，8次正面，估计抛硬币出现正面的概率。\\\n",
    "> 凭感觉就可以知道这个概率是0.8，其实这就是极大似然的思想。下面我们来细化我们的思路。\\\n",
    "> 假定抛出正面的概率为$\\theta$，那么抛出如题所述的这10次结果的概率为：\\\n",
    "> $$L(\\theta)=P(8正\\mid \\theta)=\\theta^{8} \\cdot(1-\\theta)^{2}$$ \\\n",
    "> 这个L就是似然函数。为了方便求导，我们对L取对数，得到对数似然函数LL，然后令LL的导数等于0，解得最优化的$\\theta$。\\\n",
    "> $$L L(\\theta)=\\log L(\\theta)=8 \\log \\theta+2 \\log (1-\\theta)$$ \\\n",
    "> $$\\frac{d L L}{d \\theta}=\\frac{8}{\\theta}-\\frac{2}{1-\\theta}=0 \\Longrightarrow \\theta=0.8$$ \\\n",
    "> 简单来说，每一组参数，都会对应一个观测结果出现的概率。极大似然估计，就是找出使得观测结果出现的概率最大的参数。\n",
    "\n",
    "现实中$\\theta$可能是多个分布的参数，也就是说这个$x$的分布并不是只属于一个分布的，而是多个分布的组合，例如混合高斯模型。如果是多个分布的参数，那么利用分别求偏导再令偏导等于0，去求参数值的方法就不好求了。我们需要引入隐含变量来简化问题，下面就是为了简化问题，而使用的方法，是别人反复研究的成果。\n",
    "\n",
    "\n",
    "如果我们得到的观察数据有未观察到的隐含数据$z=\\left(z_{1}, z_{2}, \\ldots, z_{n}\\right)$，即上文中每个样本属于哪个分布是未知的，此时我们极大化模型分布的对数似然函数如下：\n",
    "$$\\hat{\\theta}=\\operatorname{argmax} \\sum_{i=1}^{n} \\log p\\left(x_{i} ; \\theta\\right)=\\operatorname{argmax} \\sum_{i=1}^{n} \\log \\sum_{z_{i}} p\\left(x_{i}, z_{i} ; \\theta\\right)$$\n",
    "\n",
    "\n",
    "上面这个式子是根据$x_i$的边缘概率计算得来，引入$z$不影响结果，但仍然没有办法直接求出θ，再进一步进行变化，\n",
    "$$\\sum_{i=1}^{n} \\log \\sum_{z_{i}} p\\left(x_{i}, z_{i} ; \\theta\\right)=\\sum_{i=1}^{n} \\log \\sum_{z_{i}} Q_{i}\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}$$\n",
    "\n",
    "\n",
    "此时我们发现，有些规律可循了，$\\sum_{z_{i}} Q_{i}\\left(z_{i}\\right) \\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}$是个期望，因为对于某离散随机变量X来说，其数学期望$\\mathrm{E}[X]=\\sum_{i} p_{i} x_{i}$。竟然是个期望我们就可以利用Jensen不等式进一步往下推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Jensen不等式**\\\n",
    ">（1）定义 \\\n",
    "> 设f是定义域为实数的函数，如果对所有的实数x，f(x)的二阶导数都大于0，那么f是凸函数。\\\n",
    "> Jensen不等式定义如下：\\\n",
    "> 如果f是凸函数，X是随机变量，那么：$E[f(X)] \\geq f(E[X])$。当且仅当X是常量时，该式取等号。其中，E(X)表示X的数学期望。\\\n",
    "> 注：Jensen不等式应用于凹函数时，不等号方向反向。当且仅当x是常量时，该不等式取等号。\\\n",
    ">（2）举例\\\n",
    "><img src=\"./imgs/jensen_inequation.png\" alt=\"jensen不等式图\" width=\"300\"/> \\\n",
    ">图中，实线f表示凸函数，X是随机变量，有0.5的概率是a，有0.5的概率是b。X的期望值就是a和b的中值，从图中可以看到$E[f(X)] \\geq f(E[X])$成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么\n",
    "$$\n",
    "\\begin{align}\n",
    "X&=\\frac{p\\left(x_{i}, z_{i};\\theta\\right)}{Q_{i}\\left(z_{i}\\right)} \\\\\n",
    "f(E[X])&=\\sum_{i=1}^{n} \\log \\sum_{z_{i}} Q_{i}\\left(z_{i}\\right)\\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}\\\\\n",
    "E[f(X)]&=\\sum p(x)*f(x)=\\sum_{i=1}^{n} \\sum_{z_{i}} Q_{i}\\left(z_{i}\\right)\\log\\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为f(x)在这里是log函数，以10为底的函数，它是凹函数，当jensen不等式引用于凹函数时，需要取反。\n",
    "\n",
    "> <img src=\"./imgs/log.png\" alt=\"log函数图像\" width=\"300\" />\n",
    "\n",
    "所以\n",
    "$$\n",
    "\\begin{align}\n",
    "f(E[x])&>=E[f(x)]\\\\\n",
    "即\\sum_{i=1}^{n} \\log \\sum_{z_{i}} Q_{i}\\left(z_{i}\\right)\\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}&>=\\sum_{i=1}^{n} \\sum_{z_{i}} Q_{i}\\left(z_{i}\\right)\\log\\frac{p\\left(x_{i}, z_{i} ; \\theta\\right)}{Q_{i}\\left(z_{i}\\right)}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到了这一步，我们发现仍然求不出最大值，这里只有个最小值。我们设前一个式子为$L(\\theta)$，后一个式子为$J(z,Q)$，那么$L(\\theta)>=J(z,Q)$。\n",
    "那么我们可以通过不断的最大化这个下界J，来使得L(θ)不断提高，最终达到它的最大值。\n",
    "\n",
    "\n",
    "\n",
    "要想做到这件事情需要解决两个问题，一个是"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
