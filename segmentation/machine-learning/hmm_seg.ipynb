{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用hmm分词\n",
    "\n",
    "## 原理\n",
    "\n",
    "&emsp;&emsp;依据hmm模型$λ=(A,B,Π)$，我们需要利用样本数据训练出一个hmm模型的所有参数\n",
    "\n",
    "&emsp;&emsp;状态值集合$A$：{B：分词词首；M：分词词中；E：分词词尾；S：单个词分词}\n",
    "\n",
    "&emsp;&emsp;观测值集合$B$：每个观测值就是每个字。集合就是我们训练集各个字的合集\n",
    "\n",
    "&emsp;&emsp;状态值序列：需要我们得到的结果{S，S，B，M，E，B，E，S，S，B，E}\n",
    "\n",
    "&emsp;&emsp;观测值序列：一句话，如\"客户就是上帝\"。我们的训练集就是很多句子的集合\n",
    "\n",
    "&emsp;&emsp;$A$：状态转移概率矩阵\n",
    "\n",
    "| | B | M | E | S |\n",
    "| - ||||-|\n",
    "|B|||||\n",
    "|M|||||\n",
    "|E|||||\n",
    "|S|||||\n",
    "\n",
    "&emsp;&emsp;$B$：观测概率矩阵\n",
    "\n",
    "| | 客 | 户 | 就 | 是 |上| 帝 |\n",
    "| - ||||||-|\n",
    "|B|||||||\n",
    "|M|||||||\n",
    "|E|||||||\n",
    "|S|||||||\n",
    "\n",
    "&emsp;&emsp;$π$：初始状态概率向量\n",
    "\n",
    "| | B | M | E | S |\n",
    "| - ||||-|\n",
    "|初始选择概率|||||||\n",
    "\n",
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "['１', '９', '８', '６', '年', '，']\n",
      "[0, 1, 1, 1, 2, 3]\n",
      "['B', 'M', 'M', 'M', 'E', 'S']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from sklearn.model_selection import train_test_split  # 进行训练集和测试集划分\n",
    "import pickle  # 进行参数保存\n",
    " \n",
    " \n",
    "INPUT_DATA = \"./data/RenMinData.txt\"  # 数据集\n",
    "SAVE_PATH=\"./model/datasave.pkl\"  # 保存路径\n",
    "id2tag = ['B','M','E','S']   # B：分词头部 M：分词词中 E：分词词尾 S：独立成词 id与状态值\n",
    "tag2id={'B':0,'M':1,'E':2,'S':3}  # 状态值对应的id\n",
    "word2id={}  # 每个汉字对应的id\n",
    "id2word=[]  # 每个id对应的汉字\n",
    " \n",
    "def getList(input_str):\n",
    "    '''\n",
    "    单个分词转换为tag序列\n",
    "    :param input_str: 单个分词\n",
    "    :return: tag序列\n",
    "    '''\n",
    "    outpout_str = []\n",
    "    if len(input_str) == 1:   # 长度为1 单个字分词\n",
    "        outpout_str.append(tag2id['S'])\n",
    "    elif len(input_str) == 2:  # 长度为2 两个字分词，BE\n",
    "        outpout_str = [tag2id['B'],tag2id['E']]\n",
    "    else:  # 长度>=3 多个字分词 中间加length-2个M 首尾+BE\n",
    "        M_num = len(input_str) -2\n",
    "        M_list = [tag2id['M']] * M_num\n",
    "        outpout_str.append(tag2id['B'])\n",
    "        outpout_str.extend(M_list)\n",
    "        outpout_str.append(tag2id['E'])\n",
    "    return outpout_str\n",
    " \n",
    " \n",
    "def handle_data():\n",
    "    '''\n",
    "    处理数据，并保存至savepath\n",
    "    :return:\n",
    "    '''\n",
    "    x_data=[]   # 观测值序列集合\n",
    "    y_data=[]  # 状态值序列集合\n",
    "    wordnum=0\n",
    "    line_num=0\n",
    "    with open(INPUT_DATA,'r',encoding=\"utf-8\") as ifp:\n",
    "        for line in ifp:  # 对每一个sentence\n",
    "            line_num =line_num+1\n",
    "            line = line.strip()\n",
    "            if not line:continue\n",
    "            line_x = []\n",
    "            for i in range(len(line)):\n",
    "                if line[i] == \" \":continue\n",
    "                if(line[i] in id2word):   # word与id对应进行记录\n",
    "                    line_x.append(word2id[line[i]])\n",
    "                else:\n",
    "                    id2word.append(line[i])\n",
    "                    word2id[line[i]]=wordnum\n",
    "                    line_x.append(wordnum)\n",
    "                    wordnum=wordnum+1\n",
    "            x_data.append(line_x)\n",
    " \n",
    "            lineArr = line.split(\" \")\n",
    "            line_y = []\n",
    "            for item in lineArr:  # 对每一个分词进行状态值转换\n",
    "                line_y.extend(getList(item))\n",
    "            y_data.append(line_y)\n",
    " \n",
    "    print(x_data[0])\n",
    "    print([id2word[i] for i in x_data[0]])\n",
    "    print(y_data[0])\n",
    "    print([id2tag[i] for i in y_data[0]])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=43)   # 分为训练集和测试集\n",
    "    with open(SAVE_PATH, 'wb') as outp:   # 保存\n",
    "        pickle.dump(word2id, outp)\n",
    "        pickle.dump(id2word, outp)\n",
    "        pickle.dump(tag2id, outp)\n",
    "        pickle.dump(id2tag, outp)\n",
    "        pickle.dump(x_train, outp)\n",
    "        pickle.dump(y_train, outp)\n",
    "        pickle.dump(x_test, outp)\n",
    "        pickle.dump(y_test, outp)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    handle_data()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    '''\n",
    "    参数初始化\n",
    "    Trans = {}  # 状态转移矩阵\n",
    "    Emit = {}  # 观测概率矩阵\n",
    "    Count_dic = {} # 每个状态的数量计数\n",
    "    Start = {}  # 初始概率矩阵\n",
    "    '''\n",
    "    for tag in tag2id:\n",
    "        Trans[tag2id[tag]] = {}\n",
    "        for tag2 in tag2id:\n",
    "            Trans[tag2id[tag]][tag2id[tag2]] = 0.0\n",
    "    for tag in tag2id:\n",
    "        Start[tag2id[tag]] = 0.0\n",
    "        Emit[tag2id[tag]] = {}\n",
    "        Count_dic[tag2id[tag]] = 0\n",
    "def train():\n",
    "    '''\n",
    "    根据输入的训练集进行各个数组的填充\n",
    "    :return:\n",
    "    '''\n",
    "    for sentence, tags in zip(x_train, y_train):\n",
    "        for i in range(len(tags)):\n",
    "            if i == 0:\n",
    "                Start[tags[0]] += 1\n",
    "                Count_dic[tags[0]] += 1\n",
    "            else:\n",
    "                Trans[tags[i - 1]][tags[i]] += 1\n",
    "                Count_dic[tags[i]] += 1\n",
    "                if sentence[i] not in Emit[tags[i]] :\n",
    "                    Emit[tags[i]][sentence[i]] = 0.0\n",
    "                else:\n",
    "                    Emit[tags[i]][sentence[i]] += 1\n",
    " \n",
    "    for tag in Start:\n",
    "        Start[tag] = Start[tag] * 1.0 / len(x_train)\n",
    "    for tag in Trans:\n",
    "        for tag1 in Trans[tag]:\n",
    "            Trans[tag][tag1] = Trans[tag][tag1] / Count_dic[tag]\n",
    " \n",
    "    for tag in Emit:\n",
    "        for word in Emit[tag]:\n",
    "            Emit[tag][word] = Emit[tag][word] / Count_dic[tag]\n",
    "    print(Start)\n",
    "    print(Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, tag_list):\n",
    "    '''\n",
    "    :param sentence:  输入的句子\n",
    "    :param tag_list:  所有的tag\n",
    "    :return: prob预测的最大的概率 bestpath 预测的tag序列\n",
    "    '''\n",
    "    V = [{}]   # tabular\n",
    "    path = {}\n",
    "    backpointers = []\n",
    "    for y in tag_list:   # init\n",
    "        V[0][y] = Start[y] * (Emit[y].get(sentence[0],0.00000001))\n",
    "        path[y]=y\n",
    "    backpointers.append(path)\n",
    "    for t in range(1,len(sentence)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "        path = {}\n",
    "        for y in tag_list:\n",
    "            (prob,state ) = max([(V[t-1][y0] * Trans[y0].get(y,0.00000001) * Emit[y].get(sentence[t],0.00000001) ,y0) for y0 in tag_list])\n",
    "            V[t][y] =prob\n",
    "            path[y]=state\n",
    "        backpointers.append(path)\n",
    "    (prob, state) = max([(V[len(sentence) - 1][y], y) for y in tag_list])\n",
    "    best_path=[]\n",
    "    best_path.append(state)\n",
    "    for pathi in reversed(backpointers):\n",
    "        state = pathi[state]\n",
    "        best_path.append(state)\n",
    "    best_path.pop()\n",
    "      #  Pop off the start tag (we dont want to return that to the caller)\n",
    "    best_path.reverse()\n",
    "    return (prob, best_path)\n",
    " \n",
    "def test():\n",
    "    '''\n",
    "    计算Precision和Recall以及Fscore\n",
    "    '''\n",
    "    taglist=[tag2id[tag] for tag in tag2id]\n",
    "    entityres = []  # 根据预测结果的分词序列\n",
    "    entityall = []  # 根据真实结果的分词序列\n",
    "    for sentence, tags in zip(x_test, y_test):\n",
    "          # score, predict=viterbi(sentence,taglist,Start,Trans,Emit)\n",
    "        score, predict = viterbi(sentence, taglist)\n",
    "        entityres = calculate(sentence, predict, id2word, id2tag, entityres)\n",
    "        entityall = calculate(sentence, tags, id2word, id2tag, entityall)\n",
    " \n",
    "    rightpre = [i for i in entityres if i in entityall]  # 预测成功的分词序列\n",
    "    if len(rightpre) != 0:\n",
    "        precision = float(len(rightpre)) / len(entityres)\n",
    "        recall = float(len(rightpre)) / len(entityall)\n",
    "        print(\"precision: \", precision)\n",
    "        print(\"recall: \", recall)\n",
    "        print(\"fscore: \", (2 * precision * recall) / (precision + recall))\n",
    "    else:\n",
    "        print(\"precision: \", 0)\n",
    "        print(\"recall: \", 0)\n",
    "        print(\"fscore: \", 0)\n",
    "        \n",
    "def calculate(x,y,id2word,id2tag,res=[]):\n",
    "    '''\n",
    "    :param id2word: id2word\n",
    "    :param id2tag: id2tag\n",
    "    :param res: 添加输入句子的词组划分 BME S\n",
    "    :return: res\n",
    "    '''\n",
    "    entity=[]\n",
    "    for j in range(len(x)):\n",
    "        if id2tag[y[j]]=='B':\n",
    "            entity=[id2word[x[j]]]\n",
    "        elif id2tag[y[j]]=='M' and len(entity)!=0:\n",
    "            entity.append(id2word[x[j]])\n",
    "        elif id2tag[y[j]]=='E' and len(entity)!=0:\n",
    "            entity.append(id2word[x[j]])\n",
    "            res.append(entity)\n",
    "            entity=[]\n",
    "        elif id2tag[y[j]]=='S':\n",
    "            entity=[id2word[x[j]]]\n",
    "            res.append(entity)\n",
    "            entity=[]\n",
    "        else:\n",
    "            entity=[]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI-NLP-Tutorial]",
   "language": "python",
   "name": "ai-nlp-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
