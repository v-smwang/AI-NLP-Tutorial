{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 条件随机场\n",
    "\n",
    "    序列标注模型条件随机场，这种模型与感知机同属结构化学习大家族，但性能比感知机还要强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、机器学习的模型谱系\n",
    "\n",
    "<img src=\"./imgs/machine_learning_model.png\" alt=\"机器学习的模型谱系\" width=\"800\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    生成式模型包含的算法：\n",
    "\n",
    "> 朴素贝叶斯\\\n",
    "K近邻\\\n",
    "混合高斯模型\\\n",
    "隐马尔科夫模型\\\n",
    "贝叶斯网络\\\n",
    "Sigmoid Belief Networks\\\n",
    "马尔科夫随机场(markov random fields)\\\n",
    "深度信念网络\\\n",
    "LDA文档主题生成模型\n",
    "\n",
    "    判别式模型包含的算法：\n",
    "\n",
    "> 线性回归(linear regression)\\\n",
    "逻辑回归(logistic regression)\\\n",
    "线性判别分析\\\n",
    "支持向量机\\\n",
    "CART(classification and regression tree)\\\n",
    "神经网络(NN)\\\n",
    "高斯过程(gaussian process)\\\n",
    "条件随机场(conditional random field)\n",
    "    \n",
    "【假设我们有训练数据(X,Y)，X是属性集合，Y是类别标记。这时来了一个新的样本x，我们想要预测它的类别y。\n",
    "    我们最终的目的是求得最大的条件概率P(y|x)作为新样本的分类。】\n",
    "    \n",
    "根据建模的究竟是联合概率（同时发生的概率）分布P(x,y)还是条件概率分布P(y|x)。派生出生成式模型与判别式模型，这两个模型最终算的都是\n",
    "    P(y|x)\n",
    "\n",
    "### 1、生成式模型\n",
    "\n",
    "    【一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。比如说类别标签有｛猫，狗，猪｝，那首先根据猫的特征学习出一个猫的模型，再根据狗的特征学习出狗的模型，之后分别计算新样本 x 跟三个类别的联合概率 P(x,y) ，然后根据贝叶斯公式计算出P(y|x)】\n",
    "\n",
    "    生成式模型：模拟数据的生成过程，两类随机变量存在因果先后关系，先有因素 y，后有结果 x，这种因果关系由联合分布模拟：\n",
    "\n",
    "$$P(x,y) = P(y)P(x|y)$$\n",
    "\n",
    "    通过联合分布 P(x,y)，生成式模型其实间接建模了 P(x)：\n",
    "    \n",
    "$$P(x) = \\sum_{y \\in Y}P(x,y)$$\n",
    "\n",
    "    然后依据贝叶斯公式可以得到P(y|x)【最终求的仍然是P(y|x)，只是通过了P(x,y)来求】:\n",
    "    \n",
    "$$P(y|x) = \\frac{P(x,y)}{P(x)}$$\n",
    "\n",
    "    这里有两个缺陷:\n",
    "\n",
    "    1. P(x) 很难准确估计，因为特征之间并非相互独立，而是存在错综复杂的依赖关系。\n",
    "    2. P(x) 在分类中也没有直接作用。\n",
    "\n",
    "    为了克服这两个问题，判别式模型出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、判别式模型\n",
    "\n",
    "    【根据训练数据得到分类函数和分界面，比如说根据SVM模型得到一个分界面，然后直接计算条件概率 P(y|x) ，我们将最大的 P(y|x) 作为新样本的分类。判别式模型是对条件概率建模，学习不同类别之间的最优边界，无法反映训练数据本身的特性，能力有限，其只能告诉我们分类的类别。】\n",
    "\n",
    "<font color='red'>&emsp;&emsp;判别式模型直接跳过了 P(x)，直接对条件概率 P(y|x) 建模。不管 x 内部存在多复杂的关系，也不影响判别式模型对 y 的判断，于是就能够放心大胆的利用各种各样丰富的、有关联的特征。 所以我们会看到感知机分词的准确率高于隐马尔可夫模型。</font>\n",
    "\n",
    "$$P(y|x) = \\frac{exp(score(x,y))}{\\sum_{x,y}exp(score(x,y))} \\ \\ \\ \\ 说明：exp是指数函数$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1、举例说明\n",
    "\n",
    "    假设要对猫和狗进行区分：\n",
    "\n",
    "    生成式模型：是根据狗的特征首先学习出一个狗的模型，然后根据猫的特征学习出一个猫的模型，然后从这个动物中提取特征，放到狗模型中看概率是多少，再放到猫模型中看概率是多少，哪个大就是哪个。\n",
    "    \n",
    "    判别式模型：用判别式模型的方法是从历史数据中学习到模型，然后通过提取该动物的特征来预测出是狗的概率，是猫的概率。\n",
    "    \n",
    "### 2.2、数据计算过程样例\n",
    "\n",
    "    假设现在有一个分类问题，X是特征，Y是类标记。用判别式模型学习一个条件概率分布P(y|x)，用生成式模型学习一个联合概率分布 P(x,y) 。用一个简单的例子来说明这个问题。假设X就是两个特征（1或2），Y有两类（0或1），有如下训练样本（1，0）、（1，0）、（1，1）、（2，1）。\n",
    "\n",
    "    而学习到的联合概率分布（生成式模型）如下：\n",
    "| | 0 | 1 |\n",
    "|-|-|-|\n",
    "|1|1/2|1/4|\n",
    "|2|0|1/4|\n",
    "\n",
    "    则学习到的条件概率分布（判别式模型）如下：\n",
    "    \n",
    "| | 0 | 1 |\n",
    "|-|-|-|\n",
    "|1|2/3|1/3|\n",
    "|2|0|1|\n",
    "\n",
    "    在实际分类问题中，判别式模型可以直接用来判断特征的类别情况；而生成式模型需要加上贝叶斯公式，然后应用到分类中。但是，生成式模型的概率分布可以有其他应用，就是说生成式模型更一般更普适。不过判别式模型更直接，更简单。两种方法目前交叉较多。由生成式模型可以得到判别式模型，但由判别式模型得不到生成式模型。\n",
    "\n",
    "### 2.3、用图说明区别\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/discriminative_vs_generative.png\" alt=\"判别式和生成式模型比对\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    上图左边为判别式模型而右边为生成式模型，可以很清晰地看到差别，判别式模型是在寻找一个决策边界，通过该边界来将样本划分到对应类别。而生成式模型则不同，它学习了每个类别的边界，它包含了更多信息，可以用来生成样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4、两个模型总结\n",
    "\n",
    "    不管是生成式模型还是判别式模型，它们最终的判断依据都是条件概率P(y|x)，但是生成式模型先计算了联合概率P(x,y)，再由贝叶斯公式计算得到条件概率。因此，生成式模型可以体现更多数据本身的分布信息，其普适性更广。\n",
    "\n",
    "####  &emsp; &emsp;生成式模型的特点：\n",
    "    \n",
    "    生成式模型学习的是联合概率密度分布 P(X,Y)，可以从统计的角度表示分布的情况，能够反映同类数据本身的相似度，它不关心到底划分不同类的边界在哪里。生成式模型的学习收敛速度更快，当样本容量增加时，学习到的模型可以更快的收敛到真实模型，当存在隐变量时，依旧可以用生成式模型，此时判别式方法就不行了。具体来说，有以下特点：\n",
    "> 对联合概率建模，学习所有分类数据的分布。\\\n",
    "> 学习到的数据本身信息更多，能反应数据本身特性。\\\n",
    "> 学习成本较高，需要更多的计算资源。\\\n",
    "> 需要的样本数更多，样本较少时学习效果较差。\\\n",
    "> 推断时性能较差。\\\n",
    "> 一定条件下能转换成判别式。\n",
    "\n",
    "#### &emsp; &emsp;判别式模型特点：\n",
    "    \n",
    "    判别式模型直接学习决策函数 Y=f(X) 或者条件概率 P(Y|X) ，不能反映训练数据本身的特性，但它寻找不同类别之间的最优分裂面，反映的是异类数据之间的差异，直接面对预测往往学习准确度更高。具体来说有以下特点：\n",
    "        \n",
    "> 对条件概率建模，学习不同类别之间的最优边界。\\\n",
    "> 捕捉不同类别特征的差异信息，不学习本身分布信息，无法反应数据本身特性。\\\n",
    "> 学习成本较低，需要的计算资源较少。\\\n",
    "> 需要的样本数可以较少，少样本也能很好学习。\\\n",
    "> 预测时拥有较好性能。\\\n",
    "> 无法转换成生成式。\n",
    "\n",
    "总之，判别式模型和生成式模型都是使后验概率最大化，判别式是直接对后验概率建模，而生成式模型通过贝叶斯定理这一“桥梁”使问题转化为求联合概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、有向概率图模型\n",
    "\n",
    "    概率图模型能够分析错综复杂的随机变量关系。\n",
    "\n",
    "    有向概率图模型也称为贝叶斯网络。\n",
    "    \n",
    "    贝叶斯网络的一个基本要求是图必须是有向无环图（DAG/directed acyclic graph）。\n",
    "    \n",
    "    概率图模型( Probabilistic Graphical Model, PGM)是用来表示与推断多维随机变量联合分布 P(X,Y) 的强大框架，被广泛用于计算机视觉、知识表达、贝叶斯统计与自然语言处理。它利用节点 V 来表示随机变量，用边 E 连接有关联的随机变量，将多维随机变量分布表示为图 G=(V,E)。这样就带来了一个好处，那就是整个图可以分解为子图再进行分析.子图中的随机变量更少，建模更加简单。具体如何分解，据此派生出有向图模型和无向图模型。\n",
    "\n",
    "    有向图模型按事件的先后因果顺序将节点连接为有向图。如果事件 A 导致事件 B，则用箭头连接两个事件 A-->B。\n",
    "\n",
    "<img src=\"./imgs/PGM.png\" alt=\"有向概率图模型\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    有向图模型都将概率有向图分解为一系列条件概率之积，有向图模型经常用生成式模型来实现。定义 π(v) 表示节点 v 的所有前驱节点，则分布为:\n",
    "\n",
    "$$P(X,Y)=\\prod_{v=V}P(v|π(v))$$\n",
    "\n",
    "#### 3.1、有向概率图模型有啥用\n",
    "    \n",
    "    很多现实问题都可以使用有向概率图进行建模。\n",
    "    \n",
    "    例如：\n",
    "    \n",
    "<img src=\"./imgs/student_network.png\" alt=\"学生网络模型\" width=\"500\" align=\"center\"/>    \n",
    "\n",
    "    这个图描述了某个学生注册某个大学课程的设定。该图中有 5 个随机变量：\n",
    "> 课程的难度（Difficulty）：可取两个值，0 表示低难度，1 表示高难度\\\n",
    "    学生的智力水平（Intelligence）：可取两个值，0 表示不聪明，1 表示聪明\\\n",
    "    学生的评级（Grade）：可取三个值，1 表示差，2 表示中，3 表示优\\\n",
    "    学生的 SAT 成绩（SAT）：可取两个值，0 表示低分，1 表示高分\\\n",
    "    在完成该课程后学生从教授那里所得到的推荐信的质量（Letter）：可取两个值，0 表示推荐信不好，1 表示推荐信很好\n",
    "    \n",
    "    该图中的边编码了这些变量之间的依赖关系。\n",
    "    学生的 Grade 取决于课程的 Difficulty 和学生的 Intelligence；\n",
    "    而 Grade 又反过来决定了学生能否从教授那里得到一份好的 Letter；\n",
    "    另外，学生的 Intelligence 除了会影响他们的 Grade，还会影响他们的 SAT 分数。\n",
    "    注意其中箭头的方向表示了因果关系——Intelligence 会影响 SAT 分数，但 SAT 不会影响 Intelligence。\n",
    "    我们看到的每个节点关联的表格，它们的正式名称是条件概率分布（CPD/conditional probability distribution）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、无向概率图模型\n",
    "    \n",
    "    无向概率图模型也称为马尔科夫网络。\n",
    "\n",
    "    相反，无向图模型则不探究每个事件的因果关系，也就是说不涉及条件概率分解。无向图模型的边没有方向，仅仅代表两个事件有关联。\n",
    "    \n",
    "<img src=\"./imgs/PGM_without_direction.png\" alt=\"无向概率图模型\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    无向概率图模型的几个概念\n",
    "    \n",
    "<img src=\"./imgs/PGM_without_direction2.png\" alt=\"无向概率图模型例子\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.1、团（clique）\n",
    "\n",
    "<img src=\"./imgs/tuan.png\" alt=\"团\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.2、极大团（maximum clique）\n",
    "\n",
    "<img src=\"./imgs/maximum_tuan.png\" alt=\"极大团\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "#### 4.3、最大团 （maximul clique）\n",
    "\n",
    "<img src=\"./imgs/max_tuan.png\" alt=\"最大团\" width=\"400\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    ">团：必须两两相连（任意两点都相连）\\\n",
    "极大团：无法再加入一个节点使其成为团\\\n",
    "最大团：极大团里节点最多的团\n",
    "\n",
    "#### 4.4、因子节点\n",
    "    \n",
    "<img src=\"./imgs/factor.png\" alt=\"最大团\" width=\"150\" align=\"center\"/>    \n",
    "\n",
    "    因子节点是两个点间虚拟出来的点，用来辅助计算【暂时这么理解】。\n",
    "    \n",
    "    因子分解：将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数乘积形式的操作\n",
    "    \n",
    "    依据Hammersley- Clifford定理，无向图模型的联合概率分布表示为其最大团上的随机变量的函数乘积形式\n",
    "    \n",
    "$$P(X,Y) = \\frac{1}{Z}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "其中，a 是因子节点，$\\Psi_{a}$ 则是一个因子节点对应的函数，参数 $X_{a}, Y_{a}$ 是与因子节点相连的所有变量节点。为了将式子约束为概率分布，定义常数 Z 为如下归一化因子:\n",
    "\n",
    "$$Z = \\sum_{x,y}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "    在机器学习中，常用指数家族的因子函数:\n",
    "$$\\Psi_{a}(X_{a}, Y_{a}) = exp\\left\\{\\sum_{k}W_{ak}F_{ak}(X_{a}, Y_{a})\\right\\}$$\n",
    "\n",
    "其中，k 为特征的编号，$F_{ak}$ 是特征函数，$W_{ak}$ 为相应的特征权重。\n",
    "\n",
    "判别式模型经常用无向图来表示，只需要在归一化时，对每种 x 都求一个归一化因子:\n",
    "\n",
    "$$Z(X) = \\sum_{y}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "<font color=\"red\">然后 P(X,Y) 就转化为判别式模型所需的条件概率分布【之所以能转化是因为在条件随机场模型中，X是给定的，Y是条件随机场】:</font>\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "到这里，最后一个公式就是条件随机场的一般形式。\n",
    "\n",
    "### 5、为什么有了有向概率图模型，还需要无向概率图模型\n",
    "\n",
    "    原因是有些问题使用有向图表示会更加自然，比如上面提到的学生网络，有向图可以轻松描述变量之间的因果关系——学生的智力水平会影响 SAT 分数，但 SAT 分数不会影响智力水平（尽管它也许能反映学生的智力水平）。\n",
    "    \n",
    "    而对于其它一些问题，比如图像，你可能需要将每个像素都表示成一个节点。我们知道相邻的像素互有影响，但像素之间并不存在因果关系；它们之间的相互作用是对称的。所以我们在这样的案例中使用无向图模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、条件随机场\n",
    "\n",
    "条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。本系列主要关注于CRF的特殊形式：线性链(Linear chain) CRF。\n",
    "\n",
    "### 1、什么样的问题需要CRF模型\n",
    "\n",
    "和HMM类似，在讨论CRF之前，我们来看看什么样的问题需要CRF模型。这里举一个简单的例子：\n",
    "\n",
    "假设我们有Bob一天从早到晚的一系列照片，Bob想考考我们，要我们猜这一系列的每张照片对应的活动，比如: 工作的照片，吃饭的照片，唱歌的照片等等。一个比较直观的办法就是，我们找到Bob之前的日常生活的一系列照片，然后找Bob问清楚这些照片代表的活动标记，这样我们就可以用监督学习的方法来训练一个分类模型，比如逻辑回归，接着用模型去预测这一天的每张照片最可能的活动标记。\n",
    "\n",
    "这种办法虽然是可行的，但是却忽略了一个重要的问题，就是这些照片之间的顺序其实是有很大的时间顺序关系的，而用上面的方法则会忽略这种关系。比如我们现在看到了一张Bob闭着嘴的照片，那么这张照片我们怎么标记Bob的活动呢？比较难去打标记。但是如果我们有Bob在这一张照片前一点点时间的照片的话，那么这张照片就好标记了。如果在时间序列上前一张的照片里Bob在吃饭，那么这张闭嘴的照片很有可能是在吃饭咀嚼。而如果在时间序列上前一张的照片里Bob在唱歌，那么这张闭嘴的照片很有可能是在唱歌。\n",
    "\n",
    "为了让我们的分类器表现的更好，可以在标记数据的时候，可以考虑相邻数据的标记信息。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。\n",
    "\n",
    "在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的，当然CRF不是唯一的选择，也有很多其他的词性标注方法。\n",
    "\n",
    "### 2、从随机场到马尔科夫随机场\n",
    "\n",
    "首先，我们来看看什么是随机场。“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词...)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。\n",
    "\n",
    "了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。继续举十个词的句子词性标注的例子：　如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性有关。\n",
    "\n",
    "### 3、从马尔科夫随机场到条件随机场\n",
    "\n",
    "理解了马尔科夫随机场，再理解CRF就容易了。CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有𝑋和𝑌两种变量，𝑋一般是给定的，而𝑌一般是在给定𝑋的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。在我们十个词的句子词性标注的例子中，𝑋是词，𝑌是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。\n",
    " \n",
    "对于CRF，我们给出准确的数学语言描述：\n",
    "\n",
    "设$𝑋$与$𝑌$是随机变量，$𝑃(𝑌|𝑋)$是给定$𝑋$时$𝑌$的条件概率分布，若随机变量𝑌构成的是一个马尔科夫随机场，则称条件概率分布$𝑃(𝑌|𝑋)$是条件随机场。\n",
    "\n",
    "### 4、从条件随机场到线性链条件随机场\n",
    "\n",
    "注意在CRF的定义中，我们并没有要求𝑋和𝑌有相同的结构。而实现中，我们一般都假设𝑋和𝑌有相同的结构，即:\n",
    "\n",
    "\n",
    "$$X =(x_1,x_2,...x_n),\\;\\;Y=(y_1,y_2,...y_n)$$\n",
    "\n",
    "$$【有n个词，对应有n个词性】$$\n",
    "\n",
    "𝑋和𝑌有相同的结构的CRF就构成了线性链条件随机场(Linear chain Conditional Random Fields,以下简称 linear-CRF)。\n",
    "\n",
    "在我们的十个词的句子的词性标记中，词有十个，词性也是十个，因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个linear-CRF。\n",
    "\n",
    "我们再来看看 linear-CRF的数学定义：\n",
    "\n",
    "设$X =(x_1,x_2,...x_n)，Y=(y_1,y_2,...y_n)$均为线性链表示的随机变量序列，在给定随机变量序列𝑋的情况下，随机变量𝑌的条件概率分布𝑃(𝑌|𝑋)构成条件随机场，即满足马尔科夫性：\n",
    "\n",
    "$$P(y_i|X,y_1,y_2,...y_n) = P(y_i|X,y_{i-1},y_{i+1})；i\\in[1,n]$$\n",
    "\n",
    "$$表示在给定一个句子的所有词时，对应某个词的词性的概率，只和这个词的前后词性有关$$\n",
    "\n",
    "则称𝑃(𝑌|𝑋)为线性链条件随机场。\n",
    "\n",
    "### 5、线性链条件随机场的参数化形式\n",
    "\n",
    "对于上一节讲到的linear-CRF，我们如何将其转化为可以学习的机器学习模型呢？这是通过特征函数和其权重系数来定义的。什么是特征函数呢？\n",
    "\n",
    "在linear-CRF中，特征函数分为两类，第一类【状态特征，对应hmm里的发射矩阵】是定义在$y_i$节点上的节点特征函数，这类特征函数只和当前节点有关，记为：\n",
    "\n",
    "$$s_l(y_i,X,i),\\;\\; l =1,2,...L$$\n",
    "\n",
    "其中𝐿是定义在该节点的节点特征函数的总个数【一个节点会有L个特征函数，符合特征函数的s值为1，不符合的s值为0】，𝑖是当前节点在序列的位置。\n",
    "\n",
    "第二类【转移特征，对应hmm里的转移矩阵】是定义在$y_i$节点 上下文的局部特征函数，这类特征函数只和当前节点和上一个节点有关，记为：\n",
    "\n",
    "$$t_k(y_{i-1},y_i,X,i),\\;\\; k =1,2,...K$$\n",
    "\n",
    "$$这里为了简化计算，暂时当前词性先只和前一个词的词性有关$$\n",
    "\n",
    "其中𝐾是定义在该节点的局部特征函数的总个数【一个节点会有K个特征函数，符合特征函数的t值为1，不符合的t值为0】，𝑖是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之间的特征函数，是因为我们的linear-CRF满足马尔科夫性。\n",
    "\n",
    "无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设$𝑡_𝑘$的权重系数是$𝜆_𝑘,𝑠_𝑙$的权重系数是$𝜇_𝑙$,则linear-CRF由我们所有的$𝑡_𝑘,𝜆_𝑘,𝑠_𝑙,𝜇_𝑙$共同决定。\n",
    "\n",
    "此时我们得到了linear-CRF的参数化形式如下：\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}exp\\Big(\\sum\\limits_{i,k} \\lambda_kt_k(y_{i-1},y_i, X,i) +\\sum\\limits_{i,l}\\mu_ls_l(y_i, X,i)\\Big)$$\n",
    "$$表示在给定X情况下，某个猜测的词性序列的概率。就是给一个句子的所有词，这些词对应的词性组成序列的一种可能占所有可能的概率$$\n",
    "\n",
    "其中，𝑍(X)为规范化因子【表示给定X情况下，所有可能的词性序列的（特征函数值乘以特征权重）的累加和】：\n",
    "\n",
    "$$Z(X) =\\sum\\limits_{y} exp\\Big(\\sum\\limits_{i,k} \\lambda_kt_k(y_{i-1},y_i, X,i) +\\sum\\limits_{i,l}\\mu_ls_l(y_i, X,i)\\Big)$$\n",
    "\n",
    "$$y表示一个句子所有词的所有可能词性序列$$\n",
    "\n",
    "回到特征函数本身，每个特征函数定义了一个linear-CRF的规则，则其系数定义了这个规则的可信度。所有的规则和其可信度一起构成了我们的linear-CRF的最终的条件概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、线性链条件随机场实例\n",
    "\n",
    "这里我们给出一个linear-CRF用于词性标注的实例，为了方便，我们简化了词性的种类。假设输入的都是三个词的句子，即$𝑋=(𝑋_1,𝑋_2,𝑋_3)$,输出的词性标记为$𝑌=(𝑌_1,𝑌_2,𝑌_3)$,其中$𝑌\\in \\{1(名词)，2(动词)\\}$\n",
    "\n",
    "| <div style=\"width: 100pt\">特征函数</div> | <div style=\"width: 300pt\">函数值和对应的条件</div> | <div style=\"width: 200pt\">特征权重</div> |\n",
    "| :------ | :------ | :------ |\n",
    "| $t_1$ | =1;($y_1=1,y_2=2,X,2$)或($y_2=1,y_3=2,X,3$)<br>=0;其他 | $𝜆_1=1$ |\n",
    "| $t_2$ | =1;($y_1=1,y_2=1,X,2$)<br>=0;其他 | $𝜆_2=0.5$ |\n",
    "| $t_3$ | =1;($y_2=2,y_3=1,X,3$)<br>=0;其他 | $𝜆_3=1$ |\n",
    "| $t_4$ | =1;($y_1=2,y_2=1,X,2$)<br>=0;其他 | $𝜆_4=1$ |\n",
    "| $t_5$ | =1;($y_2=2,y_3=2,X,3$)<br>=0;其他 | $𝜆_5=0.2$ |\n",
    "| $s_1$ | =1;($y_1=2,X,1$)<br>=0;其他 | $\\mu_1=1$ |\n",
    "| $s_2$ | =1;($y_1=2,X,1$)或($y_2=2,X,2$)<br>=0;其他 | $\\mu_1=0.5$ |\n",
    "| $s_3$ | =1;($y_2=1,X,2$)或($y_3=1,X,3$)<br>=0;其他 | $\\mu_1=0.8$ |\n",
    "| $s_4$ | =1;($y_3=2,X,3$)<br>=0;其他 | $\\mu_1=0.5$ |\n",
    "\n",
    "求Y为(1,2,2)词性序列时的非规范化概率。\n",
    "\n",
    "利用linear-CRF的参数化公式，我们有：\n",
    "\n",
    "$$exp\\Big(\\sum\\limits_{i,k} \\lambda_kt_k(y_{i-1},y_i, X,i) +\\sum\\limits_{i,l}\\mu_ls_l(y_i, X,i)\\Big) = exp\\Big[\\sum\\limits_{k=1}^5\\lambda_k\\sum\\limits_{i=2}^3t_k(y_{i-1},y_i, X,i) + \\sum\\limits_{l=1}^4\\mu_l\\sum\\limits_{i=1}^3s_l(y_i, X,i) \\Big]$$\n",
    "\n",
    "带入(1,2,2)我们有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "exp\\Big[\\sum\\limits_{k=1}^5\\lambda_k\\sum\\limits_{i=2}^3t_k(y_{i-1},y_i, X,i) + \\sum\\limits_{l=1}^4\\mu_l\\sum\\limits_{i=1}^3s_l(y_i, X,i) \\Big] &= \\lambda_1(t_1(y_1=1,y_2=2,X,2)+t_1(y_2=2,y_3=2,X,3)) \\\\\n",
    "&+\\lambda_2(t_2(y_1=1,y_2=2,X,2)+t_2(y_2=2,y_3=2,X,3)) \\\\ \n",
    "&+\\lambda_3(t_3(y_1=1,y_2=2,X,2)+t_3(y_2=2,y_3=2,X,3)) \\\\ \n",
    "&+\\lambda_4(t_4(y_1=1,y_2=2,X,2)+t_4(y_2=2,y_3=2,X,3)) \\\\ \n",
    "&+\\lambda_5(t_5(y_1=1,y_2=2,X,2)+t_5(y_2=2,y_3=2,X,3)) \\\\ \n",
    "&+\\mu_1(s_1(y_1=1,X,1)+s_2(y_2=2,X,2)+s_3(y_3=2,X,3)) \\\\ \n",
    "&+\\mu_2(s_1(y_1=1,X,1)+s_2(y_2=2,X,2)+s_3(y_3=2,X,3)) \\\\ \n",
    "&+\\mu_3(s_1(y_1=1,X,1)+s_2(y_2=2,X,2)+s_3(y_3=2,X,3)) \\\\ \n",
    "&+\\mu_4(s_1(y_1=1,X,1)+s_2(y_2=2,X,2)+s_3(y_3=2,X,3)) \\\\ \n",
    "&=(1+0+0+0+0.2)+(1+0.5+0+0.5) \\\\ \n",
    "&=exp(3.2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(X)&= y为(y_1=1,y_2=1,y_3=1) \\\\\n",
    "&+y为(y_1=1,y_2=1,y_3=2) \\\\\n",
    "&+y为(y_1=1,y_2=2,y_3=1) \\\\\n",
    "&+y为(y_1=2,y_2=1,y_3=1) \\\\\n",
    "&+y为(y_1=2,y_2=1,y_3=2) \\\\\n",
    "&+y为(y_1=2,y_2=2,y_3=1) \\\\\n",
    "&+y为(y_1=2,y_2=2,y_3=2) \\\\\n",
    "&+y为(y_1=1,y_2=2,y_3=2) \\\\\n",
    "&=exp(4.2)+exp(5.2)+exp(6.2)+exp(7.2)+exp(8.2)+...+exp(3.2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$P(y_1=1,y_2=2,y_3=2|X)=\\frac{exp(3.2)}{exp(4.2)+exp(5.2)+exp(6.2)+exp(7.2)+exp(8.2)+...+exp(3.2)}$$\n",
    "\n",
    "注意：\n",
    "\n",
    "可以看出一共有2x2x2=8种组合，也就是8种Y可能序列。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、线性链条件随机场的简化形式\n",
    "\n",
    "在上几节里面，我们用$𝑠_𝑙$表示节点特征函数，用$𝑡_𝑘$表示局部特征函数，同时也用了不同的符号表示权重系数，导致表示起来比较麻烦。其实我们可以对特征函数稍加整理，将其统一起来。\n",
    "\n",
    "假设我们在某一节点我们有$𝐾_1$个局部特征函数和$𝐾_2$个节点特征函数，总共有$𝐾=𝐾_1+𝐾_2$个特征函数。我们用一个特征函数$𝑓_𝑘(𝑦_{𝑖−1},𝑦_𝑖,X,𝑖)$来统一表示如下:\n",
    "\n",
    "$$f_k(y_{i-1},y_i,X,i)= \\begin{cases} t_k(y_{i-1},y_i, X,i) & {k=1,2,...K_1}\\\\ s_l(y_i, x,i)& {k=K_1+l,\\; l=1,2...,K_2} \\end{cases}$$\n",
    "\n",
    "以为由Y，X参数就可以知道$y_{i-1},y_i,X,i$，所以：\n",
    "\n",
    "$$f_k(Y,X) = \\sum\\limits_{i=1}^nf_k(y_{i-1},y_i, X,i)\\ \\ \\ \\text{理解为：n代表有n个词}$$\n",
    "\n",
    "同时我们也统一$𝑓_𝑘(𝑦_{𝑖−1},𝑦_𝑖,X,𝑖)$对应的权重系数$𝑤_𝑘$如下：\n",
    "\n",
    "$$w_k= \\begin{cases} \\lambda_k & {k=1,2,...K_1}\\\\ \\mu_l & {k=K_1+l,\\; l=1,2...,K_2} \\end{cases}$$\n",
    "\n",
    "这样，我们的linear-CRF的参数化形式简化为：\n",
    "\n",
    "$$P(Y|X) =  \\frac{1}{Z(X)}exp\\sum\\limits_{k=1}^Kw_kf_k(Y,X)$$\n",
    "\n",
    "其中，𝑍(X)为规范化因子：\n",
    "\n",
    "$$Z(X) =\\sum\\limits_{y}exp\\sum\\limits_{k=1}^Kw_kf_k(Y,X)$$\n",
    "\n",
    "如果将上两式中的$𝑤_𝑘$与$𝑓_𝑘$的用向量表示，即:\n",
    "\n",
    "$$w=(w_1,w_2,...w_K)^T\\;\\;\\; F(Y,X) =(f_1(Y,X),f_2(Y,X),...f_K(Y,X))^T$$\n",
    "\n",
    "则linear-CRF的参数化形式简化为内积形式如下：\n",
    "\n",
    "$$P_w(Y|X) = \\frac{exp(w \\bullet F(Y,X))}{Z_w(X)} = \\frac{exp(w \\bullet F(Y,X))}{\\sum\\limits_{y}exp(w \\bullet F(Y,X))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、线性链条件随机场的矩阵形式\n",
    "\n",
    "将上一节统一后的linear-CRF公式加以整理，我们还可以将linear-CRF的参数化形式写成矩阵形式。为此我们定义一个𝑚×𝑚的矩阵𝑀，𝑚为𝑦所有可能的状态的取值个数。𝑀定义如下：\n",
    "\n",
    "$$M_i(X) = \\Big[M_i(y_{i-1},y_i|X)\\Big] = \\Big[exp(W_i(y_{i-1},y_i|X))\\Big] = \\Big[  exp(\\sum\\limits_{k=1}^Kw_kf_k(y_{i-1},y_i,X,i))\\Big]$$\n",
    "\n",
    "我们引入起点和终点标记$𝑦_0=𝑠𝑡𝑎𝑟𝑡,𝑦_{𝑛+1}=𝑠𝑡𝑜𝑝$, 这样，标记序列𝑦的规范化概率可以通过𝑛+1个矩阵元素的乘积得到，即：\n",
    "\n",
    "$$P_w(Y|X) = \\frac{1}{Z_w(X)}\\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|X)$$\n",
    "\n",
    "其中$𝑍_𝑤(X)$为规范化因子。\n",
    "\n",
    "以上就是linear-CRF的模型基础，后面我们会讨论linear-CRF和HMM类似的三个问题的求解方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、前向后向算法评估标记序列概率\n",
    "\n",
    "## 1、linear-CRF的三个基本问题\n",
    "\n",
    "在隐马尔科夫模型HMM中，我们讲到了HMM的三个基本问题，而linear-CRF也有三个类似的的基本问题。不过和HMM不同，在linear-CRF中，我们对于给出的观测序列𝑥是一直作为一个整体看待的，也就是不会拆开看(𝑥1,𝑥2,...)，因此linear-CRF的问题模型要比HMM简单一些，如果你很熟悉HMM，那么CRF的这三个问题的求解就不难了。\n",
    "\n",
    "~~linear-CRF第一个问题是评估，即给定 linear-CRF的条件概率分布𝑃(𝑦|𝑥)【也就是知道了所有特征函数的权重是多少了】, 在给定输入序列𝑥和输出序列𝑦时【计算一个句子中，某个词为某个词性，它的概率是多少；即计算给定的这个y中对应$y_i$取所有的可能值的概率分别是多少】，计算条件概率$𝑃(𝑦_𝑖|𝑥)和𝑃(𝑦_{𝑖−1}，𝑦_𝑖|𝑥)$以及对应的期望。~~\n",
    "\n",
    "\n",
    "> ~~再来解释下这个问题：\\\n",
    ">给定了$P(y|x)$，你只能拿到所有y对应的序列条件概率分布，却没有在某一序列位置i对应的取值为$y_i$的概率。这个概率需要去计算。\\\n",
    ">举例：3长度序列，2个状态取值，对于给定的x，我们已知$y=(𝑦_0=0,𝑦_1=1,𝑦_2=1)$的条件分布概率。但是此时$y_1=0$或者1的条件分布概率还需要我们去计算。 ~~\n",
    "\n",
    "linear-CRF第一个问题是评估，即给定linear-CRF的条件概率分布𝑃(𝑦|𝑥)【也就是知道了所有特征函数的权重是多少了】，在给定x的情况下，求条件概率$𝑃(𝑦_𝑖|𝑥)和𝑃(𝑦_{𝑖−1}，𝑦_𝑖|𝑥)$以及对应的期望。\n",
    "\n",
    "linear-CRF第二个问题是学习，即给定训练数据集𝑋和𝑌，学习linear-CRF的模型参数$𝑤_𝑘$和条件概率$𝑃_𝑤(𝑦|𝑥)$，这个问题的求解比HMM的学习算法简单的多，普通的梯度下降法，拟牛顿法都可以解决。\n",
    "\n",
    "linear-CRF第三个问题是解码，即给定 linear-CRF的条件概率分布𝑃(𝑦|𝑥),和输入序列𝑥, 计算使条件概率最大的输出序列𝑦。类似于HMM，使用维特比算法可以很方便的解决这个问题。　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、第一个问题评估\n",
    "\n",
    "### 2.1 linear-CRF的前向后向概率概述\n",
    "\n",
    "给定x输入\n",
    "\n",
    "例如：给定$x=\\{x_1=\"我\",x_2=\"是\",x_3=\"中国人\"\\}，y\\in \\{\"动词\",\"名词\"\\}$\n",
    "\n",
    "要计算条件概率$𝑃(𝑦_𝑖|𝑥)和𝑃(𝑦_{𝑖−1},𝑦_𝑖|𝑥)$【例如：求$P(y_2=\"动词\"|x)和P(y_1=\"名词\",y_2=\"动词\"|x)$】，我们也可以使用和HMM类似的方法，使用前向后向算法来完成。\n",
    "\n",
    "我们先看下$𝑃(𝑦_𝑖|𝑥)$等于什么。\n",
    "\n",
    ">**二维和多维边缘概率** \\\n",
    ">假设有两个随机变量A和B，取值都为{0,1}，我们有:\\\n",
    ">$P(A)=\\sum_B P(A,B)=P(A,0)+P(A,1)$ \\\n",
    ">那么\\\n",
    ">$P(A=0)=P(0,0)+P(0,1)$ \\\n",
    ">$P(A=1)=P(1,0)+P(1,1)$ \\\n",
    ">\\\n",
    ">对于三个随机变量A、B、C，取值都为{0,1}，我们依然有: \\\n",
    ">$P(A)=\\sum_B \\sum_C P(A,B,C)=\\sum_B [P(A,B,0)+P(A,B,1)]=[P(A,0,0)+P(A,0,1)]+[P(A,1,0)+P(A,1,1)]$\\\n",
    ">那么\\\n",
    ">$P(A=0)=[P(0,0,0)+P(0,0,1)]+[P(0,1,0)+P(0,1,1)]$ \\\n",
    ">$P(A=1)=[P(1,0,0)+P(1,0,1)]+[P(1,1,0)+P(1,1,1)]$ \\\n",
    ">再来假设\\\n",
    ">$P(A,B,C)=f_1(A)f_2(B)f_3(C)$ &emsp; &emsp; $f_1,f_2,f_3$是分别关于变量A，B，C的函数 \\\n",
    ">那么\\\n",
    ">$$\n",
    "\\begin{align}\n",
    "P(A)=\\sum_B \\sum_C P(A,B,C)&=\\sum_B \\sum_C[f_1(A)f_2(B)f_3(C)] \\\\\n",
    "&=\\sum_B[f_1(A)f_2(B)f_3(0)+f_1(A)f_2(B)f_3(1)]\\\\\n",
    "&=[f_1(A)f_2(0)f_3(0)+f_1(A)f_2(0)f_3(1)]+[f_1(A)f_2(1)f_3(0)+f_1(A)f_2(1)f_3(1)]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    ">\\\n",
    "$$\n",
    "\\begin{align}\n",
    "f_1(A)\\big[[\\sum_B f_2(B)][\\sum_C f_3(C)]\\big]&=f_1(A)\\big[[f_2(0)+f_2(1)][f_3(0)+f_3(1)]\\big]\\\\\n",
    "&=f_1(A)\\big[f_2(0)f_3(0)+f_2(0)f_3(1)+f_2(1)f_3(0)+f_2(1)f_3(1)\\big]\\\\\n",
    "&=f_1(A)f_2(0)f_3(0)+f_1(A)f_2(0)f_3(1)+f_1(A)f_2(1)f_3(0)+f_1(A)f_2(1)f_3(1)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    ">所以：\\\n",
    ">$P(A)=\\sum_B \\sum_C[f_1(A)f_2(B)f_3(C)]=f_1(A)\\big[[\\sum_B f_2(B)][\\sum_C f_3(C)]\\big]$这两种形式是等价的\n",
    "\n",
    "\n",
    "假设一个句子总共有n个词，依据边缘概率有：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "𝑃(𝑦_𝑖|𝑥) \n",
    "&= \\sum_{[y_1,\\ y_{i-1}\\ ]}\\sum_{[y_{i+1}\\ ,\\ y_{n}\\ ]}P(y_1,y_2,...,y_i,...,y_N|x) \\\\\n",
    "&= \\sum_{[y_1,\\ y_{i-1}\\ ]}\\sum_{[y_{i+1}\\ ,\\ y_{n}\\ ]}P(y|x) \\\\\n",
    "&= \\sum_{[y_1,\\ y_{i-1}\\ ]}\\sum_{[y_{i+1}\\ ,\\ y_{n}\\ ]}\\frac{1}{Z(x)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}, y_{t}, x)\\right\\}\\\\\n",
    "&我们另f_t(y_{t-1}, y_{t})=exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}, y_{t}, x)\\right\\} \\\\\n",
    "&那么 \\\\\n",
    "&= \\sum_{[y_1,\\ y_{i-1}\\ ]}\\sum_{[y_{i+1}\\ ,\\ y_{n}\\ ]}\\frac{1}{Z(x)}\\prod_{t=1}^{T}f_t(y_{t-1}, y_{t})\\\\\n",
    "&假设y_i的取值个数为s，则上式的计算复杂度为O(s^T*T)，指数级复杂度基本就没法暴力计算，所以我们找技巧\\\\\n",
    "&=\\frac{1}{Z(x)}.\\\\\n",
    "&\\big[\\sum_{y_{i-1}}\\big(f_i(y_{i-1}, y_{i})...\\sum_{y_2}\\big(f_3(y_{2}, y_{3})\\sum_{y_1}\\big(f_2(y_{1}, y_{2})\\sum_{y_0}f_1(y_{0}, y_{1})\\big)\\big)\\big)\\big].\\\\\n",
    "&\\big[\\sum_{y_{i+1}}\\big(f_{i+1}(y_{i}, y_{i+1})...\\sum_{y_{n-1}}\\big(f_{n-1}(y_{n-2}, y_{n-1})\\sum_{y_{n}}\\big(f_n(y_{n-1}, y_{n})\\sum_{y_{n+1}}f_{n+1}(y_{n}, y_{n+1})\\big)\\big)\\big)\\big]\\\\\n",
    "&在这里我们增加了y_0和y_{n+1}是为了方便我们计算 \\\\\n",
    "&我们定义\\alpha_i(y_i|x)表示序列位置i的标记是y_i时，在位置i之前(包括i)的部分标记序列的非规范化概率，\\\\\n",
    "&我们定义\\beta_i(y_i|x)表示序列位置i的标记是y_i时，在位置𝑖之后的从i+1到n的部分标记序列的非规范化概率 \\\\\n",
    "&在起点和终点处，我们定义： \\\\\n",
    "&\\alpha_0(y_0|x)= \\begin{cases} 1 & {y_0 =start}\\\\ 0 & {else} \\end{cases}\\\\\n",
    "&\\beta_{n+1}(y_{n+1}|x)= \\begin{cases} 1 & {y_{n+1} =stop}\\\\ 0 & {else} \\end{cases}\\\\\n",
    "&我们写出递推公式为：\\\\\n",
    "&\\alpha_i(y_i|x)=\\sum_{y_{i-1}}\\big(f_i(y_{i-1}, y_{i})\\alpha_{i-1}(y_{i-1}|x)\\big)\\\\\n",
    "&\\beta_i(y_i|x)=\\sum_{y_{i+1}}\\big(f_{i+1}(y_{i}, y_{i+1})\\beta_{i+1}(y_{i+1}|x)\\big)\\\\\n",
    "&所以最终我们的推导结果为：\\\\\n",
    "&=\\frac{1}{Z(x)}.\\alpha_i(y_i|x).\\beta_i(y_i|x)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理可以得到：\n",
    "$$\n",
    "\\begin{align}\n",
    "𝑃(𝑦_{𝑖−1},𝑦_𝑖|𝑥)=\\frac{1}{Z(x)}.\\alpha_{i-1}(y_{i-1}|x)f_i(y_{i-1},y_i).\\beta_i(y_i|x)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 linear-CRF的期望计算\n",
    "\n",
    "某函数f(x)的关于某分布P(x)的期望：$\\sum_xP(x)f(x)$\n",
    "\n",
    "有了上一节计算的条件概率，我们也可以很方便的计算联合分布𝑃(𝑥,𝑦)与条件分布𝑃(𝑦|𝑥)的期望。\n",
    "\n",
    "特征函数$𝑓_𝑘(𝑥,𝑦)$关于条件分布𝑃(𝑦|𝑥)的期望表达式是：\n",
    "\n",
    "$$\\begin{align} E_{P(y|x)}[f_k]  & = E_{P(y|x)}[f_k(y,x)] \\\\ & = \\sum\\limits_{i=1}^{n+1} \\sum\\limits_{y_{i-1}\\;\\;y_i}P(y_{i-1},y_i|x)f_k(y_{i-1},y_i,x, i) \\\\ & =  \\sum\\limits_{i=1}^{n+1} \\sum\\limits_{y_{i-1}\\;\\;y_i}f_k(y_{i-1},y_i,x, i)  \\frac{\\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\\beta_i(y_i|x)}{Z(x)} \\end{align}$$\n",
    "\n",
    "同样可以计算联合分布𝑃(𝑥,𝑦)的期望：\n",
    "\n",
    "$$\n",
    "\\begin{align} E_{P(x,y)}[f_k]  & = \\sum\\limits_{x,y}P(x,y) \\sum\\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) \\\\& =  \\sum\\limits_{x}\\tilde{P}(x) \\sum\\limits_{y}P(y|x) \\sum\\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) \\\\& =  \\sum\\limits_{x}\\tilde{P}(x)\\sum\\limits_{i=1}^{n+1} \\sum\\limits_{y_{i-1}\\;\\;y_i}f_k(y_{i-1},y_i,x, i)  \\frac{\\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\\beta_i(y_i|x)}{Z(x)}    \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这里假设已知边缘分布 P(X) 的经验分布为 $\\tilde{P}(x)$ ，经验分布就是根据训练数据，用频数估计的方式得到 $\\tilde{P}(x)=\\frac{𝑥}{𝑁}$，可以参考最大熵模型算法。\n",
    "\n",
    "假设一共有𝐾个特征函数，则𝑘=1,2,...𝐾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、第二个问题模型学习\n",
    "\n",
    "在linear-CRF模型参数学习问题中，我们给定训练数据集𝑋和对应的标记序列𝑌，𝐾个特征函数$𝑓_𝑘(𝑥,𝑦)$，需要学习linear-CRF的模型参数$𝑤_𝑘$和条件概率$𝑃_𝑤(𝑦|𝑥)$，其中条件概率$𝑃_𝑤(𝑦|𝑥)$和模型参数$𝑤_𝑘$满足一下关系：\n",
    "\n",
    "$$P_w(y|x) = P(y|x) =  \\frac{1}{Z_w(x)}exp\\sum\\limits_{k=1}^Kw_kf_k(x,y) =  \\frac{exp\\sum\\limits_{k=1}^Kw_kf_k(x,y)}{\\sum\\limits_{y}exp\\sum\\limits_{k=1}^Kw_kf_k(x,y)}$$\n",
    "\n",
    "所以我们的目标就是求出所有的模型参数𝑤𝑘，这样条件概率𝑃𝑤(𝑦|𝑥)可以从上式计算出来。\n",
    "\n",
    "求解这个问题有很多思路，比如梯度下降法，牛顿法，拟牛顿法。同时，这个模型中$𝑃_𝑤(𝑦|𝑥)$的表达式和最大熵模型原理小结中的模型一样，也可以使用最大熵模型中使用的改进的迭代尺度法(improved iterative scaling, IIS)来求解。\n",
    "\n",
    "下面我们只简要介绍用梯度下降法的求解思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 linear-CRF模型参数学习之梯度下降法求解\n",
    "\n",
    "在使用梯度下降法求解模型参数之前，我们需要定义我们的优化函数，一般极大化条件分布$𝑃_𝑤(𝑦|𝑥)$的对数似然函数如下：\n",
    "\n",
    "$$L(w)=  log\\prod_{x,y}P_w(y|x)^{\\tilde{P}(x,y)} = \\sum\\limits_{x,y}\\tilde{P}(x,y)logP_w(y|x)$$\n",
    "\n",
    "> <font color=\"red\">这里可以加经验概率也可以不加，加入的好处： \\\n",
    "> 举个简单的例子，中国人口分布，如果你没有先验知识，你会假设每个地方人口的分布都是一样的，均匀的。然后根据你的训练数据来优化你的人口分布认知。\\\n",
    ">如果你有先验知识，知道西部人口少，东部人口多，那么你的先验分布就不是均匀分布了，这也是那个指数经验分布的意义。\n",
    ">\n",
    ">这里用这个指数来标示当前样本出现的频度。\n",
    ">\n",
    ">这个频度如果没有先验知识，就是从训练样本中得出，可以每个样本取值为1，优化目标也就是你上面提到的直接取对数。否则从先验知识得出。</font>\n",
    "\n",
    "其中$\\tilde{P}(𝑥,𝑦)$为经验分布，可以从先验知识和训练集样本中得到，这点和最大熵模型类似。为了使用梯度下降法，我们现在极小化$𝑓(𝑤)=−𝐿(𝑃_𝑤)$如下：\n",
    "\n",
    "$$\\begin{align}f(w) & = -\\sum\\limits_{x,y}\\tilde{P}(x,y)logP_w(y|x) \\\\ &=  \\sum\\limits_{x,y}\\tilde{P}(x,y)logZ_w(x) - \\sum\\limits_{x,y}\\tilde{P}(x,y)\\sum\\limits_{k=1}^Kw_kf_k(x,y) \\\\& =  \\sum\\limits_{x}\\tilde{P}(x)logZ_w(x) - \\sum\\limits_{x,y}\\tilde{P}(x,y)\\sum\\limits_{k=1}^Kw_kf_k(x,y) \\\\& =  \\sum\\limits_{x}\\tilde{P}(x)log\\sum\\limits_{y}exp\\sum\\limits_{k=1}^Kw_kf_k(x,y) - \\sum\\limits_{x,y}\\tilde{P}(x,y)\\sum\\limits_{k=1}^Kw_kf_k(x,y)  \\end{align}$$\n",
    "\n",
    "\n",
    "对𝑤求导可以得到：\n",
    "\n",
    "$$\\frac{\\partial f(w)}{\\partial w} = \\sum\\limits_{x,y}\\tilde{P}(x)P_w(y|x)f(x,y) -  \\sum\\limits_{x,y}\\tilde{P}(x,y)f(x,y)$$\n",
    "\n",
    "有了𝑤的导数表达式，就可以用梯度下降法来迭代求解最优的𝑤了。注意在迭代过程中，每次更新𝑤后，需要同步更新$𝑃_𝑤(𝑥,𝑦)$，以用于下一次迭代的梯度计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、第三个问题解码\n",
    "\n",
    "利用维特比算法解码\n",
    "\n",
    "我们现在已经学习出了$𝑃_𝑤(y|X)$，也就是给定一句话，其中某个词是某个词性的概率，现在我们要解码出，给定一句话，每个词的词性都是什么时，概率最大。\n",
    "\n",
    "对于我们linear-CRF中的维特比算法，我们的第一个局部状态定义为$𝛿_𝑖(𝑙)$,表示在位置𝑖标记𝑙各个可能取值(1,2...m)对应的非规范化概率的最大值。之所以用非规范化概率是，规范化因子𝑍(𝑥)不影响最大值的比较。根据$𝛿_𝑖(𝑙)$的定义，我们递推在位置𝑖+1标记𝑙的表达式为：\n",
    "\n",
    "$$\\delta_{i+1}(l) = \\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\;, l=1,2,...m$$\n",
    "\n",
    "和HMM的维特比算法类似，我们需要用另一个局部状态$Ψ_{𝑖+1}(𝑙)$来记录使$𝛿_{𝑖+1}(𝑙)$达到最大的位置𝑖的标记取值,这个值用来最终回溯最优解，$Ψ_{𝑖+1}(𝑙)$的递推表达式为：\n",
    "\n",
    "$$\\Psi_{i+1}(l) = arg\\;\\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\; ,l=1,2,...m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、条件随机场—分词\n",
    "\n",
    "### 1、条件随机场介绍\n",
    "\n",
    "    条件随机场( Conditional Random Field, CRF)是一种给定输入随机变量 x，求解条件概率 p(y|x) 的概率无向图模型。用于序列标注时，特例化为线性链( linear chain )条件随机场。此时，输人输出随机变量为等长的两个序列。\n",
    "    \n",
    "<img src=\"./imgs/crf.png\" alt=\"条件随机场\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "每个 $X_{t}$ 上方有 3 个灰色节点，代表 $X_{t}$ 的 3 个特征，当然还可以是任意数量的特征，体现了特征的丰富性，黑色方块是因子节点，可以理解为一个特征函数 。其中仅仅利用了 $X_{t}$ 和 $X_{t}$ 的特征称作状态特征，利用了 $Y_{t-1}$ 的特征则称作转移特征，与感知机的特征函数相同。\n",
    "\n",
    "    线性链条件随机场的定义如下:\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "&emsp;&emsp;&emsp;K代表特征的总个数， \n",
    "\n",
    "&emsp;&emsp;&emsp;$F_{k}$ 代表第k个特征的特征函数，\n",
    "\n",
    "&emsp;&emsp;&emsp;$W_{k}$代表第k个特征函数求出的特征值所对应的权重。\n",
    "\n",
    "    其中，Z(X)为归一化函数:\n",
    "    \n",
    "$$Z(X) = \\sum_{y}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "<font color=\"red\">注意：\n",
    "    y就是所有可能的序列，计算开销很大。线性链条件随机场公式的分子是一个句子某个类标序列的总得分，分母是一个句子所有可能类标序列的总得分，如果一个句子有10个字，那么所有可能的类标序列就有$4^{10}$种。</font>\n",
    "\n",
    "    上式定义在所有可能的标注序列上。如果将所有特征函数与权重分别写作向量形式，则线性链条件随机场的定义可简化为:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Y|X) &= \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\\\\n",
    "&= \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{W.\\phi(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\\\\n",
    "&= \\frac{1}{Z(X)}exp\\left\\{\\sum_{t=1}^{T}W.\\phi(Y_{t-1}, Y_{t}, X_{t})\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    对比结构化感知机的打分函数:\n",
    "\n",
    "$$score(X,Y) = \\sum_{t=1}^{T}W.\\phi(Y_{t-1}, Y_{t}, X_{t})$$\n",
    "\n",
    "    可以发现结构化感知机打分函数与条件随机场的指数部分完全相同，由于给定实例 X，Z(X) 就是一个常数 c，所以有:\n",
    "    \n",
    "$$P(Y|X) = \\frac{1}{c}exp\\left\\{score(X,Y)\\right\\}$$\n",
    "\n",
    "    于是，条件随机场就和结构化感知机有以下联系:\n",
    "\n",
    "> 条件随机场和结构化感知机的特征函数完全一致。\\\n",
    "结构化感知机预测打分越高，条件随机场给予该预测的概率也越大。\\\n",
    "这种相似性使得我们能够复用结构化感知机的预测算法，也就是维特比算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、条件随机场的训练\n",
    "\n",
    "指定训练集$D=\\lbrace x^{(i)},y^{(i)}\\rbrace _{i=1}^{N}$，N是总样本数（如果是句子，就是所有待训练的句子数）。其中每个$X^{(i)}={X_1^{(i)},X_2^{(i)},...X_T^{(i)}}$为输入序列，$Y^{(i)}={Y_1^{(i)},Y_2^{(i)},...Y_T^{(i)}}$为相应的标签序列。\n",
    "\n",
    "\n",
    "    根据极大似然估计，我们想要最大化给定模型参数W时，训练集D的似然概率：\n",
    "$$P(D|W)=\\prod_{i=1}^{N}P(Y^{(i)}|X^{(i)})$$\n",
    "\n",
    "    等价于极大化对数似然函数：\n",
    "$$L(W)=\\sum_{i=1}^{N}lnP(Y^{(i)}|X^{(i)})$$\n",
    "\n",
    "将$P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$代入上式得到：\n",
    "\n",
    "$$L(W)=\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\sum_{i=1}^{N}lnZ(X)$$\n",
    "\n",
    "$$Z(X) = \\sum_{y}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "其中N是样本集总数量，现在我们要用随机梯度下降法（SGD）来极大似然函数（准确地说应该是利用梯度上升法，来求最大值），因此每次只用处理一个训练序列，于是有：\n",
    "$$L(W)=\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-lnZ(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    那么对数似然函数的偏导为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W_{k}}&=\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\frac{1}{Z(X)}\\frac{\\partial Z(X)}{\\partial W_{k}}  \\\\\n",
    "&=\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\frac{1}{Z(X)}\\sum_{y}\\bigg(exp\\left\\{\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_kF_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\bigg)  \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为:\n",
    "\n",
    "$$P(Y|X)=\\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "所以偏导式子化简为：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{k}}=\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\sum_{y}\\bigg(P(Y|X)\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1},y_{t},x_{t}^{(i)})\\bigg)$$\n",
    "\n",
    "推导完毕，这个式第一项是真实值，第二项是期望值，当两者相等时，梯度为0，迭代停止。\n",
    "\n",
    "<font color=\"red\">**这里需要累加所有样本的梯度，因为crf的权重更新是针对所有样本的一次迭代，假设样本数为M：**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{k}}=\\sum_{m=1}^M\\left[\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\sum_{y}\\bigg(P(Y|X)\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1},y_{t},x_{t}^{(i)})\\bigg)\\right]$$\n",
    "\n",
    "实际应用中特征函数、观测值的取值数量可能非常大，梯度下降法、牛顿法求解速度太慢，一般采用L-BFGS算法，其原理在《机器学学习与应用》一书中有详细的讲解。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <font color=\"red\">**第一项真实值** \n",
    ">\n",
    "> 就是一句话对应的真实类标序列。\n",
    ">\n",
    "> **第二项期望值** \n",
    ">  \n",
    "> 对一句话的每个字进行类标的排列组合序列计算每个类标序列对应的概率$P(Y|X)$，乘以每个类别序列所对应的$\\sum_{t=1}^{T}F_k(y_{t-1},y_{t},x_{t}^{(i)})$累计得分，即为每个类标序列得分在$P(Y|X)$概率分布下的期望值。\n",
    ">\n",
    "> **例子：**\n",
    ">\n",
    "><img src=\"imgs/tag.png\" width=\"400\"/>\n",
    ">\n",
    "> $x^i$ = 小明是中国人；$y^i$=BESSBME\n",
    ">\n",
    "> W是每个特征函数的权重，包含了x特征和y特征【这个是依据hanlp的平均结构化感知器的逻辑，可以参考它的权重定义和更新规则】。\n",
    ">\n",
    "> 首先利用模型+维特比算法预测出该句子的类标序列$\\hat{y}$，然后比较$\\hat{y}$和$y^i$，如果不一致，则更新W权重。\n",
    ">\n",
    ">关于W偏导式子的第一项就是计算真实类标序列在每个特征上的得分的累加值，如果几个字有相同的特征那么该特征得分就会累加。\n",
    ">\n",
    ">第二项就是句子的字数乘以类标数的自由组合的类标序列，求得每个类标序列的得分然后乘以该类标序列的概率。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用前向-后向算法，特征函数的期望可以用下面的式子进行求解\n",
    "\n",
    "$$E_{P(Y|X)}(f_k)=\\sum_{y}P(y|x){f_k}(y,x)={\\sum_{i=1}^{n+1}} \\sum_{y_{i-1}\\ \\ ,y_i} f_k(y_{i-1},y_i,x,i)\\bullet{\\frac{{\\alpha_i}^T(y_{i-1}|x)M_i (y_{i-1},{y_i}|x){\\beta_i}(y_i|x)}{Z(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、条件随机场和结构化感知器对比\n",
    "\n",
    "    结构化感知机和条件随机场的相同点:\n",
    "\n",
    "><font color=\"red\">特征函数相同：选取的特征和对应的特征函数是同结构化感知器一样的</font>\\\n",
    ">权重向量相同\\\n",
    ">打分函数相同\\\n",
    ">预测算法相同\\\n",
    ">同属结构化学习\n",
    "\n",
    "    不同点:\n",
    "    \n",
    ">1.感知机更新参数时，只使用一个训练实例，没有考虑整个数据集【应该是每次只考虑一个字，而没有考虑整个句子；后期已经证实是每个训练实例即每个句子】，难免顾此失彼；而条件随机场对数似然函数及其梯度则使用了整个数据集(整个句子；后期已经证实是整个数据集)<font color=\"red\">【条件随机场应该可以设置batch，即训练多少个句子更新一次权重；后期已经证实他就是整个数据集做一次权重更新】</font>。\n",
    ">\n",
    "> 2.条件随机场更新参数更加合理，条件随机场更新参数如下:\n",
    "$$W \\leftarrow W+\\phi(X^{i},Y^{i})-E_{w}\\left[\\phi(X^{i},Y)\\right]$$\n",
    ">\n",
    "> 对比感知机的更新参数表达式:\n",
    ">$$W \\leftarrow W+\\phi(X^{i},Y^{i})-\\phi(X^{i},\\hat{Y})$$\n",
    ">\n",
    "> 两者的差距一目了然，感知机奖励正确答案对应的特征函数 ϕ，但仅惩罚错误最厉害的那个 y，而条件随机场同时惩罚所有答案 y，分摊惩罚总量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "\n",
    "两者的差距一目了然，感知机奖励正确答案对应的特征函数中$\\phi(X^{i},Y^{i})$。但仅惩罚错得最厉害的那一个类标于对应的特征函数$\\phi(X^{i},\\hat{Y})$，可谓枪打出头鸟。\n",
    "\n",
    "而条件随机场不但奖励正确答案对应的特征函数，还同时惩罚所有答案y。这样就导致所有错误结果对应的特征函数都受到惩罚，可谓有罪么罚。条件随机场的惩罚总量依然为1。但所有答案分摊。而且每个答案的特征函数受到的惩罚力度精确为模型赋于它的概率，可谓依罪量刑。\n",
    "\n",
    "正确答案的特征函数反而已经受到一单位的奖励(即$+\\phi(X^{i},Y^{i})$)。所以不在平那点小于一单位的您罚(即$\\sum_{y}\\bigg(P(Y|X)\\sum_{t=1}^{T}\\sum_{k=1}^{K}F_k(y_{t-1},y_{t},x_{t}^{(i)})\\bigg)$，由于$P(Y|X)\\leq 1$，所以称作小于单位的惩罚)。\n",
    "\n",
    "当特征函数的模型期望与经验分布期望一致时(即$P(Y|X) = 1$)，梯度为0，模型参数不再变化。此时，全量梯度上升算法一定收敛到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">注意：\n",
    "\n",
    "CRF++训练详解\n",
    "\n",
    "流程概述:\n",
    "\n",
    "1. 生成特征函数\n",
    "\n",
    "2. 构建概率图\n",
    "\n",
    "3. 计算node和path的代价:node代价计算即该node对应的所有特征函数权重;path代价即该path对应的所有特征函数权重之和(node和path是有特定的label的)\n",
    "\n",
    "4. 前向-后向算法计算alpha和beta, 计算期望\n",
    "\n",
    "以上都是针对某个训练样本\n",
    "\n",
    "以下是针对所有样本\n",
    "\n",
    "5. 计算梯度g(w)(所有训练样本的期望-1),计算目标函数L(W)\n",
    "\n",
    "6. LGBLF算法一轮迭代后更新W(代码中的alpha)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
