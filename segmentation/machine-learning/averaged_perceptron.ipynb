{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于平均感知器的分词器\n",
    "\n",
    "## 原理\n",
    "\n",
    "&emsp;&emsp;我们知道对于一句话进行分词，我们可以通过给每个字进行标注状态（B：分词词首；M：分词词中；E：分词词尾；S：单个词分词）来分词，然后去计算出每个字到底是什么状态，从而完成分词。\n",
    "\n",
    "&emsp;&emsp;那使用感知器，无非也就是用感知器来预测出每个字的4种状态的概率分别是多少，然后利用维特比算法找出整个句子最大概率的状态路径，从而完成分词。和hmm分词差不多，只是将hmm预测隐状态概率换成了使用感知器的多分类来预测隐状态概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class AveragedPerceptron(object):\n",
    "\n",
    "    '''An averaged perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            weights = self.weights[feat]\n",
    "            for label, weight in weights.items():\n",
    "                scores[label] += value * weight\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda label: (scores[label], label))\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "def train(nr_iter, examples):\n",
    "    '''Return an averaged perceptron model trained on ``examples`` for\n",
    "    ``nr_iter`` iterations.\n",
    "    '''\n",
    "    model = AveragedPerceptron()\n",
    "    for i in range(nr_iter):\n",
    "        random.shuffle(examples)\n",
    "        for features, class_ in examples:\n",
    "            scores = model.predict(features)\n",
    "            guess, score = max(scores.items(), key=lambda i: i[1])\n",
    "            if guess != class_:\n",
    "                model.update(class_, guess, features)\n",
    "    model.average_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "1、对每个句子中的每个字，取7种特征【unigrams+bigrams】\n",
    "\n",
    "> $C_{i-1}$ #前一个词 \\\n",
    "> $C_i$ #当前词 \\\n",
    "> $C_{i+1}$ #后一个词 \\\n",
    "> $C_{i-2}/C_{i-1}$ #前两个词与前一个词 \\\n",
    "> $C_{i-1}/C_i$ #前一个词与当前词 \\\n",
    "> $C_i/C_{i+1}$  #当前词与后一个词 \\\n",
    "> $C_{i+1}/C_{i+2}$  #后一个词与后两个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_feature [6, 7, 7, 8, 9, 10, 11]\n",
      "char_feature [12, 13, 13, 14, 15, 16, 17]\n",
      "char_feature [18, 19, 19, 20, 21, 22, 23]\n",
      "char_feature [24, 25, 25, 26, 27, 28, 29]\n",
      "char_feature [30, 31, 31, 32, 33, 34, 35]\n",
      "char_feature [36, 37, 37, 38, 39, 40, 41]\n",
      "char_feature [6, 42, 42, 8, 43, 44, 45]\n",
      "char_feature [46, 47, 47, 48, 49, 50, 51]\n",
      "char_feature [52, 53, 53, 54, 55, 56, 57]\n",
      "char_feature [58, 59, 59, 60, 61, 62, 63]\n",
      "char_feature [64, 65, 65, 66, 67, 68, 69]\n",
      "char_feature [70, 71, 71, 72, 73, 74, 75]\n",
      "char_feature [76, 77, 77, 78, 79, 80, 81]\n",
      "char_feature [82, 83, 83, 84, 85, 86, 87]\n",
      "char_feature [88, 89, 89, 90, 91, 92, 93]\n",
      "char_feature [94, 95, 95, 96, 97, 98, 99]\n",
      "char_feature [100, 101, 101, 102, 103, 104, 105]\n",
      "char_feature [106, 107, 107, 108, 109, 110, 111]\n",
      "char_feature [112, 113, 113, 114, 115, 116, 41]\n",
      "char_feature [6, 117, 117, 8, 118, 119, 120]\n",
      "char_feature [121, 122, 122, 123, 124, 125, 126]\n",
      "char_feature [127, 95, 95, 128, 129, 130, 131]\n",
      "char_feature [100, 132, 132, 133, 134, 135, 29]\n",
      "char_feature [136, 31, 31, 137, 138, 34, 35]\n",
      "char_feature [36, 37, 37, 139, 39, 40, 41]\n",
      "[[[6, 7, 7, 8, 9, 10, 11], [12, 13, 13, 14, 15, 16, 17], [18, 19, 19, 20, 21, 22, 23], [24, 25, 25, 26, 27, 28, 29], [30, 31, 31, 32, 33, 34, 35], [36, 37, 37, 38, 39, 40, 41]], [[6, 42, 42, 8, 43, 44, 45], [46, 47, 47, 48, 49, 50, 51], [52, 53, 53, 54, 55, 56, 57], [58, 59, 59, 60, 61, 62, 63], [64, 65, 65, 66, 67, 68, 69], [70, 71, 71, 72, 73, 74, 75], [76, 77, 77, 78, 79, 80, 81], [82, 83, 83, 84, 85, 86, 87], [88, 89, 89, 90, 91, 92, 93], [94, 95, 95, 96, 97, 98, 99], [100, 101, 101, 102, 103, 104, 105], [106, 107, 107, 108, 109, 110, 111], [112, 113, 113, 114, 115, 116, 41]], [[6, 117, 117, 8, 118, 119, 120], [121, 122, 122, 123, 124, 125, 126], [127, 95, 95, 128, 129, 130, 131], [100, 132, 132, 133, 134, 135, 29], [136, 31, 31, 137, 138, 34, 35], [36, 37, 37, 139, 39, 40, 41]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_feature = {'BL=B':0, 'BL=M':1, 'BL=E':2, 'BL=S':3, 'BL=_BL_':4}  # 特征取值集合\n",
    "sentence_feature_list = []  # 记录每个句子中每个字的特征\n",
    "sentence_tag_list = []  # 每个字的类标\n",
    "START_CHAR = '\\1'\n",
    "END_CHAR = '\\2'\n",
    "\n",
    "count = 0\n",
    "\n",
    "def handle_feature(feature, char_feature, all_feature):\n",
    "    feature_id = all_feature[feature] if feature in all_feature else len(all_feature) + 1\n",
    "    all_feature[feature] = feature_id\n",
    "    char_feature.append(feature_id)\n",
    "\n",
    "# weights = np.ones(())\n",
    "data_f = open('./data/RenMinData.txt', 'r', encoding='utf-8')\n",
    "for line in data_f.readlines():\n",
    "    count += 1\n",
    "    if count > 3: break\n",
    "    line = line.strip()\n",
    "    # 打标签\n",
    "    words = line.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        tag_list = []\n",
    "        if len(word) == 1:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "        else:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "            for w in word[1:len(word)-1]: # 中间字\n",
    "                tag_list.append(all_feature['BL=M'])\n",
    "            tag_list.append(all_feature['BL=E'])\n",
    "    sentence_tag_list.append(tag_list)\n",
    "    \n",
    "    # 获取特征    \n",
    "    sentence = line.replace(' ','')\n",
    "    sentence_feature = []\n",
    "    for i, char in enumerate(sentence):\n",
    "        char_feature = []\n",
    "        # 前2\n",
    "        pre2 = sentence[i-2] if i>=2 else START_CHAR\n",
    "        # 前1\n",
    "        pre1 = sentence[i-1] if i>=1 else START_CHAR\n",
    "        # 当前\n",
    "        cur = char\n",
    "        # 后1\n",
    "        next1 = sentence[i+1] if i < len(sentence)-1 else END_CHAR\n",
    "        # 后2\n",
    "        next2 = sentence[i+2] if i < len(sentence)-2 else END_CHAR\n",
    "        \n",
    "        # unigrams\n",
    "        one = pre1+'1'\n",
    "        handle_feature(one, char_feature, all_feature)\n",
    "        \n",
    "        two = cur+'2'\n",
    "        handle_feature(two, char_feature, all_feature)\n",
    "        \n",
    "        three = cur+'2'\n",
    "        handle_feature(three, char_feature, all_feature)\n",
    "        \n",
    "        # bigrams\n",
    "        four = pre2+'/'+pre1+'4'\n",
    "        handle_feature(four, char_feature, all_feature)\n",
    "        \n",
    "        five = pre1+'/'+cur+'5'\n",
    "        handle_feature(five, char_feature, all_feature)\n",
    "        \n",
    "        six = cur+'/'+next1+'6'\n",
    "        handle_feature(six, char_feature, all_feature)\n",
    "        \n",
    "        seven = next1+'/'+next2+'7'\n",
    "        handle_feature(seven, char_feature, all_feature)\n",
    "        \n",
    "        sentence_feature.append(char_feature)\n",
    "        print('char_feature',char_feature)\n",
    "    sentence_feature_list.append(sentence_feature)     \n",
    "print(sentence_feature_list[:3])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI-NLP-Tutorial]",
   "language": "python",
   "name": "ai-nlp-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
