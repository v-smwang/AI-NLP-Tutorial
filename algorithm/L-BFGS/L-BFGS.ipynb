{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、L-BFGS算法简介\n",
    "\n",
    "  我们知道算法在计算机中运行的时候是需要很大的内存空间的.就像我们解决函数最优化问题常用的梯度下降,它背后的原理就是依据了泰勒一次展开式.泰勒展开式展开的次数越多,结果越精确,没有使用三阶四阶或者更高阶展开式的原因就是目前硬件内存不足以存储计算过程中演变出来更复杂体积更庞大的矩阵.L-BFGS算法翻译过来就是有限内存中进行BFGS算法,L是limited memory的意思.\n",
    "\n",
    "Broyden,Fletcher,Goldfarb,Shanno.四位数学家名字的首字母是BFGS,所以算法的名字就叫做BFGS算法.接下来我们就一起来学习BFGS算法的内容.\n",
    "\n",
    "学习BFGS必须要先了解牛顿法的求根问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、牛顿法求根\n",
    "\n",
    "牛顿发现,一个函数$f(x)$的根（也就是函数图像与x轴的交点）从物理的角度就可以根据函数图像迭代求得.牛顿法求根的思路是:\n",
    "\n",
    "a.在X轴上随机一点,经过做X轴的垂线,得到垂线与函数图像的交点.\n",
    "\n",
    "b.通过做函数的切线,得到切线与X轴的交点.\n",
    "\n",
    "c.迭代a/b两步.\n",
    "\n",
    "下面附上一张动图方便理解:\n",
    "\n",
    "<img src=\"imgs/niudun.gif\" width=\"400\"/>\n",
    "\n",
    "通过图片我们可以看到.在X轴上取的点会随着迭代次数的增加而越来越接近函数的根.经过无限多次的迭代,$x_n$就等于函数的根$f(x)$.但牛顿法在实际应用的时候我们不会让算法就这么迭代下去,所以当$x_{k-1}$和$x_k$相同或者两个值的差小于一个阈值的时候,$x_k$就是函数$f(x)$的根.\n",
    "\n",
    "那么问题来了,怎么样找到$f(x)$的导函数与X轴的交点.请看下图:\n",
    "\n",
    "<img src=\"imgs/niudun2.png\" width=\"400\"/>\n",
    "\n",
    "图片是上边动图从$x_{1}$到$x_{2}$的动作.可以看到,三角形绿色的直角边的值是$x_{1} - x_{2}$,  $x_{1}$是已知的(随机出来的),而且函数表达式f(x)也是已知的(不知道要求的函数咱们折腾啥呢).所以三角形的另一条直角边$f(x_{1})$也是已知的.根据导函数定义,函数$f(x)$在$x_{1}$点的导函数就是$f^{'}(x_1) = \\frac{f(x_{1})}{x_{1} - x_{2}}$(公式一)。\n",
    "\n",
    "公式一变换一下得到:$x_{2}=x_{1}-\\frac{f(x_{1})}{f^{'}(x_1)}$(公式二)，同理可以得出每一次迭代的表达式都是$x_{k}=x_{k-1}-\\frac{f(x_{k-1}\\ \\ )}{f^{'}(x_{k-1}\\ \\ )}$(公式三)。\n",
    "\n",
    "所以,根据牛顿法求根的思路,我们可以总结(模仿)一下使用牛顿法求根的步骤:\n",
    "\n",
    "a.已知函数$f(x)$的情况下,随机产生点$x_0$。\n",
    "\n",
    "b.由已知的点$x_0$按照的公式$x_{k}=x_{k-1}-\\frac{f(x_{k-1}\\ \\ )}{f^{'}(x_{k-1}\\ \\ )}$进行k次迭代。\n",
    "\n",
    "c.如果$x_k$与上一次的迭代结果$x_{k-1}$相同或者相差结果小于一个阈值时，本次结果$x_k$就是函数$f(x)$的根。\n",
    "\n",
    "以上为牛顿法的求根的思路.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、牛顿法求函数的驻点\n",
    "\n",
    "我们知道,机器学习过程中的函数最优化问题,大部分优化的都是目标函数的导函数,我们要求得导函数为零时的点或近似点来作为机器学习算法表现最好的点.现在我们知道了牛顿求根法,那把牛顿求根法的函数换成咱们要优化的导函数不就行了么.要求的的导函数为零的点叫做驻点.所以用牛顿法求函数驻点同求函数的根是没有任何区别的.只是公式二中的$f(x)$变为$f^{'}(x)$了,原来的$f^{'}(x)$变成了一阶导函数$f^{'}(x)$的导函数也就是二阶导函数$f^{''}(x)$,求的迭代公式如下:\n",
    "\n",
    "$$x_{k}=x_{k-1}-\\frac{f^{'}(x_{k-1}\\ \\ )}{f^{''}(x_{k-1}\\ \\ )}$$\n",
    "\n",
    "这样,我们通过几何直觉,得到了求解函数根的办法,那这么厉害的一个想法,有没有什么理论依据作为支撑呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、牛顿法求驻点的本质\n",
    "\n",
    "牛顿法求驻点的本质其实是二阶泰勒展开.我们来看二阶泰勒展开式:\n",
    "\n",
    "$$\\varphi(x)=f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)+\\frac{1}{2} f^{\\prime \\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)^{2}$$\n",
    "\n",
    "$\\varphi(x)$是我们要求解的原函数的近似式.当我们要对原函数求解时,可以直接求得$\\varphi(x)$的导函数$\\varphi^{'}(x)$令$\\varphi^{'}(x) = 0$时的结果就是原函数的解.所以对$\\varphi (x)$求导,消去常数项后得到公式如下:\n",
    "\n",
    "$$f^{\\prime}\\left(x_{k}\\right)+f^{\\prime \\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)=0$$\n",
    "\n",
    "经过变换就可以得到公式四：\n",
    "\n",
    "$$x_{k}=x_{k-1}-\\frac{f^{'}(x_{k-1}\\ \\ )}{f^{''}(x_{k-1}\\ \\ )}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、梯度下降的本质\n",
    "\n",
    "梯度下降其实也能使用泰勒展开式来解释，我们来看一阶泰勒展开式：\n",
    "\n",
    "$$f(x) \\approx \\varphi(x)=f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)$$\n",
    "\n",
    "我们知道梯度下降的公式是：\n",
    "\n",
    "$$x = x_k- \\eta f^{'}(x_k)$$\n",
    "\n",
    "在这里我们把x看做参数【类似于把感知器的x看做w】\n",
    "\n",
    "如果想让f(x)每次都减小，那么参数x该如何变化呢，只要每次$x_k\\rightarrow x$让函数值$f(x)-f(x_k)<0$就可以了。\n",
    "\n",
    "$$f(x)\\approx f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)$$\n",
    "\n",
    "整理得到：\n",
    "\n",
    "$$f(x)-f\\left(x_{k}\\right) \\approx f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right) <0$$\n",
    "\n",
    "$$f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)<0$$\n",
    "\n",
    "$x-x_{k}$是一个向量，既有大小又有方向，（大部分$x$都是多维的向量，即使是一维的，表示成标量或者向量也是一样的）大小就是步长$\\eta$，方向我们可以用单位向量$v$来表示。\n",
    "\n",
    "$$x-x_{k}=\\eta v$$\n",
    "\n",
    "所以$\\eta.v.f^{\\prime}\\left(x_{k}\\right)<0$。\n",
    "\n",
    "我们先来看下两个向量的积：\n",
    "\n",
    "<img src=\"imgs/vector_product.png\" width=\"400\"/>\n",
    "\n",
    "A和B均为向量，$\\alpha$为两个向量之间的夹角。A和B的乘积为：\n",
    "\n",
    "$$A \\cdot B=\\|A\\| \\cdot\\|B\\| \\cdot \\cos (\\alpha)$$\n",
    "\n",
    "\n",
    "||Al||和||B||均为标量，在||A||和||B||确定的情况下，<font color=\"red\">只要$cos(\\alpha)=-1$，即A和B完全反向，就能让A和B的向量乘积最小(负最大值)</font>。顾名思义，当$v$与$f^{\\prime}(x_{k})$互为反向，<font color=\"red\">即$v$为当前梯度方向的负方向的时候，能让$v.f^{\\prime}\\left(x_{k}\\right)$最大程度地小，也就保证了$v$的方向是局部下降最快的方向。</font>\n",
    "\n",
    "知道$v$是$f^{\\prime}(x_{k})$的反方向后,可直接得到:\n",
    "\n",
    "$$v=-\\frac{f^{\\prime}\\left(x_{k}\\right)}{\\left\\|f^{\\prime}\\left(x_{k}\\right)\\right\\|}$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$x-x_{k}=\\eta v=-\\eta \\frac{f^{\\prime}\\left(x_{k}\\right)}{\\left\\|f^{\\prime}\\left(x_{k}\\right)\\right\\|}$$\n",
    "\n",
    "因为$|f^{\\prime}\\left(x_{k}\\right)|$是标量可以归入$\\eta$里，所以得到：\n",
    "\n",
    "$$x=x_{k}-\\eta.f^{\\prime}\\left(x_{k}\\right)$$\n",
    "\n",
    "\n",
    "所以：\n",
    "\n",
    "1. 如果想让$f(x)$每次都局部下降最快（梯度下降），必须沿着梯度的反方向走\n",
    "\n",
    "2. 如果想让$f(x)$每次都局部上降最快（梯度上升），必须沿着梯度的方向走"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、拟牛顿法解决牛顿法面对多元函数，向量的二阶导难求的问题\n",
    "\n",
    "## 1、拟牛顿法条件\n",
    "\n",
    "在一元函数求解的问题中,我们可以很愉快的使用牛顿法求驻点。但我们知道，在机器学习的优化问题中，我们要优化的都是多元函数，所以$x$往往不是一个实数，而是一个向量。所以将牛顿求根法利用到机器学习中时，$x$是一个向量，$f^{'}(x)$也是一个向量，但是对$f^{'}(x)$求导以后得到的是一个矩阵，叫做Hessian矩阵。等价公式如下：\n",
    "\n",
    "$$\\mathrm{x}_{k+1}=\\mathrm{x}_{k}-H_{k}^{-1} \\cdot \\mathrm{g}_{k}, \\quad k=0,1, \\cdots$$\n",
    "\n",
    "公式中，$g_k$为公式四中的$f^{'}(x)$，$H_{k}^{-1}$代表二阶导函数的倒数.\n",
    "\n",
    "当$x$的维度特别多的时候，我们想求得$f^{''}(x)$是非常困难的。而牛顿法求驻点又是一个迭代算法，所以这个困难我们还要面临无限多次，导致了牛顿法求驻点在机器学习中无法使用。有没有什么解决办法呢？\n",
    "\n",
    "在牛顿法的迭代中，需要计算海森矩阵的逆矩阵$H^{-1}$，这一计算比较复杂，考虑用一个$n$阶矩阵$G_k=G(x^{(k)})$来近似代替$H_k^{(-1)}=H^{(-1)}(x^{(k)})$。这就是拟牛顿法的基本想法。\n",
    "\n",
    "要找到近似的替代矩阵，必定要和$H_k$有类似的性质。先看下牛顿法迭代中海森矩阵$H_k$满足的条件。首先$H_k$满足以下关系：\n",
    "\n",
    "取$x=x^{(k-1)}$，由\n",
    "\n",
    "$$f^{'}(x)=g_{k}+H_{k}\\left(x-x^{(k)}\\right)$$\n",
    "\n",
    "得\n",
    "\n",
    "$$g_{k-1}-g_{k}=H_{k}\\left(x^{(k-1)}-x^{(k)}\\right)$$\n",
    "\n",
    "记$y_{k}=g_{k}-g_{k-1}, \\quad \\delta_{k}=x^{(k)}-x^{(k-1)}$，则\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "y_{k-1}=H_{k} \\delta_{k-1} \\\\\n",
    "H_{k}^{-1} y_{k-1}=\\delta_{k-1}\n",
    "\\end{array}$$\n",
    "\n",
    "称为拟牛顿条件(Secant equation)。\n",
    "\n",
    "注：在李航老师《统计机器学习》附录B中，取的是$x=x^{(k+1)}$，得到的拟牛顿条件是$H_{k}^{-1} y_{k}=\\delta_{k}$，但是用$x=x^{(k-1)}$得到的拟牛顿条件和下面推导吻合，但还没找到原因，还望高手指教。\n",
    "\n",
    "其次，如果$H_k$是正定的（$H_k^{-1}$也是正定的），那么保证牛顿法的搜索方向$P_k$是下降方向。这是因为搜索方向是$P_k=-H_k^{-1}g_k$，\n",
    "由\n",
    "\n",
    "$$x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}$$\n",
    "\n",
    "有\n",
    "\n",
    "$$x=x^{(k)}-\\lambda H_{k}^{-1} g_{k}=x^{(k)}+\\lambda p_{k}$$\n",
    "\n",
    "则$f(x)$在$x^{(k)}$的泰勒展开可近似为\n",
    "\n",
    "$$f(x)=f\\left(x^{(k)}\\right)-\\lambda g_{k}^{T} H_{k}^{-1} g_{k}$$\n",
    "\n",
    "由于$H_{k}^{-1}$正定，故$g_{k}^{T} H_{k}^{-1} g_{k} \\gt 0$。当$\\lambda$为一个充分小的正数时，有$f(x) \\lt f(x^{(x)})$，即搜索方向$p_k$是下降方向。\n",
    "\n",
    "因此拟牛顿法将$G_k$作为$H_{k}^{-1}$近似。要求$G_k$满足同样的条件。首先，每次迭代矩阵$G_k$是正定的。同时，$G_k$满足下面的拟牛顿条件：\n",
    "\n",
    "$$G_{k+1} y_{k}=\\delta_{k}$$\n",
    "\n",
    "按照拟牛顿条件，在每次迭代中可以选择更新矩阵$G_{k+1}$：\n",
    "\n",
    "$$G_{k+1}=G_{k}+\\Delta G_{k}$$\n",
    "\n",
    "这种选择有一定的灵活性，因此有很多具体的实现方法能逼近$H^{-1}$：DFP、BFGS、L-BFGS等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO  https://zhuanlan.zhihu.com/p/37524275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、DFP\n",
    "\n",
    "## 3、BFGS\n",
    "\n",
    "## 4、L-BFGS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
