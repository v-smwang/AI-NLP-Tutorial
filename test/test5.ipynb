{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于平均感知器的分词器\n",
    "\n",
    "## 原理\n",
    "\n",
    "&emsp;&emsp;我们知道对于一句话进行分词，我们可以通过给每个字进行标注状态（B：分词词首；M：分词词中；E：分词词尾；S：单个词分词）来分词，然后去计算出每个字到底是什么状态，从而完成分词。\n",
    "\n",
    "&emsp;&emsp;那使用感知器，无非也就是用感知器来预测出每个字的4种状态的概率分别是多少，然后利用维特比算法找出整个句子最大概率的状态路径，从而完成分词。和hmm分词差不多，只是将hmm预测隐状态概率换成了使用感知器的多分类来预测隐状态概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class AveragedPerceptron(object):\n",
    "\n",
    "    '''An averaged perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            weights = self.weights[feat]\n",
    "            for label, weight in weights.items():\n",
    "                scores[label] += value * weight\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda label: (scores[label], label))\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "def train(nr_iter, examples):\n",
    "    '''Return an averaged perceptron model trained on ``examples`` for\n",
    "    ``nr_iter`` iterations.\n",
    "    '''\n",
    "    model = AveragedPerceptron()\n",
    "    for i in range(nr_iter):\n",
    "        random.shuffle(examples)\n",
    "        for features, class_ in examples:\n",
    "            scores = model.predict(features)\n",
    "            guess, score = max(scores.items(), key=lambda i: i[1])\n",
    "            if guess != class_:\n",
    "                model.update(class_, guess, features)\n",
    "    model.average_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "1、对每个句子中的每个字，取7种特征【unigrams+bigrams】\n",
    "\n",
    "> $C_{i-1}$ #前一个词 \\\n",
    "> $C_i$ #当前词 \\\n",
    "> $C_{i+1}$ #后一个词 \\\n",
    "> $C_{i-2}/C_{i-1}$ #前两个词与前一个词 \\\n",
    "> $C_{i-1}/C_i$ #前一个词与当前词 \\\n",
    "> $C_i/C_{i+1}$  #当前词与后一个词 \\\n",
    "> $C_{i+1}/C_{i+2}$  #后一个词与后两个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 1, 1, 2, 3], [3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3], [3, 2, 3, 3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_feature = {'BL=B':0, 'BL=M':1, 'BL=E':2, 'BL=S':3, 'BL=_BL_':4}  # 特征取值集合\n",
    "sentence_feature_list = []  # 记录每个句子中每个字的特征\n",
    "sentence_tag_list = []  # 每个字的类标\n",
    "START_CHAR = '\\1'\n",
    "END_CHAR = '\\2'\n",
    "\n",
    "count = 0\n",
    "\n",
    "def handle_feature(feature, char_feature, all_feature):\n",
    "    feature_id = all_feature[feature] if feature in all_feature else len(all_feature)\n",
    "    all_feature[feature] = feature_id\n",
    "    char_feature.append(feature_id)\n",
    "\n",
    "# weights = np.ones(())\n",
    "data_f = open('./data/RenMinData.txt', 'r', encoding='utf-8')\n",
    "for line in data_f.readlines():\n",
    "    count += 1\n",
    "    line = line.strip()\n",
    "    # 打标签\n",
    "    words = line.split(' ')\n",
    "    tag_list = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "        else:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "            for w in word[1:len(word)-1]: # 中间字\n",
    "                tag_list.append(all_feature['BL=M'])\n",
    "            tag_list.append(all_feature['BL=E'])\n",
    "    sentence_tag_list.append(tag_list)\n",
    "    \n",
    "    # 获取特征    \n",
    "    sentence = line.replace(' ','')\n",
    "    sentence_feature = []\n",
    "    for i, char in enumerate(sentence):\n",
    "        char_feature = []\n",
    "        # 前2\n",
    "        pre2 = sentence[i-2] if i>=2 else START_CHAR\n",
    "        # 前1\n",
    "        pre1 = sentence[i-1] if i>=1 else START_CHAR\n",
    "        # 当前\n",
    "        cur = char\n",
    "        # 后1\n",
    "        next1 = sentence[i+1] if i < len(sentence)-1 else END_CHAR\n",
    "        # 后2\n",
    "        next2 = sentence[i+2] if i < len(sentence)-2 else END_CHAR\n",
    "        \n",
    "        # unigrams\n",
    "        one = pre1+'1'\n",
    "        handle_feature(one, char_feature, all_feature)\n",
    "        \n",
    "        two = cur+'2'\n",
    "        handle_feature(two, char_feature, all_feature)\n",
    "        \n",
    "        three = next1+'3'\n",
    "        handle_feature(three, char_feature, all_feature)\n",
    "        \n",
    "        # bigrams\n",
    "        four = pre2+'/'+pre1+'4'\n",
    "        handle_feature(four, char_feature, all_feature)\n",
    "        \n",
    "        five = pre1+'/'+cur+'5'\n",
    "        handle_feature(five, char_feature, all_feature)\n",
    "        \n",
    "        six = cur+'/'+next1+'6'\n",
    "        handle_feature(six, char_feature, all_feature)\n",
    "        \n",
    "        seven = next1+'/'+next2+'7'\n",
    "        handle_feature(seven, char_feature, all_feature)\n",
    "        \n",
    "        sentence_feature.append(char_feature)\n",
    "        #print('char_feature',char_feature)\n",
    "    sentence_feature_list.append(sentence_feature)     \n",
    "print(sentence_tag_list[:3])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[0,[1,2,3]],[1,[1,2,3]]])\n",
    "b = [0,1]\n",
    "print(sum([a[0][b] for ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [1,2]\n",
    "c = [3,3]\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c', 2)\n"
     ]
    }
   ],
   "source": [
    "print(max([('a',2),('b',4),('c',2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297959\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-d2778215c7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_feature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_feature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_tag_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_list = [0,1,2,3]  # 标签id\n",
    "hidden_node_num = len(tag_list)  # 隐状态个数（标签个数）\n",
    "# 参数\n",
    "global W\n",
    "# W = np.random.randn(hidden_node_num, len(all_feature))\n",
    "W = np.zeros((hidden_node_num, len(all_feature)))\n",
    "             \n",
    "def viterbi_decode(sentence_feature, W):\n",
    "    path_prob = []  # 路径和该路径的概率\n",
    "    pre_X = []\n",
    "    best_path = []  # 最好的路径，也就是最好的tag\n",
    "    for i, char_feature in enumerate(sentence_feature):\n",
    "        \n",
    "        if i == 0:\n",
    "            char_feature.append(4)  # 前一个字的标签\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率\n",
    "            # 第一个tag是当前字的标签，第二个tag是上一个字的标签\n",
    "            path_prob.append({tag:(tag, prob) for tag, prob in enumerate(Z)})\n",
    "        else:\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率\n",
    "            res = {}\n",
    "            for j, prob in enumerate(Z):\n",
    "                # 加上上一个字的标签分别为0，1，2，3时的概率，然后求最大\n",
    "                max_prob, pre_tag =  max([(prob + W[j][pre_tag] + value[1], pre_tag) for pre_tag, value in path_prob[i-1].items()])\n",
    "                res[j] = (pre_tag, max_prob)\n",
    "            path_prob.append(res)\n",
    "    #print('path_prob:',path_prob)\n",
    "    # 确定最后一个字的tag，然后回溯，确定路径（确定每个字的tag）\n",
    "    last_tag = -1\n",
    "    max_prob = -100000\n",
    "    for tag in tag_list:\n",
    "        prob = path_prob[-1][tag][1]\n",
    "        if max_prob < prob:\n",
    "            last_tag = tag\n",
    "    #last_tag = sorted(, key=lambda x: x[1][1], reverse = True)[0][0]\n",
    "    #print('last_tag:', last_tag)\n",
    "    # last_tag = np.argmax([pp[1][1] for pp in path_prob[-1]])\n",
    "    best_path.append(last_tag)\n",
    "    for pp in reversed(path_prob):\n",
    "        best_path.append(pp[last_tag][0])\n",
    "        last_tag = pp[last_tag][0]\n",
    "    best_path.pop()\n",
    "    best_path.reverse()\n",
    "    #print('best_path:', best_path)\n",
    "    return best_path\n",
    "                \n",
    "\n",
    "def precision_and_recall(predict_sentence_tags, actual_sentence_tags):\n",
    "    correct_count = 0\n",
    "    for i in range(len(actual_sentence_tags)):\n",
    "        #print('predict_sentence_tags[i]:', predict_sentence_tags[i])\n",
    "        #print('actual_sentence_tags[i]:', actual_sentence_tags[i])\n",
    "        if predict_sentence_tags[i] == actual_sentence_tags[i]:\n",
    "            correct_count += 1\n",
    "    return correct_count / len(actual_sentence_tags)\n",
    "    \n",
    "\n",
    "import time\n",
    "# 训练\n",
    "def train(sentence_feature_list, sentence_tag_list, maxIteration, ratio = 0.7):\n",
    "    global W\n",
    "    print(len(sentence_feature_list))\n",
    "    end = int(np.floor(ratio*len(sentence_feature_list)))\n",
    "    print(end)\n",
    "    x_train = sentence_feature_list[:end]\n",
    "    x_val = sentence_feature_list[end:]\n",
    "    y_train = sentence_tag_list[:end]\n",
    "    y_val = sentence_tag_list[end:]\n",
    "    for iter in range(maxIteration):\n",
    "        start = time.time()\n",
    "        for i, sentence_feature in enumerate(x_train):\n",
    "#             print('i--', i)\n",
    "            if i % 10000 == 0:\n",
    "#                 print('10000次耗时：', time.time()-start)\n",
    "                start = time.time()\n",
    "                print('i--', i)\n",
    "#             start = time.time()\n",
    "            predict_tag = viterbi_decode(sentence_feature, W)\n",
    "#             print('viterbi_decode耗时：', time.time()-start)\n",
    "            actual_tag = y_train[i]\n",
    "#             print('predict_tag:', predict_tag)\n",
    "#             print('actual_tag:', actual_tag)\n",
    "            if predict_tag == actual_tag:\n",
    "                #print('true')\n",
    "                continue\n",
    "            else:\n",
    "                # 更新权重：每个特征上的实际标签权重增加(+1)，其他标签权重减少(-1)\n",
    "#                 start = time.time()\n",
    "                for j, char_feature in enumerate(sentence_feature):\n",
    "#                     print('predict_tag:', predict_tag)\n",
    "#                     print('actual_tag:', actual_tag)\n",
    "                    if predict_tag[j] == actual_tag[j]:\n",
    "                        continue\n",
    "#                     print('char_feature:',char_feature)\n",
    "                    for tag in tag_list:\n",
    "                        if tag == actual_tag[j]:\n",
    "                            #print('W[tag][char_feature] == ', W[tag][char_feature])\n",
    "                            W[tag][char_feature] += 1\n",
    "                            if j == 0:  # 第一个字\n",
    "                                W[tag][4] += 1\n",
    "                            else:\n",
    "                                W[tag][actual_tag[j-1]] += 1\n",
    "                            #print('W[tag][char_feature] ++1 == ', W[tag][char_feature])\n",
    "                        elif tag == predict_tag[j]:\n",
    "                            W[tag][char_feature] -= 1\n",
    "                            if j == 0:  # 第一个字\n",
    "                                W[tag][4] -= 1\n",
    "                            else:\n",
    "                                W[tag][predict_tag[j-1]] -= 1\n",
    "#                 print('W耗时：', time.time()-start)\n",
    "\n",
    "        # 本轮次训练集和验证集上的结果\n",
    "        train_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_train[:100]]\n",
    "        print('训练集准确率：', precision_and_recall(train_predict_sentence_tags[:100], y_train[:100]))\n",
    "        val_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_val]\n",
    "        print('验证集准确率：', precision_and_recall(val_predict_sentence_tags, y_val))\n",
    "\n",
    "print(len(sentence_feature_list))  \n",
    "result_dict = dict(zip(sentence_feature_list, sentence_tag_list))\n",
    "import random\n",
    "random.shuffle(result_dict)\n",
    "sentence_feature_list_shuffle = result_dict.keys()\n",
    "sentence_tag_list_shuffle = result_dict.values()\n",
    "sentence_feature_list2 = sentence_feature_list_shuffle[:40000]\n",
    "sentence_tag_list2 = sentence_tag_list_shuffle[:40000]\n",
    "\n",
    "train(sentence_feature_list_shuffle, sentence_tag_list_shuffle, 3, ratio = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 193, 194, 8, 793, 197, 794, 4, 4, 4, 4, 4, 4], [199, 200, 629, 795, 203, 796, 797], [206, 635, 798, 208, 799, 800, 801], [640, 802, 803, 804, 805, 806, 807], [808, 809, 810, 811, 812, 813, 814], [815, 816, 276, 817, 818, 819, 820], [821, 282, 822, 823, 824, 825, 826], [286, 827, 364, 828, 829, 830, 831], [832, 370, 833, 834, 835, 836, 837], [376, 838, 124, 839, 840, 841, 128], [842, 130, 42, 843, 844, 133, 46]]\n",
      "[3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[[5, 1127, 13765, 8, 11196, 178760, 663255, 4, 4, 4], [1133, 13769, 1875, 11198, 178761, 663256, 782791], [13772, 1880, 25248, 178762, 663257, 782793, 25252], [1886, 25254, 16351, 663259, 782795, 25257, 132857], [25259, 16356, 35, 782796, 25261, 132859, 39], [16362, 41, 42, 25263, 132860, 45, 46]]\n",
      "[3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "print(sentence_feature_list[10])\n",
    "print(sentence_tag_list[10])\n",
    "random.seed(1)\n",
    "random.shuffle(sentence_feature_list)\n",
    "random.shuffle(sentence_tag_list)\n",
    "print(sentence_feature_list[10])\n",
    "print(sentence_tag_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709877\n"
     ]
    }
   ],
   "source": [
    "print(max(all_feature.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "0.0004749298095703125\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print(W[1][1])\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.]\n",
      " [-1. -2.]\n",
      " [-1.  0.]\n",
      " [-1.  0.]\n",
      " [-1.  0.]\n",
      " [-1.  0.]\n",
      " [-1.  0.]]\n",
      "0.0006079673767089844\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print(W.take([[1,1],[1,3],[1,5],[1,7],[1,9],[1,44],[1,445]]))\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0002579689025878906\n",
      "(4, 1709878)\n"
     ]
    }
   ],
   "source": [
    "arr = [[0 for x in range(len(all_feature))] for y in range(hidden_node_num)]\n",
    "s = time.time()\n",
    "print(arr[1][1])\n",
    "e = time.time()\n",
    "print(e-s)\n",
    "print(np.array(arr).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297959\n",
      "297959\n",
      "208571\n",
      "10000次耗时： 4.76837158203125e-06\n",
      "i-- 0\n",
      "10000次耗时： 2.3740899562835693\n",
      "i-- 10000\n",
      "10000次耗时： 2.0779740810394287\n",
      "i-- 20000\n",
      "10000次耗时： 1.9559738636016846\n",
      "i-- 30000\n",
      "10000次耗时： 1.9666359424591064\n",
      "i-- 40000\n",
      "10000次耗时： 1.8721880912780762\n",
      "i-- 50000\n",
      "10000次耗时： 1.891402006149292\n",
      "i-- 60000\n",
      "10000次耗时： 1.8912060260772705\n",
      "i-- 70000\n",
      "10000次耗时： 2.078833818435669\n",
      "i-- 80000\n",
      "10000次耗时： 1.8802359104156494\n",
      "i-- 90000\n",
      "10000次耗时： 1.9765539169311523\n",
      "i-- 100000\n",
      "10000次耗时： 1.9043879508972168\n",
      "i-- 110000\n",
      "10000次耗时： 1.8886809349060059\n",
      "i-- 120000\n",
      "10000次耗时： 1.8028919696807861\n",
      "i-- 130000\n",
      "10000次耗时： 1.9215140342712402\n",
      "i-- 140000\n",
      "10000次耗时： 2.02199125289917\n",
      "i-- 150000\n",
      "10000次耗时： 1.937804937362671\n",
      "i-- 160000\n",
      "10000次耗时： 1.860177993774414\n",
      "i-- 170000\n",
      "10000次耗时： 1.8545746803283691\n",
      "i-- 180000\n",
      "10000次耗时： 1.8961849212646484\n",
      "i-- 190000\n",
      "10000次耗时： 1.8013789653778076\n",
      "i-- 200000\n",
      "训练集准确率： 0.85\n",
      "验证集准确率： 0.7147044345997225\n",
      "10000次耗时： 1.5020370483398438e-05\n",
      "i-- 0\n",
      "10000次耗时： 1.9403977394104004\n",
      "i-- 10000\n",
      "10000次耗时： 1.963986873626709\n",
      "i-- 20000\n",
      "10000次耗时： 2.0211379528045654\n",
      "i-- 30000\n",
      "10000次耗时： 2.0358049869537354\n",
      "i-- 40000\n",
      "10000次耗时： 1.8273401260375977\n",
      "i-- 50000\n",
      "10000次耗时： 1.8383541107177734\n",
      "i-- 60000\n",
      "10000次耗时： 1.8550989627838135\n",
      "i-- 70000\n",
      "10000次耗时： 1.9115779399871826\n",
      "i-- 80000\n",
      "10000次耗时： 1.838937759399414\n",
      "i-- 90000\n",
      "10000次耗时： 2.269411087036133\n",
      "i-- 100000\n",
      "10000次耗时： 2.05138897895813\n",
      "i-- 110000\n",
      "10000次耗时： 1.8674860000610352\n",
      "i-- 120000\n",
      "10000次耗时： 1.7624258995056152\n",
      "i-- 130000\n",
      "10000次耗时： 1.8614280223846436\n",
      "i-- 140000\n",
      "10000次耗时： 1.8871901035308838\n",
      "i-- 150000\n",
      "10000次耗时： 1.8678491115570068\n",
      "i-- 160000\n",
      "10000次耗时： 1.8902289867401123\n",
      "i-- 170000\n",
      "10000次耗时： 1.9194469451904297\n",
      "i-- 180000\n",
      "10000次耗时： 1.9997689723968506\n",
      "i-- 190000\n",
      "10000次耗时： 1.8097779750823975\n",
      "i-- 200000\n",
      "训练集准确率： 0.91\n",
      "验证集准确率： 0.7627869512686266\n",
      "10000次耗时： 4.291534423828125e-06\n",
      "i-- 0\n",
      "10000次耗时： 1.9420511722564697\n",
      "i-- 10000\n",
      "10000次耗时： 1.9730267524719238\n",
      "i-- 20000\n",
      "10000次耗时： 2.0600671768188477\n",
      "i-- 30000\n",
      "10000次耗时： 2.0738871097564697\n",
      "i-- 40000\n",
      "10000次耗时： 1.8527779579162598\n",
      "i-- 50000\n",
      "10000次耗时： 1.9177019596099854\n",
      "i-- 60000\n",
      "10000次耗时： 1.8956849575042725\n",
      "i-- 70000\n",
      "10000次耗时： 1.9288549423217773\n",
      "i-- 80000\n",
      "10000次耗时： 2.2335381507873535\n",
      "i-- 90000\n",
      "10000次耗时： 2.1484427452087402\n",
      "i-- 100000\n",
      "10000次耗时： 1.9151501655578613\n",
      "i-- 110000\n",
      "10000次耗时： 2.0154600143432617\n",
      "i-- 120000\n",
      "10000次耗时： 1.9229152202606201\n",
      "i-- 130000\n",
      "10000次耗时： 2.1640169620513916\n",
      "i-- 140000\n",
      "10000次耗时： 2.0674021244049072\n",
      "i-- 150000\n",
      "10000次耗时： 2.2106728553771973\n",
      "i-- 160000\n",
      "10000次耗时： 2.1591267585754395\n",
      "i-- 170000\n",
      "10000次耗时： 2.0903260707855225\n",
      "i-- 180000\n",
      "10000次耗时： 2.1011922359466553\n",
      "i-- 190000\n",
      "10000次耗时： 1.9925870895385742\n",
      "i-- 200000\n",
      "训练集准确率： 0.97\n",
      "验证集准确率： 0.7723743679241062\n",
      "10000次耗时： 5.0067901611328125e-06\n",
      "i-- 0\n",
      "10000次耗时： 2.170403003692627\n",
      "i-- 10000\n",
      "10000次耗时： 2.120753765106201\n",
      "i-- 20000\n",
      "10000次耗时： 2.0510499477386475\n",
      "i-- 30000\n",
      "10000次耗时： 2.0288078784942627\n",
      "i-- 40000\n",
      "10000次耗时： 1.914830207824707\n",
      "i-- 50000\n",
      "10000次耗时： 1.8922979831695557\n",
      "i-- 60000\n",
      "10000次耗时： 1.8659331798553467\n",
      "i-- 70000\n",
      "10000次耗时： 1.8936707973480225\n",
      "i-- 80000\n",
      "10000次耗时： 1.85107421875\n",
      "i-- 90000\n",
      "10000次耗时： 1.941046953201294\n",
      "i-- 100000\n",
      "10000次耗时： 1.8937342166900635\n",
      "i-- 110000\n",
      "10000次耗时： 1.8957409858703613\n",
      "i-- 120000\n",
      "10000次耗时： 1.7802181243896484\n",
      "i-- 130000\n",
      "10000次耗时： 1.8724069595336914\n",
      "i-- 140000\n",
      "10000次耗时： 1.9050328731536865\n",
      "i-- 150000\n",
      "10000次耗时： 1.8672819137573242\n",
      "i-- 160000\n",
      "10000次耗时： 1.9804587364196777\n",
      "i-- 170000\n",
      "10000次耗时： 1.8744010925292969\n",
      "i-- 180000\n",
      "10000次耗时： 1.9203200340270996\n",
      "i-- 190000\n",
      "10000次耗时： 1.8235828876495361\n",
      "i-- 200000\n",
      "训练集准确率： 0.97\n",
      "验证集准确率： 0.8044703987112364\n",
      "10000次耗时： 4.0531158447265625e-06\n",
      "i-- 0\n",
      "10000次耗时： 1.9667689800262451\n",
      "i-- 10000\n",
      "10000次耗时： 1.9977898597717285\n",
      "i-- 20000\n",
      "10000次耗时： 1.967240810394287\n",
      "i-- 30000\n",
      "10000次耗时： 2.0140421390533447\n",
      "i-- 40000\n",
      "10000次耗时： 2.0268378257751465\n",
      "i-- 50000\n",
      "10000次耗时： 2.1149661540985107\n",
      "i-- 60000\n",
      "10000次耗时： 2.1730332374572754\n",
      "i-- 70000\n",
      "10000次耗时： 2.1221189498901367\n",
      "i-- 80000\n",
      "10000次耗时： 2.091871976852417\n",
      "i-- 90000\n",
      "10000次耗时： 2.189911127090454\n",
      "i-- 100000\n",
      "10000次耗时： 2.0142672061920166\n",
      "i-- 110000\n",
      "10000次耗时： 1.8696620464324951\n",
      "i-- 120000\n",
      "10000次耗时： 1.7869770526885986\n",
      "i-- 130000\n",
      "10000次耗时： 1.8812649250030518\n",
      "i-- 140000\n",
      "10000次耗时： 1.9143579006195068\n",
      "i-- 150000\n",
      "10000次耗时： 1.8848249912261963\n",
      "i-- 160000\n",
      "10000次耗时： 1.9355700016021729\n",
      "i-- 170000\n",
      "10000次耗时： 1.8722150325775146\n",
      "i-- 180000\n",
      "10000次耗时： 1.9452219009399414\n",
      "i-- 190000\n",
      "10000次耗时： 1.8352398872375488\n",
      "i-- 200000\n",
      "训练集准确率： 0.97\n",
      "验证集准确率： 0.8166644292298743\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_list = [0,1,2,3]  # 标签id\n",
    "hidden_node_num = len(tag_list)  # 隐状态个数（标签个数）\n",
    "# 参数\n",
    "global W\n",
    "W = [0 for i in range(hidden_node_num*len(all_feature))]\n",
    "             \n",
    "def viterbi_decode(sentence_feature, W):\n",
    "    path_prob = []  # 路径和该路径的概率\n",
    "    pre_X = []\n",
    "    best_path = []  # 最好的路径，也就是最好的tag\n",
    "    for i, char_feature in enumerate(sentence_feature):\n",
    "        \n",
    "        if i == 0:\n",
    "            char_feature.append(4)  # 前一个字的标签\n",
    "            Z = [0 for i in range(4)]   # 每个tag的概率\n",
    "            for tag in tag_list:\n",
    "                for feature in char_feature:\n",
    "                    Z[tag] += W[tag*feature]\n",
    "            # 第一个tag是当前字的标签，第二个tag是上一个字的标签\n",
    "            path_prob.append({tag:(tag, prob) for tag, prob in enumerate(Z)})\n",
    "        else:\n",
    "            Z = [0 for i in range(4)]   # 每个tag的概率\n",
    "            for tag in tag_list:\n",
    "                for feature in char_feature:\n",
    "                    Z[tag] += W[tag*feature]\n",
    "            res = {}\n",
    "            for j, prob in enumerate(Z):\n",
    "                # 加上上一个字的标签分别为0，1，2，3时的概率，然后求最大\n",
    "                max_prob, pre_tag =  max([(prob + W[j*pre_tag] + value[1], pre_tag) for pre_tag, value in path_prob[i-1].items()])\n",
    "                res[j] = (pre_tag, max_prob)\n",
    "            path_prob.append(res)\n",
    "    #print('path_prob:',path_prob)\n",
    "    # 确定最后一个字的tag，然后回溯，确定路径（确定每个字的tag）\n",
    "    last_tag = -1\n",
    "    max_prob = -100000\n",
    "    for tag in tag_list:\n",
    "        prob = path_prob[-1][tag][1]\n",
    "        if max_prob < prob:\n",
    "            last_tag = tag\n",
    "    #last_tag = sorted(, key=lambda x: x[1][1], reverse = True)[0][0]\n",
    "    #print('last_tag:', last_tag)\n",
    "    # last_tag = np.argmax([pp[1][1] for pp in path_prob[-1]])\n",
    "    best_path.append(last_tag)\n",
    "    for pp in reversed(path_prob):\n",
    "        best_path.append(pp[last_tag][0])\n",
    "        last_tag = pp[last_tag][0]\n",
    "    best_path.pop()\n",
    "    best_path.reverse()\n",
    "    #print('best_path:', best_path)\n",
    "    return best_path\n",
    "                \n",
    "\n",
    "def precision_and_recall(predict_sentence_tags, actual_sentence_tags):\n",
    "    correct_count = 0\n",
    "    for i in range(len(actual_sentence_tags)):\n",
    "        #print('predict_sentence_tags[i]:', predict_sentence_tags[i])\n",
    "        #print('actual_sentence_tags[i]:', actual_sentence_tags[i])\n",
    "        if predict_sentence_tags[i] == actual_sentence_tags[i]:\n",
    "            correct_count += 1\n",
    "    return correct_count / len(actual_sentence_tags)\n",
    "    \n",
    "\n",
    "import time\n",
    "# 训练\n",
    "def train(sentence_feature_list, sentence_tag_list, maxIteration, ratio = 0.7):\n",
    "    global W\n",
    "    print(len(sentence_feature_list))\n",
    "    end = int(np.floor(ratio*len(sentence_feature_list)))\n",
    "    print(end)\n",
    "    x_train = sentence_feature_list[:end]\n",
    "    x_val = sentence_feature_list[end:]\n",
    "    y_train = sentence_tag_list[:end]\n",
    "    y_val = sentence_tag_list[end:]\n",
    "    for iter in range(maxIteration):\n",
    "        start = time.time()\n",
    "        for i, sentence_feature in enumerate(x_train):\n",
    "#             print('i--', i)\n",
    "            if i % 10000 == 0:\n",
    "                print('10000次耗时：', time.time()-start)\n",
    "                start = time.time()\n",
    "                print('i--', i)\n",
    "#             start = time.time()\n",
    "            predict_tag = viterbi_decode(sentence_feature, W)\n",
    "#             print('viterbi_decode耗时：', time.time()-start)\n",
    "            actual_tag = y_train[i]\n",
    "#             print('predict_tag:', predict_tag)\n",
    "#             print('actual_tag:', actual_tag)\n",
    "            if predict_tag == actual_tag:\n",
    "                #print('true')\n",
    "                continue\n",
    "            else:\n",
    "                # 更新权重：每个特征上的实际标签权重增加(+1)，其他标签权重减少(-1)\n",
    "#                 start = time.time()\n",
    "                for j, char_feature in enumerate(sentence_feature):\n",
    "#                     print('predict_tag:', predict_tag)\n",
    "#                     print('actual_tag:', actual_tag)\n",
    "                    if predict_tag[j] == actual_tag[j]:\n",
    "                        continue\n",
    "#                     print('char_feature:',char_feature)\n",
    "                    for tag in tag_list:\n",
    "                        if tag == actual_tag[j]:\n",
    "                            #print('W[tag][char_feature] == ', W[tag][char_feature])\n",
    "                            for feature in char_feature:\n",
    "                                W[tag*feature] += 1\n",
    "                            if j == 0:\n",
    "                                W[tag*4] += 1\n",
    "                            else:\n",
    "                                W[tag*predict_tag[j-1]] += 1\n",
    "                            #print('W[tag][char_feature] ++1 == ', W[tag][char_feature])\n",
    "                        elif tag == predict_tag[j]:\n",
    "                            for feature in char_feature:\n",
    "                                W[tag*feature] -= 1\n",
    "                            if j == 0:\n",
    "                                W[tag*4] -= 1\n",
    "                            else:\n",
    "                                W[tag*predict_tag[j-1]] -= 1\n",
    "#                 print('W耗时：', time.time()-start)\n",
    "\n",
    "        # 本轮次训练集和验证集上的结果\n",
    "        train_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_train[:100]]\n",
    "        print('训练集准确率：', precision_and_recall(train_predict_sentence_tags[:100], y_train[:100]))\n",
    "        val_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_val]\n",
    "        print('验证集准确率：', precision_and_recall(val_predict_sentence_tags, y_val))\n",
    "\n",
    "print(len(sentence_feature_list))  \n",
    "sentence_feature_list2 = sentence_feature_list[:40000]\n",
    "sentence_tag_list2 = sentence_tag_list[:40000]\n",
    "train(sentence_feature_list, sentence_tag_list, 5, ratio = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-e7696cd90931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "a[10][1] = 10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/Users/a1/Downloads/intro.txt','r',encoding='utf-8') as f:\n",
    "    i=0\n",
    "    for line in f.readlines():\n",
    "        i+=1\n",
    "        line = line.strip()\n",
    "        with open('/Users/a1/Downloads/intro/'+str(i)+'.ann','a+',encoding='utf-8') as wf:\n",
    "            wf.write('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
