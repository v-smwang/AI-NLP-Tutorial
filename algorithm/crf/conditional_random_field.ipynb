{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 条件随机场\n",
    "\n",
    "    序列标注模型条件随机场，这种模型与感知机同属结构化学习大家族，但性能比感知机还要强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、机器学习的模型谱系\n",
    "\n",
    "<img src=\"./imgs/machine_learning_model.png\" alt=\"机器学习的模型谱系\" width=\"800\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    生成式模型包含的算法：\n",
    "\n",
    "> 朴素贝叶斯\\\n",
    "K近邻\\\n",
    "混合高斯模型\\\n",
    "隐马尔科夫模型\\\n",
    "贝叶斯网络\\\n",
    "Sigmoid Belief Networks\\\n",
    "马尔科夫随机场(markov random fields)\\\n",
    "深度信念网络\\\n",
    "LDA文档主题生成模型\n",
    "\n",
    "    判别式模型包含的算法：\n",
    "\n",
    "> 线性回归(linear regression)\\\n",
    "逻辑回归(logistic regression)\\\n",
    "线性判别分析\\\n",
    "支持向量机\\\n",
    "CART(classification and regression tree)\\\n",
    "神经网络(NN)\\\n",
    "高斯过程(gaussian process)\\\n",
    "条件随机场(conditional random field)\n",
    "    \n",
    "【假设我们有训练数据(X,Y)，X是属性集合，Y是类别标记。这时来了一个新的样本x，我们想要预测它的类别y。\n",
    "    我们最终的目的是求得最大的条件概率P(y|x)作为新样本的分类。】\n",
    "    \n",
    "根据建模的究竟是联合概率（同时发生的概率）分布P(x,y)还是条件概率分布P(y|x)。派生出生成式模型与判别式模型，这两个模型最终算的都是\n",
    "    P(y|x)\n",
    "\n",
    "### 1、生成式模型\n",
    "\n",
    "    【一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。比如说类别标签有｛猫，狗，猪｝，那首先根据猫的特征学习出一个猫的模型，再根据狗的特征学习出狗的模型，之后分别计算新样本 x 跟三个类别的联合概率 P(x,y) ，然后根据贝叶斯公式计算出P(y|x)】\n",
    "\n",
    "    生成式模型：模拟数据的生成过程，两类随机变量存在因果先后关系，先有因素 y，后有结果 x，这种因果关系由联合分布模拟：\n",
    "\n",
    "$$P(x,y) = P(y)P(x|y)$$\n",
    "\n",
    "    通过联合分布 P(x,y)，生成式模型其实间接建模了 P(x)：\n",
    "    \n",
    "$$P(x) = \\sum_{y \\in Y}P(x,y)$$\n",
    "\n",
    "    然后依据贝叶斯公式可以得到P(y|x):\n",
    "    \n",
    "$$P(y|x) = \\frac{P(x,y)}{P(x)}$$\n",
    "\n",
    "    这里有两个缺陷:\n",
    "\n",
    "    1、P(x) 很难准确估计，因为特征之间并非相互独立，而是存在错综复杂的依赖关系。\n",
    "    2、P(x) 在分类中也没有直接作用。\n",
    "\n",
    "    为了克服这两个问题，判别式模型出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、判别式模型\n",
    "\n",
    "    【根据训练数据得到分类函数和分界面，比如说根据SVM模型得到一个分界面，然后直接计算条件概率 P(y|x) ，我们将最大的 P(y|x) 作为新样本的分类。判别式模型是对条件概率建模，学习不同类别之间的最优边界，无法反映训练数据本身的特性，能力有限，其只能告诉我们分类的类别。】\n",
    "\n",
    "<font color='red'>&emsp;&emsp;判别式模型直接跳过了 P(x)，直接对条件概率 P(y|x) 建模。不管 x 内部存在多复杂的关系，也不影响判别式模型对 y 的判断，于是就能够放心大胆的利用各种各样丰富的、有关联的特征。 所以我们会看到感知机分词的准确率高于隐马尔可夫模型。</font>\n",
    "\n",
    "$$P(y|x) = \\frac{exp(score(x,y))}{\\sum_{x,y}exp(score(x,y))} \\ \\ \\ \\ 说明：exp是指数函数$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1、举例说明\n",
    "\n",
    "    假设要对猫和狗进行区分：\n",
    "\n",
    "    生成式模型：是根据狗的特征首先学习出一个狗的模型，然后根据猫的特征学习出一个猫的模型，然后从这个动物中提取特征，放到狗模型中看概率是多少，再放到猫模型中看概率是多少，哪个大就是哪个。\n",
    "    \n",
    "    判别式模型：用判别式模型的方法是从历史数据中学习到模型，然后通过提取该动物的特征来预测出是狗的概率，是猫的概率。\n",
    "    \n",
    "### 2.2、数据计算过程样例\n",
    "\n",
    "    假设现在有一个分类问题，X是特征，Y是类标记。用判别式模型学习一个条件概率分布P(y|x)，用生成式模型学习一个联合概率分布 P(x,y) 。用一个简单的例子来说明这个问题。假设X就是两个特征（1或2），Y有两类（0或1），有如下训练样本（1，0）、（1，0）、（1，1）、（2，1）。\n",
    "\n",
    "    而学习到的联合概率分布（生成式模型）如下：\n",
    "| | 0 | 1 |\n",
    "|-|-|-|\n",
    "|1|1/2|1/4|\n",
    "|2|0|1/4|\n",
    "\n",
    "    则学习到的条件概率分布（判别式模型）如下：\n",
    "    \n",
    "| | 0 | 1 |\n",
    "|-|-|-|\n",
    "|1|2/3|1/3|\n",
    "|2|0|1|\n",
    "\n",
    "    在实际分类问题中，判别式模型可以直接用来判断特征的类别情况；而生成式模型需要加上贝叶斯公式，然后应用到分类中。但是，生成式模型的概率分布可以有其他应用，就是说生成式模型更一般更普适。不过判别式模型更直接，更简单。两种方法目前交叉较多。由生成式模型可以得到判别式模型，但由判别式模型得不到生成式模型。\n",
    "\n",
    "### 2.3、用图说明区别\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/discriminative_vs_generative.png\" alt=\"判别式和生成式模型比对\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    上图左边为判别式模型而右边为生成式模型，可以很清晰地看到差别，判别式模型是在寻找一个决策边界，通过该边界来将样本划分到对应类别。而生成式模型则不同，它学习了每个类别的边界，它包含了更多信息，可以用来生成样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4、两个模型总结\n",
    "\n",
    "    不管是生成式模型还是判别式模型，它们最终的判断依据都是条件概率P(y|x)，但是生成式模型先计算了联合概率P(x,y)，再由贝叶斯公式计算得到条件概率。因此，生成式模型可以体现更多数据本身的分布信息，其普适性更广。\n",
    "\n",
    "####  &emsp; &emsp;生成式模型的特点：\n",
    "    \n",
    "    生成式模型学习的是联合概率密度分布 P(X,Y)，可以从统计的角度表示分布的情况，能够反映同类数据本身的相似度，它不关心到底划分不同类的边界在哪里。生成式模型的学习收敛速度更快，当样本容量增加时，学习到的模型可以更快的收敛到真实模型，当存在隐变量时，依旧可以用生成式模型，此时判别式方法就不行了。具体来说，有以下特点：\n",
    "> 对联合概率建模，学习所有分类数据的分布。\\\n",
    "> 学习到的数据本身信息更多，能反应数据本身特性。\\\n",
    "> 学习成本较高，需要更多的计算资源。\\\n",
    "> 需要的样本数更多，样本较少时学习效果较差。\\\n",
    "> 推断时性能较差。\\\n",
    "> 一定条件下能转换成判别式。\n",
    "\n",
    "#### &emsp; &emsp;判别式模型特点：\n",
    "    \n",
    "    判别式模型直接学习决策函数 Y=f(X) 或者条件概率 P(Y|X) ，不能反映训练数据本身的特性，但它寻找不同类别之间的最优分裂面，反映的是异类数据之间的差异，直接面对预测往往学习准确度更高。具体来说有以下特点：\n",
    "        \n",
    "> 对条件概率建模，学习不同类别之间的最优边界。\\\n",
    "> 捕捉不同类别特征的差异信息，不学习本身分布信息，无法反应数据本身特性。\\\n",
    "> 学习成本较低，需要的计算资源较少。\\\n",
    "> 需要的样本数可以较少，少样本也能很好学习。\\\n",
    "> 预测时拥有较好性能。\\\n",
    "> 无法转换成生成式。\n",
    "\n",
    "总之，判别式模型和生成式模型都是使后验概率最大化，判别式是直接对后验概率建模，而生成式模型通过贝叶斯定理这一“桥梁”使问题转化为求联合概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、有向概率图模型\n",
    "\n",
    "    概率图模型能够分析错综复杂的随机变量关系。\n",
    "\n",
    "    有向概率图模型也称为贝叶斯网络。\n",
    "    \n",
    "    贝叶斯网络的一个基本要求是图必须是有向无环图（DAG/directed acyclic graph）。\n",
    "    \n",
    "    概率图模型( Probabilistic Graphical Model, PGM)是用来表示与推断多维随机变量联合分布 P(X,Y) 的强大框架，被广泛用于计算机视觉、知识表达、贝叶斯统计与自然语言处理。它利用节点 V 来表示随机变量，用边 E 连接有关联的随机变量，将多维随机变量分布表示为图 G=(V,E)。这样就带来了一个好处，那就是整个图可以分解为子图再进行分析.子图中的随机变量更少，建模更加简单。具体如何分解，据此派生出有向图模型和无向图模型。\n",
    "\n",
    "    有向图模型按事件的先后因果顺序将节点连接为有向图。如果事件 A 导致事件 B，则用箭头连接两个事件 A-->B。\n",
    "\n",
    "<img src=\"./imgs/PGM.png\" alt=\"有向概率图模型\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    有向图模型都将概率有向图分解为一系列条件概率之积，有向图模型经常用生成式模型来实现。定义 π(v) 表示节点 v 的所有前驱节点，则分布为:\n",
    "\n",
    "$$P(X,Y)=\\prod_{v=V}P(v|π(v))$$\n",
    "\n",
    "#### 3.1、有向概率图模型有啥用\n",
    "    \n",
    "    很多现实问题都可以使用有向概率图进行建模。\n",
    "    \n",
    "    例如：\n",
    "    \n",
    "<img src=\"./imgs/student_network.png\" alt=\"学生网络模型\" width=\"500\" align=\"center\"/>    \n",
    "\n",
    "    这个图描述了某个学生注册某个大学课程的设定。该图中有 5 个随机变量：\n",
    "> 课程的难度（Difficulty）：可取两个值，0 表示低难度，1 表示高难度\\\n",
    "    学生的智力水平（Intelligence）：可取两个值，0 表示不聪明，1 表示聪明\\\n",
    "    学生的评级（Grade）：可取三个值，1 表示差，2 表示中，3 表示优\\\n",
    "    学生的 SAT 成绩（SAT）：可取两个值，0 表示低分，1 表示高分\\\n",
    "    在完成该课程后学生从教授那里所得到的推荐信的质量（Letter）：可取两个值，0 表示推荐信不好，1 表示推荐信很好\n",
    "    \n",
    "    该图中的边编码了这些变量之间的依赖关系。\n",
    "    学生的 Grade 取决于课程的 Difficulty 和学生的 Intelligence；\n",
    "    而 Grade 又反过来决定了学生能否从教授那里得到一份好的 Letter；\n",
    "    另外，学生的 Intelligence 除了会影响他们的 Grade，还会影响他们的 SAT 分数。\n",
    "    注意其中箭头的方向表示了因果关系——Intelligence 会影响 SAT 分数，但 SAT 不会影响 Intelligence。\n",
    "    我们看到的每个节点关联的表格，它们的正式名称是条件概率分布（CPD/conditional probability distribution）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、无向概率图模型\n",
    "    \n",
    "    无向概率图模型也称为马尔科夫网络。\n",
    "\n",
    "    相反，无向图模型则不探究每个事件的因果关系，也就是说不涉及条件概率分解。无向图模型的边没有方向，仅仅代表两个事件有关联。\n",
    "    \n",
    "<img src=\"./imgs/PGM_without_direction.png\" alt=\"无向概率图模型\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    无向概率图模型的几个概念\n",
    "    \n",
    "<img src=\"./imgs/PGM_without_direction2.png\" alt=\"无向概率图模型例子\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.1、团（clique）\n",
    "\n",
    "<img src=\"./imgs/tuan.png\" alt=\"团\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.2、极大团（maximum clique）\n",
    "\n",
    "<img src=\"./imgs/maximum_tuan.png\" alt=\"极大团\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "#### 4.3、最大团 （maximul clique）\n",
    "\n",
    "<img src=\"./imgs/max_tuan.png\" alt=\"最大团\" width=\"400\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    ">团：必须两两相连（任意两点都相连）\\\n",
    "极大团：无法再加入一个节点使其成为团\\\n",
    "最大团：极大团里节点最多的团\n",
    "\n",
    "#### 4.4、因子节点\n",
    "    \n",
    "<img src=\"./imgs/factor.png\" alt=\"最大团\" width=\"150\" align=\"center\"/>    \n",
    "\n",
    "    因子节点是两个点间虚拟出来的点，用来辅助计算【暂时这么理解】。\n",
    "    \n",
    "    因子分解：将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数乘积形式的操作\n",
    "    \n",
    "    依据Hammersley- Clifford定理，无向图模型的联合概率分布表示为其最大团上的随机变量的函数乘积形式\n",
    "    \n",
    "$$P(X,Y) = \\frac{1}{Z}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "其中，a 是因子节点，$\\Psi_{a}$ 则是一个因子节点对应的函数，参数 $X_{a}, Y_{a}$ 是与因子节点相连的所有变量节点。为了将式子约束为概率分布，定义常数 Z 为如下归一化因子:\n",
    "\n",
    "$$Z = \\sum_{x,y}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "    在机器学习中，常用指数家族的因子函数:\n",
    "$$\\Psi_{a}(X_{a}, Y_{a}) = exp\\left\\{\\sum_{k}W_{ak}F_{ak}(X_{a}, Y_{a})\\right\\}$$\n",
    "\n",
    "其中，k 为特征的编号，$F_{ak}$ 是特征函数，$W_{ak}$ 为相应的特征权重。\n",
    "\n",
    "判别式模型经常用无向图来表示，只需要在归一化时，对每种 x 都求一个归一化因子:\n",
    "\n",
    "$$Z(X) = \\sum_{y}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "然后 P(X,Y) 就转化为判别式模型所需的条件概率分布:\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$$\n",
    "\n",
    "到这里，最后一个公式就是条件随机场的一般形式。\n",
    "\n",
    "### 5、为什么有了有向概率图模型，还需要无向概率图模型\n",
    "\n",
    "    原因是有些问题使用有向图表示会更加自然，比如上面提到的学生网络，有向图可以轻松描述变量之间的因果关系——学生的智力水平会影响 SAT 分数，但 SAT 分数不会影响智力水平（尽管它也许能反映学生的智力水平）。\n",
    "    \n",
    "    而对于其它一些问题，比如图像，你可能需要将每个像素都表示成一个节点。我们知道相邻的像素互有影响，但像素之间并不存在因果关系；它们之间的相互作用是对称的。所以我们在这样的案例中使用无向图模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、条件随机场\n",
    "\n",
    "### 1、条件随机场介绍\n",
    "\n",
    "    条件随机场( Conditional Random Field, CRF)是一种给定输入随机变量 x，求解条件概率 p(y|x) 的概率无向图模型。用于序列标注时，特例化为线性链( linear chain )条件随机场。此时，输人输出随机变量为等长的两个序列。\n",
    "    \n",
    "<img src=\"./imgs/crf.png\" alt=\"条件随机场\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "每个 $X_{t}$ 上方有 3 个灰色节点，代表 $X_{t}$ 的 3 个特征，当然还可以是任意数量的特征，体现了特征的丰富性，黑色方块是因子节点，可以理解为一个特征函数 。其中仅仅利用了 $X_{t}$ 和 $X_{t}$ 的特征称作状态特征，利用了 $Y_{t-1}$ 的特征则称作转移特征，与感知机的特征函数相同。\n",
    "\n",
    "    线性链条件随机场的定义如下:\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "&emsp;&emsp;&emsp;K代表特征的总个数， \n",
    "\n",
    "&emsp;&emsp;&emsp;$F_{k}$ 代表第k个特征的特征函数，\n",
    "\n",
    "&emsp;&emsp;&emsp;$W_{k}$代表第k个特征函数求出的特征值所对应的权重。\n",
    "\n",
    "    其中，Z(X)为归一化函数:\n",
    "    \n",
    "$$Z(X) = \\sum_{y}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "    注意：y就是所有可能的序列，计算开销很大。\n",
    "\n",
    "    上式定义在所有可能的标注序列上。如果将所有特征函数与权重分别写作向量形式，则线性链条件随机场的定义可简化为:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Y|X) &= \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\\\\n",
    "&= \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{W.\\phi(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\\\\n",
    "&= \\frac{1}{Z(X)}exp\\left\\{\\sum_{t=1}^{T}W.\\phi(Y_{t-1}, Y_{t}, X_{t})\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    对比结构化感知机的打分函数:\n",
    "\n",
    "$$score(X,Y) = \\sum_{t=1}^{T}W.\\phi(Y_{t-1}, Y_{t}, X_{t})$$\n",
    "\n",
    "    可以发现结构化感知机打分函数与条件随机场的指数部分完全相同，由于给定实例 X，Z(X) 就是一个常数 c，所以有:\n",
    "    \n",
    "$$P(Y|X) = \\frac{1}{c}exp\\left\\{score(X,Y)\\right\\}$$\n",
    "\n",
    "    于是，条件随机场就和结构化感知机有以下联系:\n",
    "\n",
    "> 条件随机场和结构化感知机的特征函数完全一致。\\\n",
    "结构化感知机预测打分越高，条件随机场给予该预测的概率也越大。\\\n",
    "这种相似性使得我们能够复用结构化感知机的预测算法，也就是维特比算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、条件随机场的训练\n",
    "\n",
    "指定训练集$D=\\lbrace x^{(i)},y^{(i)}\\rbrace _{i=1}^{N}$，N是总样本数（如果是句子，就是所有待训练的句子数）。其中每个$X^{(i)}={X_1^{(i)},X_2^{(i)},...X_T^{(i)}}$为输入序列，$Y^{(i)}={Y_1^{(i)},Y_2^{(i)},...Y_T^{(i)}}$为相应的标签序列。\n",
    "\n",
    "\n",
    "    根据极大似然估计，我们想要最大化给定模型参数W时，训练集D的似然概率：\n",
    "$$P(D|W)=\\prod_{i=1}^{N}P(Y^{(i)}|X^{(i)})$$\n",
    "\n",
    "    等价于极大化对数似然函数：\n",
    "$$L(W)=\\sum_{i=1}^{N}lnP(Y^{(i)}|X^{(i)})$$\n",
    "\n",
    "将$P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$代入上式得到：\n",
    "\n",
    "$$L(W)=\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\sum_{i=1}^{N}lnZ(X)$$\n",
    "\n",
    "$$Z(X) = \\sum_{y}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "其中N是样本集总数量，现在我们要用随机梯度下降法（SGD）来极大似然函数（准确地说应该是利用梯度上升法，来求最大值），因此每次只用处理一个训练序列，于是有：\n",
    "$$L(W)=\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_{k}F_{k}(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-lnZ(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    那么对数似然函数的偏导为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W_{k}}&=\\sum_{t=1}^{T}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\frac{1}{Z(X)}\\frac{\\partial Z(X)}{\\partial W_{k}}  \\\\\n",
    "&=\\sum_{t=1}^{T}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\frac{1}{Z(X)}\\sum_{y}\\bigg(exp\\left\\{\\sum_{t=1}^{T}\\sum_{k=1}^{K}W_kF_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}\\sum_{t=1}^{T}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\bigg)  \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为:\n",
    "\n",
    "$$P(Y|X)=\\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X_{t})\\right\\}$$\n",
    "\n",
    "所以偏导式子化简为：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{k}}=\\sum_{t=1}^{T}F_k(y_{t-1}^{(i)},y_{t}^{(i)},x_{t}^{(i)})-\\sum_{y}\\bigg(P(Y|X)\\sum_{t=1}^{T}F_k(y_{t-1},y_{t},x_{t}^{(i)})\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、条件随机场和结构化感知器对比\n",
    "\n",
    "    结构化感知机和条件随机场的相同点:\n",
    "\n",
    ">特征函数相同\\\n",
    "权重向量相同\\\n",
    "打分函数相同\\\n",
    "预测算法相同\\\n",
    "同属结构化学习\n",
    "\n",
    "    不同点:\n",
    "    \n",
    ">1.感知机更新参数时，只使用一个训练实例，没有考虑整个数据集，难免顾此失彼；而条件随机场对数似然函数及其梯度则使用了整个数据集。\n",
    "对数似然函数:\n",
    "\n",
    "\n",
    "> 2.条件随机场更新参数更加合理，条件随机场更新参数如下:\n",
    "$$W \\leftarrow W+\\phi(X^{i},Y^{i})-E_{w}\\left[\\phi(X^{i},Y)\\right]$$\n",
    "> 对比感知机的更新参数表达式:\n",
    "$$W \\leftarrow W+\\phi(X^{i},Y^{i})-\\phi(X^{i},\\hat{Y})$$\n",
    "> 两者的差距一目了然，感知机奖励正确答案对应的特征函数 ϕ，但仅惩罚错误最厉害的那个 y，而条件随机场同时惩罚所有答案 y，分摊惩罚总量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.zhuanzhi.ai/knowledge/c7509f236728275fe6abe6b910244ff9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
