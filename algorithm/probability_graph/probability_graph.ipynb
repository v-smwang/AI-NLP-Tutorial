{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图模型 Graphical Model，亦称概率图模型 Probabilistic Graphical Model（PGM）或结构化概率模型 structured probabilistic model，是一种用图表示随机变量之间条件依赖关系的概率模型。它们通常用于概率论、统计学，尤其是贝叶斯统计学和机器学习。\n",
    "\n",
    "常用的概率图模型大致分为两类：贝叶斯网络（Bayesian network, BN）【有向概率图模型】和马尔可夫网络（Markovnetwork, MN）【无向概率图模型】或者称为马尔可夫随机场。\n",
    "\n",
    "<img src=\"imgs/classify.png\" width=\"600\"/>\n",
    "\n",
    "# 一、概率的基本性质\n",
    "\n",
    "假设现在有一组高维的随机变量$X={x_1,x_2,x_3,...x_n}$，它有两个非常基础的概率，边缘概$P(x_i)$和条件概率$P(x_j|x_i)$。\n",
    "\n",
    "依据这两个基础概率可以得到两个基本运算法则，加和乘\n",
    "\n",
    "加法：$P(x_1)=\\sum_{x_2}P(x_1,x_2)$\n",
    "\n",
    "乘法：$P(x_1,x_2)=P(x_1)P(x_2|x_1)=P(x_2)P(x_1|x_2)$\n",
    "\n",
    "根据这两个运算可以推出，链式法则和贝叶斯法则\n",
    "\n",
    "链式法则：\n",
    "\n",
    "$$P(x_1,x_2)=P(x_1)P(x_2|x_1)$$\n",
    "\n",
    "$$P(x_1,x_2,x_3)==P(x_1,x_2)P(x_3|x_1,x_2)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$P(x_1,x_2,...,x_n)=\\prod_{i=1}^n P(x_n|x_1,x_2,...,x_{n-1})$$\n",
    "\n",
    "贝叶斯法则：\n",
    "\n",
    "$$P(x_2|x_1)=\\frac{P(x_1,x_2)}{P(x_1)}=\\frac{P(x_1)P(x_2|x_1)}{\\sum_{x_2}P(x_1,x_2)}=\\frac{P(x_1)P(x_2|x_1)}{\\sum_{x_2}P(x_1)P(x_2|x_1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、条件独立性\n",
    "\n",
    "我们知道独立性，两个事件之间没有相互关系，不会相互影响。例如两个事件相互独立那么P(a,b)=P(a)P(b)。\n",
    "\n",
    "但是条件独立性啥意思，其实就是这两个事件关于某个条件事件独立，即P(a,b|c)=P(a|c)P(b|c)，$a \\perp b|c$。\n",
    "\n",
    "首先，我们想想高维随机变量所遇到的困境，也就是维度高，计算复杂度高。大家想想，当维度\n",
    "较高时，这个$P(x_1,x_2,...,x_n)=\\prod_{i=1}^n P(x_n|x_1,x_2,...,x_{n-1})$肯定会算炸去。所以，我们需要简化运\n",
    "算，之后我们来说说我们简化运算的思路。\n",
    "\n",
    "- 1. 假设每个维度之间都是相互独立的，那么我们有$P(x_1,x_2,...,x_n)=\\prod_{i=1}^n P(x_n)$。比如，朴素贝叶斯就是这样的设计思路，也就是$P(x|y) =\\prod^N_{i=1} p(x_i|y)$。但是，我们觉得这个假设太强了，实际情况中的依赖比这个要复杂很多。所以我们像放弱一点，增加之间的依赖关系，于是我们有提出了马尔科夫性质 (Markov Propert)。\n",
    "\n",
    "\n",
    "- 2. 假设每个维度之间是符合马尔科夫性质 (Markov Propert) 的。所谓马尔可夫性质就是，对于一个序列$\\{x_1, x_2, · · · , x_N\\}$，第i项仅仅只和第i-1项之间存在依赖关系。用符号的方法我们可以表示为：$$X_j \\perp X_{i+1}|x_i, j < i$$在 HMM 里面就是这样的齐次马尔可夫假设，但是还是太强了，我们还是要想办法削弱。自然界中经常会出现，序列之间不同的位置上存在依赖关系，因此我们提出了条件独立性。\n",
    "\n",
    "\n",
    "- 3. 条件独立性：条件独立性假设是概率图的核心概念。它可以大大的简化联合概率分布。而用图我们可以大大的可视化表达条件独立性。我们可以描述为：$$X_A \\perp X_B|X_C$$而$X_A, X_B, X_C$是变量的集合，彼此之间互不相交。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  三、概率图算法分类\n",
    "\n",
    "\n",
    "## 1、表示（Representation）\n",
    "\n",
    "知识表示的方法，可以分为有向图，Bayesian Network；和无向图，Markov Network，这两种图\n",
    "通常用来处理变量离散的情况。对于连续性的变量,我们通常采用高斯图,同时可以衍生出, Gaussian\n",
    "Bayesian Network和Guassian Markov Network。\n",
    "\n",
    "## 2、推断（Inference）\n",
    "\n",
    "推断可以分为精准推断和近似推断。所谓推断就是给定已知求概率分布。近似推断中可以分为确\n",
    "定性推断(变分推断)和随机推断(MCMC)，MCMC是基于蒙特卡罗采样的。\n",
    "\n",
    "## 3、学习（Learning）\n",
    "\n",
    "学习可以分为参数学习和结构学习。在参数学习中，参数可以分为变量数据和非隐数据，我们可\n",
    "以采用有向图或者无向图来解决。而隐变量的求解我们需要使用到EM算法，这个EM算法在后面的\n",
    "章节会详细推导。而结构学习则是，需要我们知道使用那种图结构更好，比如神经网络中的节点个数，\n",
    "层数等等，也就是现在非常热的Automate Machine Learning。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、概率图的三种基本结构\n",
    "\n",
    "对于一个概率图，我们可以使用拓扑排序来直接获得，条件独立性的关系。如果存在一个关系由一个节点$x_i$指向另一个节点$x_j$。我们可以记为$p(x_j|x_i)$。 我们现在需要定义一些规则来便于说明，对于一个概率图如下所示\n",
    "\n",
    "<img src=\"imgs/base_probability_graph.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个圆圈就是一个节点，连接节点的称为边，这里是有向图，所以用箭头连接。对于一个箭头来说，箭头所指的叫做head，箭头另一端称为tail，tail节点其实就是因，也就是条件，head节点就是果。\n",
    "\n",
    "若两个节点相互独立，则称为阻塞；如果两个节点不相互独立，而是有关系，则称为连通。\n",
    "\n",
    "## 1、 tail to tail结构\n",
    "\n",
    "Tail to Tail的模型结构图，如下图所示，由于b节点在a节点和C节点的Tail部分，所以被我\n",
    "们称为Tail to Tail结构。\n",
    "\n",
    "<img src=\"imgs/tail_to_tail.png\" width=\"200\"/>\n",
    "\n",
    "我们看图可以得到：\n",
    "\n",
    "$$P(a,b,c)=P(b)P(a|b)P(c|b)$$\n",
    "\n",
    "使用链式法则可以得到：\n",
    "\n",
    "$$P(a,b,c)=P(b)P(a|b)P(c|a,b)$$\n",
    "\n",
    "$$P(a,c|b)=\\frac{P(a,b,c)}{P(b)}=P(a|b)P(c|a,b)$$\n",
    "\n",
    "通过上面的式子结合可以得到：\n",
    "\n",
    "$$P(c|b)=P(c|a,b)$$\n",
    "\n",
    "$$P(c|b)P(a|b)=P(c|a,b)P(a|b)=P(a,c|b)$$\n",
    "\n",
    "- 这就说明在给定b的情况下，c和a是相互独立的，c正交于a:\n",
    "\n",
    "$$c\\perp a | b$$\n",
    "\n",
    "    我们直接从概率图上就能看出c和a是相互独立的，因为它俩之间没有连线，所以概率图能够很直接的反映出变量之间的关系。\n",
    "\n",
    "- 若不给定b，则$P(a,c)=\\sum_b P(a,b,c)=\\sum_b P(a|b)P(c|b)P(b)$，所以a和c此时不是相互独立的。只有满足P(a,c)=P(a)P(c)，才能说明a与c相互独立。\n",
    "\n",
    "**举例：**\n",
    "\n",
    "还真不大好举例说明在不给定b的情况下a和c不相互独立。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、head to tail结构\n",
    "\n",
    "<img src=\"imgs/head_to_tail.png\" width=\"200\"/>\n",
    "\n",
    "其实，和 Head to Head 结构的分析基本是上一模一样的。\n",
    "\n",
    "我们看图可以得到：\n",
    "\n",
    "$$P(a,b,c)=P(a)P(b|a)P(c|b)$$\n",
    "\n",
    "依据链式法则我们有：\n",
    "\n",
    "$$P(a,b,c)=P(a)P(b|a)P(c|a,b)$$\n",
    "\n",
    "所以\n",
    "\n",
    "$$P(c|b)=P(c|a,b)$$\n",
    "\n",
    "$$P(c|b)P(a|b)=P(c|a,b)P(a|b)=P(a,c|b)$$\n",
    "\n",
    "- 给定b的情况下，我们可以得到a⊥c|b。也就是给定b的条件下，a和c之间是条件独立的。也就是b被观测的条件下，路径被阻塞。\n",
    "\n",
    "\n",
    "- 不给定b的情况下，$P(a,c)=\\sum_b P(a,b,c)=\\sum_b P(a)P(b|a)P(c|b)=P(a)\\sum_b P(b|a)P(c|b)=P(a)P(c|a)$，说明这种情况下a和c不是相互独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3、head to head结构\n",
    "\n",
    "<img src=\"imgs/head_to_head.png\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看图得到：\n",
    "\n",
    "$$P(a,b,c)=P(a)P(b)P(c|a,b)$$\n",
    "\n",
    "依据链式法则有：\n",
    "\n",
    "$$P(a,b,c)=P(a)P(b|a)P(c|a,b)$$\n",
    "\n",
    "所以\n",
    "\n",
    "$$P(b)=P(b|a)$$\n",
    "\n",
    "$$P(a)P(b)=P(b|a)P(a)=P(a,b)$$\n",
    "\n",
    "- 在不给定c的情况下，a、b是相互独立的。\n",
    "\n",
    "- 在给定c的情况下，$P(a,b|c)=P(a,b,c)/P(c)=P(a)P(b)P(c|a,b)/P(c)，此时a、b是不相互独立的$\n",
    "\n",
    "\n",
    "**举例：**\n",
    "\n",
    "a是智商，b是考题，c是成绩。在不给定c时也就是不知道成绩时，a和b是相互独立的。如果知道了成绩，则a和b就不独立，例如智商很高，成绩很低，那么考题肯定很难。\n",
    "\n",
    "**因子分解法：**\n",
    "\n",
    "概率图模型中，图是用来表达的，将概率嵌入到了图中之后，使得表达变得非常的清晰明了。在\n",
    "我们的联合概率计算中，出现了一些问题：\n",
    "\n",
    "$$P\\left(x_{1}, x_{2}, \\cdots, x_{N}\\right)=P\\left(x_{i}\\right) \\prod_{i=1}^{N} P\\left(x_{i} \\mid x_{1: i-1}\\right)$$\n",
    "\n",
    "这样的计算维度太高了，所以我们引入了条件独立性，表达为$X_A⊥X_B|X_C$。那么采用因子分解的方法\n",
    "我们可以将联合概率的计算进行分解为：\n",
    "\n",
    "$$P\\left(x_{1}, x_{2}, \\cdots, x_{N}\\right)=\\prod_{i=1}^{N} P\\left(x_{i} \\mid x_{pa\\{i\\}}\\right)$$\n",
    "\n",
    "其中，pa{i} 表示为 $x_i$ 的父亲节点。而概率图可以有效的表达条件独立性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、有向分离（D-Separation）\n",
    "\n",
    "先说它是干啥的，它是用来快速判断两个变量是否是条件独立的，如果满足有向分离的条件那么这两个变量就是条件独立的。有向分离又称为全局马尔科夫性质 (Global Markov Property)。\n",
    "\n",
    "如果 A，B，C 是三个集合（可以是单独的节点或者是节点的集合），为了判断 A 和 B 是否是 C 条件独立的， 我们考虑 E 中所有 A 和 B 之间的 无向路径 。对于其中的一条路径，如果她满足以下两个条件中的任意一条，则称这条路径是 阻塞（block） 的：\n",
    "\n",
    "有向分离的两个条件：\n",
    "\n",
    "a. 路径中存在某个节点X是head-to-tail或者tail-to-tail节点（Example one/two），并且X是包含在C中的；\n",
    "\n",
    "b. 路径中存在某个节点X是head-to-head节点（Example Three），并且X或X的子节点是不包含在C中的；\n",
    "\n",
    "|<div style=\"width: 100pt\">网络类型</div>|<div style=\"width: 100pt\">$X \\in C$</div>|<div style=\"width: 100pt\">$X \\notin C$</div>|\n",
    "|:-|:-|:-|\n",
    "|tail-to-tail|阻塞|不阻塞|\n",
    "|head-to-tail|阻塞|不阻塞|\n",
    "|head-to-head|不阻塞|阻塞（子节点也要$\\notin C$）|\n",
    "\n",
    "如果 A，B 间所有的路径都是阻塞的，那么 A，B 就是关于 C 条件独立的；否则， A，B 不是关于 C 条件独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**举例**\n",
    "\n",
    "<img src=\"imgs/d_separation.png\" width=\"260\"/>\n",
    "\n",
    "判断图中a与b是否在b条件下独立？a与b是否在f条件下独立？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判断 a 和 b 是否是 c下条件独立的：\n",
    "\n",
    "1. 此时 A={a}, B={b}, C={c}；\n",
    "\n",
    "2. a 到 b 只有一条无向路径 a-e-f-b ；\n",
    "\n",
    "3. 考虑路径上的点 e 和 f ：其中 e 是 head-to-head 类型的，且 e 的子节点就是 c ，而 c∈C，根据 (b)，e 不阻塞该路径；\n",
    "\n",
    "4. 节点 f 是 tail-to-tail 类型节点，且 f∉C ，根据 (a)，所以也有 a，b不是 c 条件下独立；\n",
    "\n",
    "5. 所以 a 和 b 在 c 下不是条件独立的。\n",
    "\n",
    "判断 a 和 b 是否是 f 下条件独立的：\n",
    "\n",
    "1. 此时 A={a}, B={b}, C={f}；\n",
    "\n",
    "2. a 到 b 只有一条无向路径 a-e-f-b ；\n",
    "\n",
    "3. 考虑路径上的点 e 和 f ：其中e 是 head-to-head 类型的，且 e 和 e 的子节点 c 都不属于 C，根据 (b)，e 阻塞该路径；\n",
    "\n",
    "4. 节点 f 是 tail-to-tail 类型节点，且 f∈C ，根据 (a)，所以 f 节点阻塞了该路径。\n",
    "\n",
    "5. 所以 a 和 b 在 f 下是条件独立的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、有向概率图模型-贝叶斯网络\n",
    "\n",
    "有向概率图模型又称为贝叶斯网络\n",
    "\n",
    "贝叶斯网络（Bayesian network），又称信念网络（Belief Network）,或有向无环图模型（directed acyclic graphical model），是一种概率图模型。\n",
    "\n",
    "> **条件概率公式：**\n",
    ">\n",
    "> $$P(A|B) = \\frac{P(A,B)}{P(B)}$$\n",
    ">\n",
    "> **贝叶斯公式：**\n",
    ">\n",
    "> $$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$ 说明： P(A|B)是后验概率，P(B|A)是先验概率。\n",
    ">\n",
    "> 贝叶斯法则也可以是关于随机事件A和B的条件概率和边缘概率的：\n",
    ">\n",
    ">$$P(A=a|B)=\\frac{P(A=a)P(B|A=a)}{\\sum_1^jP(B|A_j)P(A_j)}$$\n",
    ">\n",
    "> **朴素贝叶斯公式：**\n",
    ">\n",
    ">当我们的数据B是多维时，P(A=a)就是a出现的频次除以总频次；而P(B|A=a)是非常难求的，所以朴素贝叶斯假设了B这个多维数据是相互独立的，即$B_1、B_2、...B_i$它们之间是相互独立的，那么$P(B|A)= P(B_1|A)P(B_2|A)...P(B_i|A)$，这样就大大简化了计算。\n",
    ">\n",
    "> **朴素贝叶斯模型:**\n",
    ">\n",
    "> 它是从训练样本中求待预测样本的每个类别概率，最大概率的类别作为该待预测样本的类别。\n",
    ">\n",
    "> $$类别a=\\underbrace{argmax}_{a} \\frac{P(A=a)P(B^{test}|A=a)}{P(B^{test})}=\\underbrace{argmax}_{a} P(A=a)P(B^{test}|A=a)=\\underbrace{argmax}_{a} P(A=a)\\prod_1^iP(B^{test}_i|A=a)$$ 因为$P(B^{test})$固定不变，所以只求分子的最大值就行了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**朴素贝叶斯网络的图表示**\n",
    "\n",
    "我们从单一，混合，时间和连续，四个角度来看看 Bayesian Network，这个四个方法是一步一步越来\n",
    "越难的。\n",
    "\n",
    "**单一**\n",
    "\n",
    "概率图中，每个节点的概率可以表示为: P(当前节点|它的父节点) ，如图中所示（y是类别，x是多维特征）：\n",
    "\n",
    "<img src=\"imgs/bayes_graph.png\" width=200/>\n",
    "\n",
    "很显然是一个 Tail to Tail 的模型，我们很简单可以得出 $x_1⊥x_2⊥ · · · ⊥x_N|y_j$。\n",
    "\n",
    "**混合**\n",
    "\n",
    "最常见的就是 Gaussian Mixture Model (GMM)，这是一种聚类模型，将每个类别拟合一个分布，\n",
    "计算数据点和分布之间的关系来确定，数据点所属的类别。我们假设Z是一个隐变量，并且Z是离\n",
    "散的变量，那么\n",
    "\n",
    "$$x|z∼N(µ, Σ)$$\n",
    "\n",
    "x|z服从正态分布N(µ, Σ)。\n",
    "\n",
    "我们用模型可以表示为：\n",
    "\n",
    "<img src=\"imgs/GMM.png\" width=200/>\n",
    "\n",
    "**时间**\n",
    "\n",
    "时间上我们大致可以分成两种。第一种是 Markov chain，这是随机过程中的一种；第二种事 Gaus\u0002sian Processing，实际上就是无限维的高斯分布。\n",
    "实际上时间和混合可以一起看，我们称之为动态系统模型。并且，我们就可以衍生出三种常见的模\n",
    "型，这里讲的比较的模糊，在后面的章节我们会进行详细的分析。第一种是隐马尔可夫模型 (HMM)，这\n",
    "是一种离散的模型；第二种是线性动态系统 (LDS)，这是一种线性的连续的模型，包括典型的 Kalman\n",
    "Filter。第三种是 Particle Filter，一种非高斯的，非线性的模型。\n",
    "\n",
    "**连续**\n",
    "\n",
    "连续就是 Guassian Bayesian Network，前面有提到过。\n",
    "大家可能听到这么多的名词会一脸懵逼呀，懵逼是很正常的，因为这些名词只是给了个印象，后\n",
    "面我们会进行详细的分析。实际上一个整体的趋势就是从单一到混合，从有限到无限。也就是从空间\n",
    "和时间两个角度来进行分析，都是从离散到连续的过程。至于具体为什么这么分，还得具体学习了算\n",
    "法我们才能够很好的理解，本小节只是起了一个高屋建瓴的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、无向概率图模型-马尔科夫网络（Markov Network）\n",
    "\n",
    "无向概率图模型也称为马尔科夫网络。\n",
    "\n",
    "我们分析了有向图 Bayesian Network，得到了因子分解法，$P\\left(x\\right)=\\prod_{i=1}^{N} P\\left(x_{i} \\mid x_{pa\\{i\\}}\\right)$。\n",
    "\n",
    "虽然，有向图中可以方便直观的表达条件独立性，但是它也有它的局限性。也就是我们提到的对于 Head to Head 的结构来说，当中间节点被观察到的时候，反而是两端的节点是相关的。这违反了条件独立性的特点，也就是当某些变量被观察到时，其他变量之间是独立的特点，这种情况有点反常，并不太\n",
    "好办。\n",
    "\n",
    "但是，在无向图中就完全不会出现这样的情况，因为本来就没有方向，而且在无向图中也有类似\n",
    "的 D-Separation 性质。\n",
    "\n",
    "\n",
    "## 1、无向图条件独立性\n",
    "\n",
    "**全局马尔可夫性（Global Markov）**\n",
    "\n",
    "$$X_A \\perp X_C|X_B$$\n",
    "\n",
    "如下图所示，无向图的条件独立性很简单，如果在给定集合$X_B$情况下，集合$X_A$和$X_C$相互独立，则必须满足如下条件：节点a属于集合$X_A$，节点c属于集合$X_C$，若存在节点b和节点a和c之间均连接，则节点b必须存在于集合$X_B$中。\n",
    "\n",
    "<img src=\"imgs/global_markov.png\" width=\"200\"/>\n",
    "\n",
    "**局部马尔可夫性（Local Markov）**\n",
    "\n",
    "$$a \\perp {e,f}|{b,c,d} $$\n",
    "\n",
    "如下图所示，局部马尔可夫性是指节点a在给定邻居节点b,c,d情况下和非邻居节点是相互独立的。\n",
    "\n",
    "<img src=\"imgs/local_markov.png\" width=\"200\"/>\n",
    "\n",
    "**成对马尔可夫性（Pair Markov）**\n",
    "\n",
    "成对马尔可夫性质可以被我们描述为：\n",
    "\n",
    "$$x_i\\perp x_j |x_{-i-j} (i \\neq j)$$\n",
    "\n",
    "其中，$x_{-i-j}$为从全集中去掉$x_i$和$x_j$而留下了的集合。\n",
    "\n",
    "那么条件独立性就可以体现在，Global，Local 和 Pair 中。其中 Global⇔Local⇔Pair。也就是这三种条件独立的方法得到的结果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、因子分解法\n",
    "\n",
    "类比有向概率图的因子分解，为了描述无向图的条件独立性，我们需要依赖于团这个概念来描述条件独立性。\n",
    "\n",
    "相反，无向图模型则不探究每个事件的因果关系，也就是说不涉及条件概率分解。无向图模型的边没有方向，仅仅代表两个事件有关联。\n",
    "    \n",
    "<img src=\"../crf/imgs/PGM_without_direction.png\" alt=\"无向概率图模型\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "无向概率图模型的几个概念\n",
    "    \n",
    "<img src=\"../crf/imgs/PGM_without_direction2.png\" alt=\"无向概率图模型例子\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.1、团（clique）\n",
    "\n",
    "<img src=\"../crf/imgs/tuan.png\" alt=\"团\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "#### 4.2、极大团（maximum clique）\n",
    "\n",
    "<img src=\"../crf/imgs/maximum_tuan.png\" alt=\"极大团\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "#### 4.3、最大团 （maximul clique）\n",
    "\n",
    "<img src=\"../crf/imgs/max_tuan.png\" alt=\"最大团\" width=\"400\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    ">团：必须两两相连（任意两点都相连）\\\n",
    "极大团：无法再加入一个节点使其成为团\\\n",
    "最大团：极大团里节点最多的团\n",
    "\n",
    "因子分解：将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数乘积形式的操作。\n",
    "\n",
    "因子分解公式：\n",
    "\n",
    "$$P(X)=\\frac{1}{Z}\\prod_{i=1}^k \\phi(x_{c_i})$$\n",
    "\n",
    "其中$X={x_1,x_2,...x_n}$,$x{c_i}$是团$c_i对应图中的节点集合，ϕ函数是势函数，Z是归一化因子（配分函数，为了使概率在[0,1]间）：\n",
    "\n",
    "$$Z=\\sum_X\\prod_{i=1}^k \\phi(x_{c_i})=\\sum_{x_1,x_2,...x_n}\\prod_{i=1}^k \\phi(x_{c_i})$$\n",
    "\n",
    "<font color=\"red\">在这里当x_1,x_2,...x_n特征变量取不同的值时，对应的每个最大团的势函数值是不一样的，对应的势函数值越大，则概率越大。</font>\n",
    "\n",
    "**势函数：**\n",
    "\n",
    "势函数可以是任意的函数，但是需要满足条件\n",
    "\n",
    "1. 非负\n",
    "2. 同一个无向图中不同团上的势函数可以具有不同的形式\n",
    "3. 当团上的变量取得不同值时（团上变量发生变化），对应势函数输出不同的值，输出的值越大，归一化后的概率值就越大。\n",
    "\n",
    "所以需要根据具体实际场景中变量之间的关系去定义势函数。\n",
    "\n",
    "在同一个无向图中，不同团上的势函数可以有不同的形式，即不同团可以拥有不同的势函数 。此外，在很多场景中， 无向图的联合概率表达中不经包含团上的势函数，也可以加入单个结点的势函数。\n",
    "\n",
    "$$P(\\mathbf{x})=\\frac{1}{Z} \\prod_{i \\in \\mathcal{V}} \\psi_{i}\\left(x_{i}\\right) \\prod_{C \\in e} \\psi_{x_{C}}\\left(x_{C}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、推断 (Inference)\n",
    "\n",
    "对于有一定机器学习基础的同学来说，一定是听说过，这也是贝叶斯方\n",
    "法中一个非常重要的理论性研究。那么什么是推断呢？推断说白了，就是求概率。比如，对于一个联\n",
    "合概率密度函数$p(x)=p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)$。我们需要求的有哪些呢？\n",
    "1. 边缘概率：\n",
    "\n",
    "$$p\\left(x_{i}\\right)=\\sum_{x_{1}} \\cdots \\sum_{x_{i-1}} \\cdots \\sum_{x_{i+1}} \\cdots \\sum_{x_{p}} p(x)_{\\text {。 }}$$\n",
    "\n",
    "2. 条件概率：\n",
    "\n",
    "$$p\\left(x_{A} \\mid x_{B}\\right), 令 x=x_{A} \\cup x_{B}$$\n",
    "\n",
    "3. MAP Inference：\n",
    "\n",
    "$$\\hat{z}=\\operatorname{argmax}_{z} p(z \\mid x) \\propto \\operatorname{argmax} p(x, z)$$\n",
    "\n",
    "因为，$p(z \\mid x)=\\frac{p(x, z)}{p(x)} \\propto p(x, z)$，因为我们的目标是求一个最优的参数 z，并不需要知道具体的数值是多少，只要知道谁大谁小就行，所以p(x)可以直接不看了。\n",
    "现在我们知道了，Inference在求什么？下一步，我们要总结 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、 Inference 求解方法\n",
    "\n",
    "### 1.1 精准推断 (Deterministic Inference)\n",
    "\n",
    "Variable Elimination (VE)，变量消除法；Belief Propagation (BP) 信念传播，这个可不是我们之\n",
    "前学习的反向传播算法，这里需要注意。同时这个算法衍生出的 Sum Product Algorithm，这就是推\n",
    "断的核心，这是一种树结构的；而 Junction Tree Algorithm，这是一种普通图结构。\n",
    "\n",
    "**Variable Elimination (VE)，变量消除法**\n",
    "\n",
    "为了减少边缘概率计算量，而采用的一种计算方法。\n",
    "<img src=\"imgs/variable_elimination.png\" width=\"400\">\n",
    "\n",
    "变量消除法的思想很简单，就是对联合概率不断求和消除其中的变量，最后得到边缘分布。消除的变量的顺序需要根据实际的概率图依赖关系制定。\n",
    "\n",
    "TODO\n",
    "\n",
    "**Belief Propagation (BP) 信念传播**\n",
    "因为变量消除法有重复计算和最优计算次序不好确定的问题，所以我们用信念传播来解决重复计算的问题\n",
    "\n",
    "TODO\n",
    "\n",
    "### 1.2 近似推断 (Approximate Inference)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、道德图（Moral Graph）\n",
    "\n",
    "Moral Graph 存在的意义就是将有向图转化为无向图来研究。\n",
    "\n",
    "而我们将有向图转换成无向图之后，有什么好处吗？也就是在判断条件独立性的时候，有时图形\n",
    "非常复杂的时候。我们在有向图中很难看出来，而在无向图中却可以很简单的得到我们想要的结果。也\n",
    "就是 Sep(A, B|C) ⇐⇒ D - Sep(A, B|C)。\n",
    "\n",
    "TODO\n",
    "\n",
    "#  十、因子图（Factor Graph）\n",
    "\n",
    "我们介绍了道德图 (Moral Graph)，它的主要作用是将有向图转换为无向图。我们考虑的都是树结构，但是在 Head to Head 结构中，会引入环的结构。但是，在我们的 Belief Propagation(BP) 算法中，只能对树进行分解。所以，这里我们就引入了因子图。因子图主要发挥两个作用：\n",
    "\n",
    "1. 去环，也就是消除无向图中的环结构；\n",
    "2. 使算法变得更加的简洁，简化计算。\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十一、生成式模型和判别式模型\n",
    "\n",
    "**生成式模型**\n",
    "\n",
    "通过联合概率来求解条件概率，求$P(y|x)=\\frac{P(x,y)}{P(x)}$，就是生成式模型。在这里P(x,y)和P(x)的求解都是假设x特征之间相互独立，因为如果不假设独立就非常难求，所以$P(x,y)=P(y)P(x|y)=P(y)P(x_1|y)P(x_2|y)...P(x_n|y)$，而$P(x)=P(x_1)P(x_2)...P(x_n)$。\n",
    "\n",
    "这样生成模型就有两个缺陷：\n",
    "\n",
    "1. P(x) 很难准确估计，因为特征之间并非相互独立，而是存在错综复杂的依赖关系。\n",
    "\n",
    "2. P(x) 在分类中也没有直接作用【对于不同的y值，P(x)是不变的】。\n",
    "\n",
    "为了克服这个两个缺点，判别式出现。\n",
    "\n",
    "**判别式模型**\n",
    "\n",
    "直接求解条件概率P(y|x)。该条件概率不需要利用贝叶斯公式进行求解，直接可以通过定义函数进行求解\n",
    "\n",
    "$$P(y|x) = \\frac{f(x,y)}{\\sum_{x,y}f(x,y)}$$\n",
    "\n",
    "判别式可以通过贝叶斯公式转化为生成式；而生成式转化不成判别式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十二、隐马尔科夫模型\n",
    "\n",
    "概率图如下：\n",
    "\n",
    "<img src=\"imgs/hmm.png\" width=\"400\"/>\n",
    "\n",
    "**它的三个问题：**\n",
    "\n",
    "1. 评估观察序列概率。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，计算在模型$λ$下观测序列O出现的概率$P(O|λ)$。这个问题的求解需要用到前向后向算法。\n",
    "\n",
    "    解：\n",
    "    \n",
    "    $$P(O|λ)=\\sum_I P(O,I|λ)=\\sum_I P(I|λ)P(O|I,λ)$$\n",
    "\n",
    "2. 模型参数学习问题。即给定观测序列$O={o_1,o_2,...o_T}$，估计模型$λ=(A,B,Π)$的参数，使该模型下观测序列的条件概率$P(O|λ)$最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。\n",
    "\n",
    "    解：\n",
    "\n",
    "    如果已经标注了$I$，则直接训练就行。\n",
    "\n",
    "    如果没有标注$I$，那么需要用EM算法，进行无监督学习。\n",
    "\n",
    "\n",
    "3. 预测问题，也称为解码问题。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。【预测出的每一个隐状态都有N个值，每个值对应一个概率值；例如句子分词，那么就需要利用维特比算法找到整个句子的最大隐状态概率对应的隐状态路径。】\n",
    "\n",
    "    也就是求$\\underbrace{argmax}_I P(I|O,λ)$\n",
    "    \n",
    "    解：\n",
    "    \n",
    "    需要先求出每一个O对应的最大概率的隐状态，然后利用维特比算法求解出全局最大。\n",
    "\n",
    "    $$\\underbrace{argmax}_I P(i_t = q_i | O,\\lambda) = \\underbrace{argmax}_I \\frac{P(i_t = q_i ,O|\\lambda)}{P(O|\\lambda)}=\\underbrace{argmax}_I P(i_t = q_i ,O|\\lambda)$$\n",
    "\n",
    "\n",
    "**它为什么是生成式模型**\n",
    "\n",
    "从求解问题上我们可以看出它一直在使用联合概率$P(O,I)$，所以它是生成模型。建模的公式是：\n",
    "\n",
    "$$P(O,I)=\\prod_{t=1}^{T}P(O_t,I_t)=\\prod_{t=1}^{T}P(I_t|I_{t-1})P(O_t|I_t)$$\n",
    "\n",
    "\n",
    "**缺点**\n",
    "\n",
    "1. 齐次马尔科夫假设\n",
    "    \n",
    "    任意时刻的隐藏状态只依赖于它前一个隐藏状态\n",
    "    \n",
    "    $P(y_t)=P(y_t|y_{t-1})$\n",
    "    \n",
    "\n",
    "2. 观察独立假设\n",
    "    \n",
    "    <font color=\"red\">任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，和其他的观察状态无关【这里是D划分，在给定$y_t$时，$x_t$和$y_{t+1}$、$x_{t+1}$独立】</font>\n",
    "    \n",
    "    $P(x_t)=P(x_t|y_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">十三、最大熵模型、最大熵马尔科夫模型（MEMM）和 线性链条件随机场模型</font>\n",
    "\n",
    "##  1、最大熵模型\n",
    "\n",
    "最大熵模型就是通过让熵H(P)最大，从而求得w权重，从而求得P(y|x)关于权重w的表达式，从而求得P(y|x)的值，预测数据时取概率值最大的y作为类标。\n",
    "\n",
    "即定义最大熵模型为条件概率分布𝑃(𝑌|𝑋)上的条件熵：\n",
    "\n",
    "$$H(P) = -\\sum\\limits_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)$$\n",
    "\n",
    "让熵最大，求导得到：\n",
    "\n",
    "$$P_w(y|x) = \\frac{1}{Z_w(x)}exp(\\sum\\limits_{i=1}^{M}w_if_i(x,y))$$\n",
    "\n",
    "## 2、最大熵马尔科夫模型（MEMM）（Maximum Entropy Markov Model）\n",
    "\n",
    "MEMM是利用了最大熵模型求解出的P(y|x)表达式，来训练w，从而在预测数据时求得P(y|x)，取概率值最大的y作为类标。\n",
    "\n",
    "它是判别式模型，直接对P(y|x)进行建模求解。\n",
    "\n",
    "MEMM加入了马尔科夫性质\n",
    "\n",
    "$$P(Y_i|Y_{i-1},X_{1:n})$$\n",
    "\n",
    "$$P(Y_{1:n}|X_{1:n})=\\prod_1^n P(Y_i|Y_{i-1},X_{1:n})$$\n",
    "\n",
    "\n",
    "<font color=\"red\">MEMM解决了hmm的观察独立假设，MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。 </font>\n",
    "\n",
    "\n",
    "memm条件概率图\n",
    "\n",
    "<img src=\"imgs/memm.png\" width=\"600\"/>\n",
    "\n",
    "<font color=\"red\">这里之所以是$X_{1:n}$是因为特征函数会根据你的定义利用$X_{1:n}$里的任意值。例如利用$x_t$的前一项或者后一项</font>\n",
    "\n",
    "><font color=\"red\">缺点：\n",
    ">\n",
    ">1. 隐状态仍然是仅和前一项有关，仍然是齐次马尔科夫假设。</font>\n",
    "\n",
    "这个缺点会导致标注偏置，因为这里是局部进行的归一化，在一个隐状态能够转移到多个隐状态时，会出现概率平摊问题，导致本来概率很大的变得较小。\n",
    "\n",
    "<img src=\"imgs/label_bias.png\" width=\"600\" />\n",
    "\n",
    "实际上，在上图中，状态1偏向于转移到状态2，而状态2总倾向于停留在状态2。 【这里利用的是维特比算法，来求最短路径问题】\n",
    "\n",
    "而路径1-1-1-1的概率：$0.4*0.45*0.5=0.09$\n",
    "\n",
    "而路径1-2-2-2的概率：$0.6*0.3*0.3=0.054$\n",
    "\n",
    "这是因为s1的转移状态很少，由于每一步的状态转移概率都要归一化，所以s1的转移概率都会被放大，而s2由于转移状态多，因此每一步转移概率归一化的时候都被平均分摊了(比如s2到s2在那个状态最大但由于s2的转移状态多，一分摊才为0.3，而s1到s2在它那个状态下概率最小但是由于转移状态少，不用分摊太多为0.4。 \n",
    "\n",
    "\n",
    "## 3、线性链条件随机场模型\n",
    "\n",
    "CRF也和MEMM一样做了一阶马尔科夫假设，即当前状态只与上一状态有关，但是区别在于CRF的特征采用了全局特征，它把观测序列当做整体来看所以它的特征函数是全局的。\n",
    "\n",
    "\n",
    "线性链条件随机场模型（crf）也是利用了最大熵模型求解出的P(y|x)表达式，来训练w，从而在预测数据时求得P(y|x)，取概率值最大的y作为类标。\n",
    "\n",
    "<font color=\"red\">【其实CRF中X是给定的、确定的东西，Y才是条件随机场，它有T个最大团，每两个节点是一个最大团，最前面需要添加一个$Y_0$，所以对于因子分解P(Y)=P(Y|X)，而$P(Y)=\\frac{1}{Z(X)}\\prod_{a}\\Psi_{a}(X_{a}, Y_{a})$，对于crf公式的来历这两种都对，其实并不冲突，我感觉无向图的因子分解可能也是利用最大熵模型求出来的。\n",
    "    \n",
    "”最大熵准则这是一个原则性的指导思想，实践中在其他应用指导下才能生效，而平常我们用的极大似然估计方法，是最大熵准则求解步骤中的最后一步。“】</font>\n",
    "\n",
    "它是判别式模型，直接对P(y|x)进行建模求解。\n",
    "\n",
    "线性链条件随机场概率图\n",
    "\n",
    "<img src=\"imgs/crf1.png\" width=\"400\"/>\n",
    "\n",
    "CRF的建模公式如下：\n",
    "\n",
    "$$P(Y|X) = \\frac{1}{Z(X)}\\prod_{t=1}^{T}exp\\left\\{\\sum_{k=1}^{K}W_{k}F_{k}(Y_{t-1}, Y_{t}, X)\\right\\}$$\n",
    "\n",
    "<font color=\"red\">这里之所以是$X$是因为特征函数会根据你的定义利用$X$里的任意值。例如利用$x_t$的前一项或者后一项</font>\n",
    "\n",
    "> <font color=\"red\">条件随机场解决了标注偏置问题【齐次马尔科夫假设】，将Z(X)归一化因子提到了累乘的外面，变成了全局归一化，这样就不会产生局部归一化的概率平摊问题了。\\\n",
    ">    \\\n",
    ">    举例：全局归一化就是当我们计算一个字的label时，我们利用当前字的得分除以整个句子的所有得分来作为概率，计算量其实挺大的。\\\n",
    ">    \\\n",
    ">    局部归一化是就当前字进行归一化，比如句子”小红是中国人“，那么字”国“，它有4个类别[B,E,O,S]，利用”国“是B的势能值除以”国“分别是[B,E,O,S]的累加和就得到局部归一化的值。\\\n",
    ">    \\\n",
    ">    而如果是全局归一化，就是利用”国“是B的势能值除以”小红是中国人“里的每个字分别是[B,E,O,S]的势能值的累加值，一共有6*4个势能值相加。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memm打破了独立性假设，但是存在标注偏置问题（该问题是由于齐次马尔科夫假设导致的）\n",
    "crf解决了标注偏置问题\n",
    "\n",
    "每一个$y_{i-1} y_i x$就是一个逻辑回归模型\n",
    "\n",
    "在crf中X是给定的、确定的东西，Y才是条件随机场，所以对于因子分解P(Y)=P(Y|X)\n",
    "\n",
    "条件随机场解决了齐次马尔科夫假设了吗？？？？【感觉标注偏置和齐次马尔科夫假设不是一个问题】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、区别\n",
    "\n",
    "<img src=\"imgs/hmm_memm_crf.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
