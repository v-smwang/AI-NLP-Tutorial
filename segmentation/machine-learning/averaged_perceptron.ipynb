{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于平均感知器的分词器\n",
    "\n",
    "## 原理\n",
    "\n",
    "&emsp;&emsp;我们知道对于一句话进行分词，我们可以通过给每个字进行标注状态（B：分词词首；M：分词词中；E：分词词尾；S：单个词分词）来分词，然后去计算出每个字到底是什么状态，从而完成分词。\n",
    "\n",
    "&emsp;&emsp;那使用感知器，无非也就是用感知器来预测出每个字的4种状态的概率分别是多少，然后利用维特比算法找出整个句子最大概率的状态路径，从而完成分词。和hmm分词差不多，只是将hmm预测隐状态概率换成了使用感知器的多分类来预测隐状态概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class AveragedPerceptron(object):\n",
    "\n",
    "    '''An averaged perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            weights = self.weights[feat]\n",
    "            for label, weight in weights.items():\n",
    "                scores[label] += value * weight\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda label: (scores[label], label))\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "def train(nr_iter, examples):\n",
    "    '''Return an averaged perceptron model trained on ``examples`` for\n",
    "    ``nr_iter`` iterations.\n",
    "    '''\n",
    "    model = AveragedPerceptron()\n",
    "    for i in range(nr_iter):\n",
    "        random.shuffle(examples)\n",
    "        for features, class_ in examples:\n",
    "            scores = model.predict(features)\n",
    "            guess, score = max(scores.items(), key=lambda i: i[1])\n",
    "            if guess != class_:\n",
    "                model.update(class_, guess, features)\n",
    "    model.average_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "1、对每个句子中的每个字，取7种特征【unigrams+bigrams】\n",
    "\n",
    "> $C_{i-1}$ #前一个词 \\\n",
    "> $C_i$ #当前词 \\\n",
    "> $C_{i+1}$ #后一个词 \\\n",
    "> $C_{i-2}/C_{i-1}$ #前两个词与前一个词 \\\n",
    "> $C_{i-1}/C_i$ #前一个词与当前词 \\\n",
    "> $C_i/C_{i+1}$  #当前词与后一个词 \\\n",
    "> $C_{i+1}/C_{i+2}$  #后一个词与后两个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 1, 1, 2, 3], [3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3], [3, 2, 3, 3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_feature = {'BL=B':0, 'BL=M':1, 'BL=E':2, 'BL=S':3, 'BL=_BL_':4}  # 特征取值集合\n",
    "sentence_feature_list = []  # 记录每个句子中每个字的特征\n",
    "sentence_tag_list = []  # 每个字的类标\n",
    "START_CHAR = '\\1'\n",
    "END_CHAR = '\\2'\n",
    "\n",
    "count = 0\n",
    "\n",
    "def handle_feature(feature, char_feature, all_feature):\n",
    "    feature_id = all_feature[feature] if feature in all_feature else len(all_feature)\n",
    "    all_feature[feature] = feature_id\n",
    "    char_feature.append(feature_id)\n",
    "\n",
    "# weights = np.ones(())\n",
    "data_f = open('./data/RenMinData.txt', 'r', encoding='utf-8')\n",
    "for line in data_f.readlines():\n",
    "    count += 1\n",
    "    line = line.strip()\n",
    "    # 打标签\n",
    "    words = line.split(' ')\n",
    "    tag_list = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "        else:\n",
    "            tag_list.append(all_feature['BL=S'])\n",
    "            for w in word[1:len(word)-1]: # 中间字\n",
    "                tag_list.append(all_feature['BL=M'])\n",
    "            tag_list.append(all_feature['BL=E'])\n",
    "    sentence_tag_list.append(tag_list)\n",
    "    \n",
    "    # 获取特征    \n",
    "    sentence = line.replace(' ','')\n",
    "    sentence_feature = []\n",
    "    for i, char in enumerate(sentence):\n",
    "        char_feature = []\n",
    "        # 前2\n",
    "        pre2 = sentence[i-2] if i>=2 else START_CHAR\n",
    "        # 前1\n",
    "        pre1 = sentence[i-1] if i>=1 else START_CHAR\n",
    "        # 当前\n",
    "        cur = char\n",
    "        # 后1\n",
    "        next1 = sentence[i+1] if i < len(sentence)-1 else END_CHAR\n",
    "        # 后2\n",
    "        next2 = sentence[i+2] if i < len(sentence)-2 else END_CHAR\n",
    "        \n",
    "        # unigrams\n",
    "        one = pre1+'1'\n",
    "        handle_feature(one, char_feature, all_feature)\n",
    "        \n",
    "        two = cur+'2'\n",
    "        handle_feature(two, char_feature, all_feature)\n",
    "        \n",
    "        three = next1+'3'\n",
    "        handle_feature(three, char_feature, all_feature)\n",
    "        \n",
    "        # bigrams\n",
    "        four = pre2+'/'+pre1+'4'\n",
    "        handle_feature(four, char_feature, all_feature)\n",
    "        \n",
    "        five = pre1+'/'+cur+'5'\n",
    "        handle_feature(five, char_feature, all_feature)\n",
    "        \n",
    "        six = cur+'/'+next1+'6'\n",
    "        handle_feature(six, char_feature, all_feature)\n",
    "        \n",
    "        seven = next1+'/'+next2+'7'\n",
    "        handle_feature(seven, char_feature, all_feature)\n",
    "        \n",
    "        sentence_feature.append(char_feature)\n",
    "        #print('char_feature',char_feature)\n",
    "    sentence_feature_list.append(sentence_feature)     \n",
    "print(sentence_tag_list[:3])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[0,[1,2,3]],[1,[1,2,3]]])\n",
    "b = [0,1]\n",
    "print(sum([a[0][b] for ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [1,2]\n",
    "c = [3,3]\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c', 2)\n"
     ]
    }
   ],
   "source": [
    "print(max([('a',2),('b',4),('c',2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297959\n",
      "297959\n",
      "208571\n",
      "i-- 0\n",
      "i-- 10000\n",
      "i-- 20000\n",
      "i-- 30000\n",
      "i-- 40000\n",
      "i-- 50000\n",
      "i-- 60000\n",
      "i-- 70000\n",
      "i-- 80000\n",
      "i-- 90000\n",
      "i-- 100000\n",
      "i-- 110000\n",
      "i-- 120000\n",
      "i-- 130000\n",
      "i-- 140000\n",
      "i-- 150000\n",
      "i-- 160000\n",
      "i-- 170000\n",
      "i-- 180000\n",
      "i-- 190000\n",
      "i-- 200000\n",
      "训练集准确率： 0.7705673367821989\n",
      "验证集准确率： 0.7137199624110618\n",
      "i-- 0\n",
      "i-- 10000\n",
      "i-- 20000\n",
      "i-- 30000\n",
      "i-- 40000\n",
      "i-- 50000\n",
      "i-- 60000\n",
      "i-- 70000\n",
      "i-- 80000\n",
      "i-- 90000\n",
      "i-- 100000\n",
      "i-- 110000\n",
      "i-- 120000\n",
      "i-- 130000\n",
      "i-- 140000\n",
      "i-- 150000\n",
      "i-- 160000\n",
      "i-- 170000\n",
      "i-- 180000\n",
      "i-- 190000\n",
      "i-- 200000\n",
      "训练集准确率： 0.845855847649002\n",
      "验证集准确率： 0.7798585939947197\n",
      "i-- 0\n",
      "i-- 10000\n",
      "i-- 20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-84c1acca3ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0msentence_feature_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_feature_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0msentence_tag_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_tag_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_feature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_tag_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-84c1acca3ab3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sentence_feature_list, sentence_tag_list, maxIteration, ratio)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mpredict_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m#print('viterbi_decode耗时：', time.time()-start)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mactual_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-84c1acca3ab3>\u001b[0m in \u001b[0;36mviterbi_decode\u001b[0;34m(sentence_feature, W)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mpath_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# 每个tag的概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-84c1acca3ab3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mpath_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# 每个tag的概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_list = [0,1,2,3]  # 标签id\n",
    "hidden_node_num = len(tag_list)  # 隐状态个数（标签个数）\n",
    "# 参数\n",
    "global W\n",
    "# W = np.random.randn(hidden_node_num, len(all_feature))\n",
    "W = np.zeros((hidden_node_num, len(all_feature)))\n",
    "             \n",
    "def viterbi_decode(sentence_feature, W):\n",
    "    path_prob = []  # 路径和该路径的概率\n",
    "    pre_X = []\n",
    "    best_path = []  # 最好的路径，也就是最好的tag\n",
    "    for i, char_feature in enumerate(sentence_feature):\n",
    "        \n",
    "        if i == 0:\n",
    "            char_feature.append(4)  # 前一个字的标签\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率\n",
    "            # 第一个tag是当前字的标签，第二个tag是上一个字的标签\n",
    "            path_prob.append({tag:(tag, prob) for tag, prob in enumerate(Z)})\n",
    "        else:\n",
    "            Z = [sum(W[tag][char_feature]) for tag in tag_list]   # 每个tag的概率\n",
    "            res = {}\n",
    "            for j, prob in enumerate(Z):\n",
    "                # 加上上一个字的标签分别为0，1，2，3时的概率，然后求最大\n",
    "                max_prob, pre_tag =  max([(prob + W[j][pre_tag] + value[1], pre_tag) for pre_tag, value in path_prob[i-1].items()])\n",
    "                res[j] = (pre_tag, max_prob)\n",
    "            path_prob.append(res)\n",
    "    #print('path_prob:',path_prob)\n",
    "    # 确定最后一个字的tag，然后回溯，确定路径（确定每个字的tag）\n",
    "    last_tag = -1\n",
    "    max_prob = -10000000\n",
    "    for tag in tag_list:\n",
    "        prob = path_prob[-1][tag][1]\n",
    "        if max_prob < prob:\n",
    "            last_tag = tag\n",
    "    #last_tag = sorted(, key=lambda x: x[1][1], reverse = True)[0][0]\n",
    "    #print('last_tag:', last_tag)\n",
    "    # last_tag = np.argmax([pp[1][1] for pp in path_prob[-1]])\n",
    "    best_path.append(last_tag)\n",
    "    for pp in reversed(path_prob):\n",
    "        best_path.append(pp[last_tag][0])\n",
    "        last_tag = pp[last_tag][0]\n",
    "    best_path.pop()\n",
    "    best_path.reverse()\n",
    "    #print('best_path:', best_path)\n",
    "    return best_path\n",
    "                \n",
    "\n",
    "def precision_and_recall(predict_sentence_tags, actual_sentence_tags):\n",
    "    correct_count = 0\n",
    "    for i in range(len(actual_sentence_tags)):\n",
    "        #print('predict_sentence_tags[i]:', predict_sentence_tags[i])\n",
    "        #print('actual_sentence_tags[i]:', actual_sentence_tags[i])\n",
    "        if predict_sentence_tags[i] == actual_sentence_tags[i]:\n",
    "            correct_count += 1\n",
    "    return correct_count / len(actual_sentence_tags)\n",
    "    \n",
    "\n",
    "import time\n",
    "# 训练\n",
    "def train(sentence_feature_list, sentence_tag_list, maxIteration, ratio = 0.7):\n",
    "    global W\n",
    "    print(len(sentence_feature_list))\n",
    "    end = int(np.floor(ratio*len(sentence_feature_list)))\n",
    "    print(end)\n",
    "    x_train = sentence_feature_list[:end]\n",
    "    x_val = sentence_feature_list[end:]\n",
    "    y_train = sentence_tag_list[:end]\n",
    "    y_val = sentence_tag_list[end:]\n",
    "    for iter in range(maxIteration):\n",
    "        for i, sentence_feature in enumerate(x_train):\n",
    "#             print('i--', i)\n",
    "            if i % 10000 == 0:\n",
    "                print('i--', i)\n",
    "            start = time.time()\n",
    "            predict_tag = viterbi_decode(sentence_feature, W)\n",
    "            #print('viterbi_decode耗时：', time.time()-start)\n",
    "            actual_tag = y_train[i]\n",
    "#             print('predict_tag:', predict_tag)\n",
    "#             print('actual_tag:', actual_tag)\n",
    "            if predict_tag == actual_tag:\n",
    "                #print('true')\n",
    "                continue\n",
    "            else:\n",
    "                # 更新权重：每个特征上的实际标签权重增加(+1)，其他标签权重减少(-1)\n",
    "                start = time.time()\n",
    "                for j, char_feature in enumerate(sentence_feature):\n",
    "#                     print('predict_tag:', predict_tag)\n",
    "#                     print('actual_tag:', actual_tag)\n",
    "                    if predict_tag[j] == actual_tag[j]:\n",
    "                        continue\n",
    "#                     print('char_feature:',char_feature)\n",
    "                    for tag in tag_list:\n",
    "                        if tag == actual_tag[j]:\n",
    "                            #print('W[tag][char_feature] == ', W[tag][char_feature])\n",
    "                            W[tag][char_feature] += 1\n",
    "                            if j == 0:\n",
    "                                W[tag][4] += 1\n",
    "                            else:\n",
    "                                W[tag][predict_tag[j-1]] += 1\n",
    "                            #print('W[tag][char_feature] ++1 == ', W[tag][char_feature])\n",
    "                        elif tag == predict_tag[j]:\n",
    "                            W[tag][char_feature] -= 1\n",
    "                            if j == 0:\n",
    "                                W[tag][4] -= 1\n",
    "                            else:\n",
    "                                W[tag][predict_tag[j-1]] -= 1\n",
    "                #print('W耗时：', time.time()-start)\n",
    "\n",
    "        # 本轮次训练集和验证集上的结果\n",
    "        train_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_train]\n",
    "        print('训练集准确率：', precision_and_recall(train_predict_sentence_tags, y_train))\n",
    "        val_predict_sentence_tags = [viterbi_decode(sentence_feature, W) for sentence_feature in x_val]\n",
    "        print('验证集准确率：', precision_and_recall(val_predict_sentence_tags, y_val))\n",
    "\n",
    "print(len(sentence_feature_list))  \n",
    "sentence_feature_list2 = sentence_feature_list[:40000]\n",
    "sentence_tag_list2 = sentence_tag_list[:40000]\n",
    "train(sentence_feature_list, sentence_tag_list, 5, ratio = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709877\n"
     ]
    }
   ],
   "source": [
    "print(max(all_feature.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1709878"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
